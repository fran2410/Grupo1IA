# AI-Open-Science

[![License](http://img.shields.io/:license-apache-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0.html)
[![Python](https://img.shields.io/badge/python-3.13-blue)](https://www.python.org/) 
[![DOI]() 
[![Documentation Status]() 
[![GitHub release]()
[![HuggingFace](https://img.shields.io/badge/models-HuggingFace-red)](https://huggingface.co/)

## Description
This repository provides a complete pipeline for the advanced analysis of scientific articles. Starting from a corpus of 30 PDF papers, it processes them using [GROBID](https://github.com/kermitt2/grobid), extracts metadata and named entities, performs semantic analyses such as topic modeling and document similarity, and builds an enriched and visualizable RDF knowledge graph.

## Features
Given a PDF file (or a directory with some of them) the tool will extract the data and make:
- Metadata and acknowledgments extraction from XML files generated by GROBID.
- Topic modeling using embeddings and KeyBERT.
- Similarity computation between abstracts using Sentence Transformers.
- Named Entity Recognition (NER) in acknowledgment sections.
- Knowledge graph (RDF) construction with metadata, topics, authors, and relationships.
- Interactive visualization through a app.py demo.

## Project Structure

```
├── entrenamiento/ # All files used to train and validate our models
│ ├── papers_pdf/ # Example research papers for the validation
│ ├── xml_papers/ # Example XML files for the validation
│ ├── Testing_Models.py # Python Script to evaluate the NER models with the train data
│ ├── Text_Extraction_results_detailed.json # Result from the validations
├── output_files/ # Output files
│ ├── Text_Extraction_results.json # Result from metadata and entity extraction from XML
│ ├── dendrogram.png # Similarity dendrogram
│ ├── knowledge_graph_linked.rdf # Final graph RDF
│ ├── similarity_data.rdf # RDF with similarity and topics
│ ├── similarity_matrix.png # Similarity matrix
├── papers_pdf/ # Example research papers
├── scripts/ # Python scripts for data extraction and visualization
│ ├── Text_Extraction.py  # Metadata and entity extraction from XML
│ ├── app.py # Application for graph and data visualization
│ ├── dict_to_rdf.py # Main RDF graph generation from JSON
│ ├── pdfToXML # Uses Grobit to transform the papers into XML
│ ├── similarity_analysis.py # Similarity analysis + topic modeling + RDF generation
├── xml_papers/ # Example XML files

├── requirements.txt # Proyect depencencies
├── README.md # THis file
```

# Installing fron Github

##  Clone the repository:
   ```bash
   git clone https://github.com/fran2410/Grupo1IA.git
   cd Grupo1IA
   ```
## 1. Conda

For installing Conda on your system, please visit the official Conda documentation [here](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html).

#### Create and activate the Conda environment
```bash
conda create -n ai-open-science python=3.13  ***********************
conda activate ai-open-science ***********************
```

## 2. Poetry

For installing Poetry on your system, please visit the official Poetry documentation [here](https://python-poetry.org/docs/#installation).

#### Install project dependencies
Run the following command in the root of the repository to install dependencies:
```bash
poetry install
```

# Installing through Docker

We provide a Docker image with the scripts already installed. To run through Docker, you may build the Dockerfile provided in the repository by running:

```bash
docker build -t ai-open-science . ***********************
```

Then, to run your image just type:

```bash
docker run --rm -it  ai-open-science ***********************
```

And you will be ready to use the scripts (see section below). If you want to have access to the results we recommend [mounting a volume](https://docs.docker.com/storage/volumes/). For example, the following command will mount the current directory as the `out` folder in the Docker image:

```bash
docker run -it --rm -v $PWD/out:/AI-Open-Science/out ai-open-science ***********************
```
If you move any files produced by the scripts or set the output folder to `/out`, you will be able to see them in your current directory in the `/out` folder.

# USAGE
You can use any folder to store the PDFs to be processed and any other to extract the results. They don’t have to be specifically named `paper`, `data`, or `results`; you just need to specify them when running the commands.
## Using GROBID for XML Extraction
To extract structured XML data from PDFs using [GROBID](https://github.com/kermitt2/grobid), follow these steps:

1. Start the [GROBID](https://github.com/kermitt2/grobid) container
Run the following command to launch a [GROBID](https://github.com/kermitt2/grobid) server using Docker:

```bash
docker run --rm -p 8070:8070 lfoppiano/grobid:latest-full
```
This will start the [GROBID](https://github.com/kermitt2/grobid) service on port 8070.

2. Process PDFs with [GROBID](https://github.com/kermitt2/grobid)
Once the [GROBID](https://github.com/kermitt2/grobid) server is running, you can extract XML from a folder of PDFs using the following command:

```bash
curl -F input=@<folder_with_pdf> "http://localhost:8070/api/processFulltextDocument" -o <output_xml>
```
Alternatively, for batch processing of all PDFs in a directory:

```bash
for file in <pdf_folder>/*.pdf; do
    curl -F input=@$file "http://localhost:8070/api/processFulltextDocument" -o "<output_folder>/$(basename "$file" .pdf).xml"
done
```
## Ejecución de scripts
### Extract entities and metadata from papers
Extracts information such as authors, organizations, keywords, and metadata from GROBID-generated XML files.

**Command:**
```bash
python scripts/Text_Extraction.py papersXML/
```
**Output:** `<output_folder>/Text_Extraction_results.json`

### Compute similarities and generate RDF  
Extracts abstracts, detects topics, computes similarity, and generates the semantic RDF.

**Command:**
```bash
python scripts/similarity_analysis.py Text_Extraction_results.json
```
**Output:** `<output_folder>/similarity_matrix.png` `<output_folder>/dendrogram.png` `<output_folder>/similarity_data.rdf`

### Complete RDF graph  
Generates the final knowledge graph including metadata, entities, topics, and similarity relationships.

**Command:**
```bash
python scripts/dict_to_rdf.py Text_Extraction_results.json
```
**Output:** `<output_folder>/knowledge_graph_linked.rdf` 

### Web visualization
Runs the interactive Streamlit demo to explore the knowledge graph.

**Command:**
```bash
streamlit run app.py
```
**Input:** `<output_folder>/knowledge_graph_linked.rdf` `<output_folder>/similarity_data.rdf`


## Examples

For a sample execution with provided XML data, see the `entrenamiento/` directory or run the scripts with sample files in `pdf_papers/`.

## Where to Get Help

For any issues or questions, please open an issue in the [project issues](https://github.com/fran2410/AI-Open-Science/issues).

## Acknowledgements

- Special thanks to the developers of [GROBID](https://github.com/kermitt2/grobid) for their tool for processing scientific documents.
- Special thanks to the developers of [HuggingFace Transformers](https://huggingface.co/transformers/) for offering a wide variety of pre-trained NLP models and making it easy to integrate cutting-edge machine learning into real-world workflows.

## License

This project is distributed under the [Apache 2.0 License](http://www.apache.org/licenses/LICENSE-2.0). Contributions to the project must follow the same licensing terms.

