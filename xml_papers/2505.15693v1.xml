<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives MILAD KAZEMI, King&apos;s College London, UK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2025-05-21">21 May 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mateo</forename><surname>Perez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Milad</forename><surname>Kazemi</surname></persName>
							<email>milad.kazemi@kcl.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Fabio</forename><surname>Somenzi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
							<email>sadegh@mpi-sws.org</email>
						</author>
						<author>
							<persName><forename type="first">Ashutosh</forename><surname>Trivedi</surname></persName>
							<email>ashutosh.trivedi@colorado.edu</email>
						</author>
						<author>
							<persName><forename type="first">Alvaro</forename><surname>Velasquez</surname></persName>
							<email>velasquez@colorado.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Colorado Boulder</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">FABIO SOMENZI</orgName>
								<orgName type="institution">University of Colorado Boulder</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
								<orgName type="institution">SADEGH SOUDJANI</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">ASHUTOSH TRIVEDI</orgName>
								<orgName type="institution" key="instit2">University of Colorado Boulder</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">ALVARO VELASQUEZ</orgName>
								<orgName type="institution" key="instit2">University of Colorado Boulder</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">colorado.edu</orgName>
								<orgName type="institution">University of Colorado Boulder</orgName>
								<address>
									<settlement>Colorado</settlement>
									<region>Boulder</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">University of Colorado Boulder</orgName>
								<address>
									<settlement>Colorado</settlement>
									<region>Boulder</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
								<address>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="institution">University of Colorado Boulder</orgName>
								<address>
									<settlement>Colorado</settlement>
									<region>Boulder</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution">University of Colorado Boulder</orgName>
								<address>
									<settlement>Colorado</settlement>
									<region>Boulder</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives MILAD KAZEMI, King&apos;s College London, UK</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-05-21">21 May 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">7A96671CD0FC8AB90BA1308FD7924C0D</idno>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
					<idno type="arXiv">arXiv:2505.15693v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2025-05-26T20:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>JAIR Track: Insert JAIR Track Name Here</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in reinforcement learning (RL) have renewed focus on the design of reward functions that shape agent behavior. Manually designing reward functions is tedious and error-prone. A principled alternative is to specify behaviors in a formal language that can be automatically translated into rewards. Omega-regular languages are a natural choice for this purpose, given their established role in formal verification and synthesis. However, existing methods using omega-regular specifications typically rely on discounted reward RL in episodic settings, with periodic resets. This setup misaligns with the semantics of omega-regular specifications, which describe properties over infinite behavior traces. In such cases, the average reward criterion and the continuing setting-where the agent interacts with the environment over a single, uninterrupted lifetime-are more appropriate.</p><p>To address the challenges of infinite-horizon, continuing tasks, we focus on absolute liveness specifications-a subclass of omega-regular languages that cannot be violated by any finite behavior prefix, making them well-suited to the continuing setting. We present the first model-free RL framework that translates absolute liveness specifications to average-reward objectives. Our approach enables learning in communicating MDPs without episodic resetting. We also introduce a reward structure for lexicographic multi-objective optimization, aiming to maximize an external average-reward objective among the policies that also maximize the satisfaction probability of a given omega-regular specification. Our method guarantees convergence in unknown communicating MDPs and supports on-the-fly reductions that do not require full knowledge of the environment, thus enabling model-free RL. Empirical results show our average-reward approach in continuing setting outperforms discount-based methods across benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reinforcement learning (RL) <ref type="bibr" target="#b85">[86]</ref> is a foundational framework for sequential decision making under uncertainty, where agents learn to optimize their behavior through trial-and-error interaction with the environment. While RL has achieved superhuman performance in domains such as board games <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b78">[79]</ref><ref type="bibr" target="#b79">[80]</ref><ref type="bibr" target="#b80">[81]</ref>, robotics <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b99">100]</ref>, and resource optimization <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref>, much of this success has been in episodic tasks, i.e., settings where interactions naturally reset after a finite sequence, and learning proceeds by leveraging repeated experience across episode. In contrast, many real-world applications-such as autonomous monitoring, industrial process control, or mission planning-are inherently continuing tasks, where the agent must learn over a single, unending interaction with its environment. For such tasks, short-term rewards often fail to capture the long-term behavioral goals of interest. Instead, it is more natural to specify objectives in terms of structured, temporally extended criteria. Formal languages, particularly ùúî-regular languages recognized by automata over infinite traces <ref type="bibr" target="#b4">[5]</ref>, provide a rich and precise way to express such long-term goals. This has led to increasing interest in integrating formal methods with RL for the synthesis of correct-by-construction policies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b54">55]</ref>, enabling logic-based specifications to guide data-driven learning. This paper explores how model-free average-reward RL, a formulation especially suited for continuing tasks <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b93">94]</ref>, can be used to synthesize policies that satisfy ùúî-regular objectives.</p><p>Why ùúî-Regular Objectives? While many successful applications of RL rely on reward functions that depend only on the agent's current state and action, it is often necessary-or more natural-to account for the agent's history when defining learning objectives. This becomes especially important in settings with sparse rewards <ref type="bibr" target="#b65">[66]</ref>, partial observability <ref type="bibr" target="#b88">[89]</ref>, or temporally extended goals <ref type="bibr" target="#b14">[15]</ref>. In such cases, the agent's goal is better expressed as a sequence of desirable or undesirable behaviors rather than as immediate outcomes. Formal language structuresparticularly automata over sequences of observations-have emerged as a powerful tool for specifying these non-Markovian objectives. Rooted in formal verification <ref type="bibr" target="#b89">[90]</ref>, this approach enables the use of automata with a variety of acceptance conditions to capture complex reward structures and behavioral constraints. In some cases, even natural language objectives can be systematically translated into automata representations <ref type="bibr" target="#b12">[13]</ref>.</p><p>This line of work has gained significant traction in the formal synthesis of control policies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55]</ref>, where developers define high-level goals in a formal language and synthesis algorithms compute correct-by-construction policies-eliminating the need for manual, error-prone reward engineering <ref type="bibr" target="#b48">[49]</ref>. In this paper, we adopt this paradigm to synthesize policies using model-free RL for a class of non-Markovian objectives defined by ùúî-regular languages <ref type="bibr" target="#b4">[5]</ref>. They capture infinite sequences of semantically meaningful observations and provide a rich formalism for specifying long-term behavioral goals.</p><p>To operationalize such specifications within RL, we introduce nondeterministic reward machines, which encode the reward structure of ùúî-regular automata. By constructing a product between the agent's environmentmodeled as a Markov Decision Process (MDP)-and the automaton, we reduce the synthesis problem to a Markovian task that can be addressed using standard model-free algorithms. This builds on the broader use of reward machines <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b40">41]</ref>, which represent reward functions as automata over histories of semantically meaningful events. These machines provide a structured and interpretable way to incorporate temporal logic into RL and enable the transformation of non-Markovian objectives into equivalent Markovian formulations over an augmented state space <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b97">98]</ref>. This transformation allows existing RL techniques to be applied to long-horizon, history-dependent tasks without modification.</p><p>Limitations of Discounted Reward in Continuing Tasks. Despite the widespread use of discounted reward formulations in RL, they are often ill-suited for continuing tasks. The discounted return, defined as the sum of future rewards scaled by a discount factor, ensures mathematical tractability by bounding the total return over infinite horizons. However, this formulation inherently prioritizes short-term gains over long-term performance, which misaligns with the objectives of many continuing systems. In practice, achieving a suitable approximation of long-run behavior requires setting the discount factor very close to one. Yet, doing so weakens the contraction properties of learning algorithms, leading to slow convergence and potential instability.</p><p>These issues are particularly pronounced in continuing environments, where no natural episode boundaries exist. While discounted RL has shown remarkable success in episodic domains <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b78">79]</ref>, its solutions are sensitive to initial state distributions-an undesirable property in continuing settings where learning should be state-agnostic over the long run. Furthermore, discounted formulations are often incompatible with function approximation techniques commonly used in large-scale RL, as highlighted by recent studies <ref type="bibr" target="#b63">[64]</ref>. Given the growing reliance on large neural architectures in modern RL, the limitations of discounted RL motivate the need for alternative formulations better aligned with the demands of continuing tasks. These limitations motivate the need to explore average-reward RL as a more principled alternative in continuing settings.</p><p>Average-Reward RL. A natural alternative to discounting in continuing environments is to optimize the agent's average reward over time. This formulation aligns more directly with long-term performance objectives, making it particularly well-suited for tasks without episodic resets. However, average-reward RL introduces unique challenges. Unlike discounted RL-where the discount factor ensures convergence through a builtin contraction-average-reward methods rely on structural properties of the underlying MDP. In particular, the convergence of model-free average-reward algorithms <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b95">96]</ref> typically requires the MDP to be communicating, meaning every state is reachable from every other state under some policy. This communicating assumption is especially important-and potentially problematic-when synthesizing policies for ùúî-regular specifications. The synthesis process involves constructing a product MDP between the agent's environment and the automaton representing the specification. While the original MDP may satisfy the communicating property, the product MDP may not, creating a barrier to applying average-reward RL directly in such settings.</p><p>Absolute Liveness Properties. Absolute liveness specifications form an important subclass of ùúî-regular specifications and are prevalent throughout the temporal property hierarchy <ref type="bibr" target="#b58">[59]</ref>. These specifications are prefixindependent-their satisfaction is unaffected by the addition of arbitrary finite prefixes to an infinite word-making them particularly well-suited for continuing tasks. Moreover, when the underlying MDP is communicating, absolute liveness specifications are satisfied with probability either zero or one.</p><p>A key observation is that many temporally extended objectives, including those expressed in Linear Temporal Logic (LTL), can be naturally framed in terms of absolute liveness semantics. In particular, a satisfiable LTL formula ùúë is considered an absolute liveness property, if it is expressively equivalent to the formula Fùúë, where Fùúë denotes eventual satisfaction of ùúë. Thus, the eventual satisfaction semantics of an arbitrary ùúî-regular or LTL specification ùúë can be captured by the absolute liveness property Fùúë, further motivating their use in long-horizon learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>The main contribution of this paper is an average-reward model-free reinforcement learning algorithm for synthesizing policies that satisfy a given absolute liveness ùúî-regular specification. Our approach preserves the communicating property in the product construction, thereby enabling the learning of optimal policies without requiring episodic resetting. Although we assume the underlying MDP is communicating, a naive synchronization of the MDP with the automaton does not, in general, preserve this property. To address this, we introduce a reward machine construction and an augmented specification that guarantee the synchronized product MDP remains communicating.</p><p>To the best of our knowledge, this is the first framework that provides a formal translation from ùúî-regular objectives to average-reward RL with convergence guarantees. Preliminary results were reported at the AAMAS conference <ref type="bibr" target="#b44">[45]</ref>; the current paper extends that work with complete proofs, generalizations (weakly communicating MDPs), and novel results on multi-objective synthesis. In particular, we explore scenarios in which multiple optimal policies satisfy the given specification. Motivated by this, we propose a lexicographic multi-objective optimization framework, where the goal is to maximize an external average-reward objective among policies that already maximize the satisfaction probability of a given ùúî-regular specification. We establish convergence guarantees for this formulation in the setting of unknown communicating MDPs and absolute liveness objectives. Importantly, our reductions are on-the-fly and do not require full knowledge of the environment, enabling seamless integration with standard model-free RL algorithms.</p><p>This paper makes the following primary contributions:</p><p>(1) We present average-reward model-free RL algorithms for: (a) synthesizing policies that satisfy a given absolute liveness ùúî-regular specification, and (b) solving lexicographic multi-objective optimizations in which the goal is to maximize a mean-payoff objective among the set of satisfying policies. <ref type="bibr" target="#b1">(2)</ref> We study the structure of ùúî-regular specifications, including safety, liveness, absolute liveness, fairness, and stability. We introduce novel automata-theoretic characterizations of absolute liveness (Lemma 3.5) and stable specifications (Lemma 3.4), which may be of independent interest. <ref type="bibr" target="#b2">(3)</ref> We analyze the convergence of our algorithms and demonstrate that naive synchronization of a communicating MDP with a specification automaton can break the communicating property. To address this, we construct reward machines that preserve communication, enabling optimal learning without episodic resets. (4) Our work is the first to establish a formal reduction from ùúî-regular objectives to average-reward RL in both single-and multi-objective settings with convergence guarantees. (5) We generalize our prior results from communicating MDPs <ref type="bibr" target="#b44">[45]</ref> to the broader class of weakly communicating MDPs, thereby extending the applicability of our framework to environments where some states may not be mutually reachable, yet still support meaningful long-run average-reward optimization. <ref type="bibr" target="#b5">(6)</ref> We implement our proposed reduction using Differential Q-learning and evaluate it on a suite of communicating MDPs with absolute liveness specifications. Unlike prior methods, our approach composes the product MDP on-the-fly without requiring episodic resetting. Empirical results show that our method reliably converges to optimal policies in the continuing setting, even when prior approaches fail due to non-communicating product MDPs. In the episodic setting, our approach remains competitive in training time while requiring no resets, highlighting its robustness and practical effectiveness.</p><p>Organization. The paper is organized as follows. Section 2 includes the preliminaries, qualitative and quantitative objectives, RL for continuing tasks, and the problem statements. Section 3 provides our results on qualitative objectives suitable for continual learning. Section 4 establishes our theoretical results on average-reward RL for qualitative objectives. Section 5 presents our algorithm for average-reward RL with lexicographic objectives, where the primary goal is to satisfy a qualitative specification and the secondary goal is to optimize a quantitative average-reward objective. In Section 6, we test the performance of our approach on different case studies and compare it against prior techniques. Section 7 discusses related work before concluding in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries and Problem Statement</head><p>This section presents the foundational concepts and formalizes the problem setting. We begin with Markov Decision Processes (MDPs) as the underlying model of the environment in Subsection 2.1. In Subsection 2.2, we introduce probabilistic reward machines, which serve as quantitative objectives for RL-based policy synthesis. Subsection 2.3 covers ùúî-regular specifications as qualitative objectives for policy synthesis. We then formulate the formal problem statements in the context of reinforcement learning for continuing tasks in Subsection 2.4, and conclude by discussing the challenges of average-reward RL in Subsection 2.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Environment: Markov Decision Processes</head><p>Let D (ùëÜ) be the set of distributions over a given set ùëÜ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.1 (Markov decision process (MDP)</head><p>). An MDP M is a tuple (ùëÜ, ùë† 0 , ùê¥,ùëá , ùê¥ùëÉ, ùêø) where ‚Ä¢ ùëÜ is a finite set of states, with ùë† 0 ‚àà ùëÜ as the initial state;</p><p>‚Ä¢ ùê¥ is a finite set of actions;</p><p>‚Ä¢ ùëá : ùëÜ √ó ùê¥ ‚áÅ D (ùëÜ) is the probabilistic transition function;</p><p>‚Ä¢ ùê¥ùëÉ is the set of atomic propositions; and</p><p>‚Ä¢ ùêø : ùëÜ ‚Üí 2 ùê¥ùëÉ is the labeling function. For any state ùë† ‚àà ùëÜ, we let ùê¥(ùë†) denote the set of actions that can be selected in state ùë†. An MDP is a Markov chain if ùê¥(ùë†) is a singleton for all ùë† ‚àà ùëÜ.</p><p>Runs. For states ùë†, ùë† ‚Ä≤ ‚àà ùëÜ and ùëé ‚àà ùê¥(ùë†), ùëá (ùë†, ùëé) (ùë† ‚Ä≤ ) equals the conditional probability ùëù (ùë† ‚Ä≤ | ùë†, ùëé), which is the probability of jumping to state ùë† ‚Ä≤ from state ùë† under action ùëé. A run of M is an ùúî-word ‚ü®ùë† 0 , ùëé 1 , ùë† 1 , . . .‚ü© ‚àà ùëÜ √ó (ùê¥√óùëÜ) ùúî such that ùëù (ùë† ùëñ+1 | ùë† ùëñ , ùëé ùëñ+1 ) &gt; 0 for all ùëñ ‚â• 0. A finite run is a finite such sequence. For a run ùëü = ‚ü®ùë† 0 , ùëé 1 , ùë† 1 , . . .‚ü© we define the corresponding labeled run as ùêø(ùëü ) = ‚ü®ùêø(ùë† 0 ), ùêø(ùë† 1 ), . . .‚ü© ‚àà (2 ùê¥ùëÉ ) ùúî . We write Runs M (FRuns M ) for the set of (finite) runs of the MDP M and Runs M (ùë†) (FRuns M (ùë†)) for the set of (finite) runs of the MDP M from ùë†. We write last(ùëü ) for the last state of a finite run ùëü . The superscript M will be dropped when clear from context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strategies or Policies.</head><p>A strategy in M is a function ùúé : FRuns ‚Üí D (ùê¥) such that supp(ùúé (ùëü )) ‚äÜ ùê¥(last(ùëü )), where supp(ùëë) denotes the support of the distribution ùëë. A memory skeleton is a tuple ùëÄ ùî∞ = (ùëÄ, Œ£, ùëö 0 , ùõº ùî∞ ) where ùëÄ is a finite set of memory states, Œ£ is a finite alphabet, ùëö 0 ‚àà ùëÄ is the initial state, and ùõº ùî∞ : ùëÄ √ó Œ£ ‚Üí ùëÄ is the memory update function. We define the extended memory update function Œ±ùî∞ : ùëÄ√óŒ£ * ‚Üí ùëÄ inductively in the usual way. A finite-memory strategy for M over a memory skeleton ùëÄ ùî∞ is a Mealy machine (ùëÄ ùî∞ , ùõº ùîû ) where ùõº ùîû : ùëÜ√óùëÄ ‚Üí D (ùê¥) is the next-action function that suggests the next action based on the MDP and memory state. The semantics of a finite-memory strategy (ùëÄ, ùõº ùîû ) is given as a strategy ùúé : FRuns ‚Üí D (ùê¥) such that, for every ùëü ‚àà FRuns, we have that ùúé (ùëü ) = ùõº ùîû (last(ùëü ), Œ±ùî∞ (ùëö 0 , ùêø(ùëü ))).</p><p>A strategy ùúé is pure if ùúé (ùëü ) is a point distribution for all runs ùëü ‚àà FRuns M and is mixed (short for strictly mixed) if supp(ùúé (ùëü )) = ùê¥(last(ùëü )) for all runs ùëü ‚àà FRuns M . Let Runs M ùúé (ùë†) denote the subset of runs Runs M (ùë†) from initial state ùë† that follow strategy ùúé. Let Œ† M be the set of all strategies. We say that ùúé is stationary if last(ùëü ) = last(ùëü ‚Ä≤ ) implies ùúé (ùëü ) = ùúé (ùëü ‚Ä≤ ) for all runs ùëü, ùëü ‚Ä≤ ‚àà FRuns M . A stationary strategy can be given as a function ùúé : ùëÜ ‚Üí D (ùê¥). A strategy is positional if it is both pure and stationary.</p><p>Probability Space. An MDP M under strategy ùúé results in a Markov chain M ùúé . If ùúé is a finite-memory strategy, then M ùúé is a finite-state Markov chain. The behavior of an MDP M under a strategy ùúé and starting state ùë† ‚àà ùëÜ is defined on the probability space (Runs M ùúé (ùë†), F Runs M ùúé (ùë† ) , Pr M ùúé (ùë†)) over the set of infinite runs of ùúé with starting state ùë†. F Runs M ùúé (ùë† ) denotes the sigma-algebra on these runs generated by the underlying MDP, and Pr M ùúé (ùë†) is the probability distribution over these runs constructed inductively. Given a random variable ùëì : Runs M ‚Üí R, we denote by E M ùúé (ùë†){ùëì } the expectation of ùëì over the runs of M starting from ùë† that follow ùúé. Structural Properties of MDPs. A sub-MDP of M is an MDP M ‚Ä≤ = (ùëÜ ‚Ä≤ , ùê¥ ‚Ä≤ ,ùëá ‚Ä≤ , ùê¥ùëÉ, ùêø ‚Ä≤ ), where ùëÜ ‚Ä≤ ‚äÇ ùëÜ, ùê¥ ‚Ä≤ ‚äÜ ùê¥ is such that ùê¥ ‚Ä≤ (ùë†) ‚äÜ ùê¥(ùë†) for all ùë† ‚àà ùëÜ ‚Ä≤ , and ùëá ‚Ä≤ and ùêø ‚Ä≤ are ùëá and ùêø restricted to ùëÜ ‚Ä≤ and ùê¥ ‚Ä≤ . An end-component <ref type="bibr" target="#b17">[18]</ref> of an MDP M is a sub-MDP M ‚Ä≤ that is closed under the transitions in ùëá ‚Ä≤ and such that for every state pair ùë†, ùë† ‚Ä≤ ‚àà ùëÜ ‚Ä≤ there is a strategy that can reach ùë† ‚Ä≤ from ùë† with positive probability. A maximal end-component is an end-component that is maximal under set-inclusion. Every state ùë† of an MDP M belongs to at most one maximal end-component. A bottom strongly connected component (BSCC) of a Markov chain is any of its end-components.</p><p>An MDP M is communicating if it is equal to its (only) maximal end-component. An MDP M is weakly communicating if its state space can be decomposed into two sets: in the first set, each state is reachable from every other state in the set under some strategy; in the second set, all states are transient under all strategies, meaning that the probability of starting from ùë† in this set and returning to ùë† is less than one under any strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Quantitative Objectives: Probabilistic Reward Machines</head><p>In the classical RL literature, the learning objective is specified using Markovian reward functions, i.e., a function ùúå : ùëÜ √ó ùê¥ √ó ùëÜ ‚Üí R assigning utility to state-action pairs. A rewardful MDP is a tuple M = (ùëÜ, ùë† 0 , ùê¥,ùëá , ùúå), where ùëÜ, ùë† 0 , ùê¥, and ùëá are defined as for MDPs, and ùúå is a Markovian reward function. A rewardful MDP M under a strategy ùúé determines a sequence of random rewards ùúå (ùëã ùëñ ‚àí1 , ùëå ùëñ , ùëã ùëñ ) ùëñ ‚â•1 , where ùëã ùëñ and ùëå ùëñ are the random variables denoting the ùëñ-th state and action, respectively. For ùúÜ ‚àà [0, 1), the discounted reward under ùúé is defined as:</p><formula xml:id="formula_0">Disct (ùúÜ) M ùúé (ùë†) := lim ùëÅ ‚Üí‚àû E M ùúé (ùë†) ùëÅ ‚àëÔ∏Å ùëñ=1 ùúÜ ùëñ ‚àí1 ùúå (ùëã ùëñ ‚àí1 , ùëå ùëñ , ùëã ùëñ ) ,</formula><p>while the average reward is defined as</p><formula xml:id="formula_1">Avg M ùúé (ùë†) := lim sup ùëÅ ‚Üí‚àû 1 ùëÅ E M ùúé (ùë†) ùëÅ ‚àëÔ∏Å ùëñ=1 ùúå (ùëã ùëñ ‚àí1 , ùëå ùëñ , ùëã ùëñ ) .</formula><p>For an objective Reward M ‚àà{Disct (ùúÜ) M , Avg M } and state ùë†, we define the optimal reward Reward M * (ùë†) as sup ùúé ‚ààŒ† M Reward M ùúé (ùë†). A strategy ùúé is optimal for Reward M if Reward M ùúé (ùë†)=Reward M * (ùë†) for all ùë† ‚àà ùëÜ. Optimal reward and strategies for these objectives can be computed in polynomial time <ref type="bibr" target="#b68">[69]</ref>.</p><p>Often, complex learning objectives cannot be expressed using Markovian reward functions. A recent trend is to express learning objectives using finite-state reward machines <ref type="bibr" target="#b38">[39]</ref>. For the objectives we consider, we require a more expressive variant of reward machines having probabilistic transitions, nondeterministic transitions, and spurious transitions labeled with ùúñ. We call them probabilistic reward machines with the understanding that any of these three types of transitions could be absent. For instance, we retrieve the definition of nondeterministic reward machine as in <ref type="bibr" target="#b44">[45]</ref> if any activated transition is taken with probability one. The use of ùúñ-transitions is to streamline the presentation of the technical result of this paper. ‚Ä¢ ùõø ùëü : ùëà √ó Œ£ ùúñ ‚Üí 2 ùëà is the transition relation (that allows nondeterminism),</p><p>‚Ä¢ ùëá ùëù : ùëà ùëù ‚áÅ D (ùëà ùëù ) is the probabilistic transition function, and</p><formula xml:id="formula_2">‚Ä¢ ùúå : ùëà √ó ùëà ùëù √ó Œ£ ùúñ √ó ùëà √ó ùëà ùëù ‚Üí R is the reward function. To simplify the notation we let ùëà R = ùëà √ó ùëà ùëù and ùë¢ R 0 = (ùë¢ 0 , ùë¢ ùëù 0 ). Definition 2.3 (Product (rewardful) MDP). Given an MDP M = (ùëÜ, ùë† 0 , ùê¥,ùëá , ùê¥ùëÉ, ùêø), a reward machine R = (Œ£ ùúñ , ùëà R , ùë¢ R 0 , ùõø ùëü ,ùëá ùëù , ùúå)</formula><p>with the alphabet Œ£ = 2 ùê¥ùëÉ , and the labeling function ùêø : ùëÜ ‚Üí Œ£, their product</p><formula xml:id="formula_3">M √ó R = (ùëÜ√óùëà R , ùë† 0 √óùë¢ R 0 , (ùê¥√óùëà R ) ‚à™ {ùúñ},ùëá √ó , ùúå √ó ) is a rewardful MDP where ùëá √ó :(ùëÜ√óùëà R ) √ó ((ùê¥√óùëà R ) ‚à™ {ùúñ}) ‚Üí D (ùëÜ√óùëà R ) is such that ùëá √ó ((ùë†, (ùë¢, ùë¢ ùëù )), ùõº) ((ùë† ‚Ä≤ , (ùë¢ ‚Ä≤ , ùë¢ ùëù ‚Ä≤ ))) = Ô£± Ô£¥ Ô£¥ Ô£¥ Ô£≤ Ô£¥ Ô£¥ Ô£¥ Ô£≥ ùëá (ùë†, ùëé) (ùë† ‚Ä≤ ) ‚Ä¢ ùëá ùëù (ùë¢ ùëù ) (ùë¢ ùëù ‚Ä≤ ) if ùõº = (ùëé, (ùë¢ ‚Ä≤ , ùë¢ ùëù ‚Ä≤ )) and (ùë¢, ùêø(ùë†), ùë¢ ‚Ä≤ ) ‚àà ùõø ùëü 1 if ùõº = ùúñ, ùë† = ùë† ‚Ä≤ , and ùõø (ùë¢, ùúñ, ùë¢ ‚Ä≤ ) ‚àà ùõø ùëü 0 otherwise. and ùúå √ó : (ùëÜ√óùëà R ) √ó ((ùê¥√óùëà R ) ‚à™ {ùúñ}) √ó (ùëÜ√óùëà R ) ‚Üí R is defined such that ùúå √ó ((ùë†, ùë¢ R ), ùõº, (ùë† ‚Ä≤ , ùë¢ R ‚Ä≤ )) = ùúå (ùë¢ R , ùêø(ùë†), ùë¢ R ‚Ä≤ ) if ùõº = (ùëé, ùë¢ R ‚Ä≤ ) ùúå (ùë¢ R , ùúñ, ùë¢ R ‚Ä≤ ) if ùõº = ùúñ.</formula><p>A deterministic reward machine is retrieved from the above definition by setting | ùõø ùëü (ùë¢, ùëé) |‚â§ 1 and ùëá ùëù (ùë¢ ùëù ) ‚àà {0, 1} for all ùë¢ ‚àà ùëà , ùëé ‚àà Œ£ ùúñ , and ùë¢ ùëù ‚àà ùëà ùëù . For technical convenience, we assume that M√óR contains only states reachable from (ùë† 0 , ùë¢ R 0 ). For both discounted and average objectives, the optimal strategies of M√óR are positional on M√óR. Moreover, these positional strategies characterize a finite memory strategy (with memory skeleton based on the states of R and the next-action function based on the positional strategy) over M maximizing the learning objective given by R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Qualitative Objectives: Omega-Regular Specifications</head><p>Formal specification languages, such as ùúî-automata and logic, provide a rigorous and unambiguous mechanism to express learning objectives for continuing tasks. There is a growing trend <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b70">71]</ref> toward expressing learning objectives in RL using linear temporal logic (LTL) and ùúî-regular languages (which strictly generalize LTL). We will describe ùúî-regular languages by good-for-MDP B√ºchi automata <ref type="bibr" target="#b32">[33]</ref>.</p><p>Linear Temporal Logic (LTL) <ref type="bibr" target="#b4">[5]</ref> is a temporal logic that is often used to specify objectives in human-readable form. Given a set of atomic propositions ùê¥ùëÉ, the LTL formulae over ùê¥ùëÉ can be defined via the following grammar:</p><formula xml:id="formula_4">ùúë := ùëé | ¬¨ùúë | ùúë ‚à® ùúë | X ùúë | ùúë U ùúë,<label>(2.1)</label></formula><p>where ùëé ‚àà ùê¥ùëÉ, using negation ¬¨, disjunction ‚à®, next X, and until U operators. Additional operators are introduced as abbreviations:</p><formula xml:id="formula_5">‚ä§ def = ùëé ‚à® ¬¨ùëé; ‚ä• def = ¬¨‚ä§; ùúë ‚àß ùúì def = ¬¨(¬¨ùúë ‚à® ¬¨ùúì ); ùúë ‚Üí ùúì def = ¬¨ùúë ‚à® ùúì ; F ùúë def = ‚ä§ U ùúë; and G ùúë def = ¬¨ F ¬¨ùúë. We write ùë§ |= ùúë if ùúî-word ùë§ over 2 ùê¥ùëÉ satisfies LTL formula ùúë.</formula><p>The satisfaction relation is defined inductively <ref type="bibr" target="#b4">[5]</ref>. We will provide details of classes of specifications including safety, liveness, and fairness in Subsection 3.</p><p>Nondeterministic B√ºchi automata are finite state acceptors for all ùúî-regular languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.4 (B√ºchi Automata). A (nondeterministic)</head><p>B√ºchi automaton is a tuple A = (Œ£, ùëÑ, ùëû 0 , ùõø, ùêπ ), where ‚Ä¢ Œ£ is a finite alphabet,</p><p>‚Ä¢ ùëÑ is a finite set of states,</p><p>‚Ä¢ ùëû 0 ‚àà ùëÑ is the initial state,</p><p>‚Ä¢ ùõø : ùëÑ √ó Œ£ ‚Üí 2 ùëÑ is the transition function, and</p><formula xml:id="formula_6">‚Ä¢ ùêπ ‚äÇ ùëÑ √ó Œ£ √ó ùëÑ is the set of accepting transitions. A run ùëü of A on ùë§ ‚àà Œ£ ùúî is an ùúî-word ‚ü®ùëü 0 , ùë§ 0 , ùëü 1 , ùë§ 1 , . . .‚ü© ‚àà (ùëÑ √ó Œ£) ùúî such that ùëü 0 = ùëû 0 and, for ùëñ &gt; 0, ùëü ùëñ ‚àà ùõø (ùëü ùëñ ‚àí1 , ùë§ ùëñ ‚àí1 ). Each triple (ùëü ùëñ ‚àí1 , ùë§ ùëñ ‚àí1 , ùëü ùëñ ) is a transition of A.</formula><p>We write Inf (ùëü ) for the set of transitions that appear infinitely often in the run ùëü . A run ùëü of A is accepting if Inf(ùëü ) ‚à© ùêπ ‚â† ‚àÖ. The language L (A) of A is the subset of words in Œ£ ùúî with accepting runs in A. A language is ùúî-regular if it is accepted by a B√ºchi automaton.</p><p>Good-for-MDP B√ºchi Automata. Given an MDP M and a specification ùúë represented with an ùúî-automaton A = (Œ£, ùëÑ, ùëû 0 , ùõø, ùêπ ), we want to compute an optimal strategy satisfying the objective. We define the satisfaction probability of ùúé from starting state ùë† as</p><formula xml:id="formula_7">PSem M A (ùë†, ùúé) = Pr M ùúé (ùë†) ùëü ‚àà Runs M ùúé (ùë†) : ùêø(ùëü ) ‚àà L (A)</formula><p>. The optimal satisfaction probability PSem M A (ùë†) for specification A is defined as sup ùúé ‚ààŒ† M Pr M ùúé (ùë†, ùúé) and we say that ùúé ‚àà Œ† M is an optimal strategy for</p><formula xml:id="formula_8">A if PSem M A (ùë†, ùúé) (ùë†) = PSem M A (ùë†).</formula><p>Definition 2.5 (Product MDP). Given an MDP M = (ùëÜ, ùë† 0 , ùê¥,ùëá , ùê¥ùëÉ, ùêø) and automaton A = (Œ£, ùëÑ, ùëû 0 , ùõø, ùêπ ) with alphabet Œ£ = 2 ùê¥ùëÉ , the product M √ó A = (ùëÜ √ó ùëÑ, (ùë† 0 , ùëû 0 ), ùê¥ √ó ùëÑ,ùëá √ó , ùêπ √ó ) is an MDP with initial state (ùë† 0 , ùëû 0 ) and accepting transitions ùêπ √ó where ùëá √ó :</p><formula xml:id="formula_9">(ùëÜ √ó ùëÑ) √ó (ùê¥ √ó ùëÑ) ‚áÅ D (ùëÜ √ó ùëÑ) is defined by ùëá √ó ((ùë†, ùëû), (ùëé, ùëû ‚Ä≤ )) ((ùë† ‚Ä≤ , ùëû ‚Ä≤ )) = ùëá (ùë†, ùëé) (ùë† ‚Ä≤ ) if (ùëû, ùêø(ùë†), ùëû ‚Ä≤ ) ‚àà ùõø 0 otherwise.</formula><p>The accepting transition set ùêπ √ó ‚äÜ (ùëÜ √ó ùëÑ) √ó (ùê¥ √ó ùëÑ) √ó (ùëÜ √ó ùëÑ) is defined by ((ùë†, ùëû), (ùëé, ùëû ‚Ä≤ ), (ùë† ‚Ä≤ , ùëû ‚Ä≤ )) ‚àà ùêπ √ó if, and only if, (ùëû, ùêø(ùë†), ùëû ‚Ä≤ ) ‚àà ùêπ and ùëá (ùë†, ùëé) (ùë† ‚Ä≤ ) &gt; 0. A strategy ùúé √ó on the product defines a strategy ùúé on the MDP with the same value, and vice versa. Note that for a stationary ùúé √ó , the strategy ùúé may need memory. End-components and runs of the product MDP are defined just like for MDPs.</p><p>A run</p><formula xml:id="formula_10">ùëü of M√óA is accepting if Inf(ùëü ) ‚à© ùêπ √ó ‚â† ‚àÖ.</formula><p>The syntactic satisfaction probability is the probability of accepting runs:</p><formula xml:id="formula_11">PSat M A ((ùë†, ùëû), ùúé √ó ) = Pr M√ó A ùúé √ó (ùë†, ùëû) ùëü ‚àà Runs M√ó A ùúé √ó (ùë†, ùëû) : Inf(ùëü ) ‚à© ùêπ √ó ‚â† ‚àÖ .</formula><p>Similarly, we define PSat M A (ùë†) as the optimal probability over the product, i.e., sup ùúé √ó PSat M A ((ùë†, ùëû 0 ), ùúé √ó ) . For a deterministic A the equality PSat M A (ùë†) = PSem M A (ùë†) holds; however, this is not guaranteed for nondeterministic B√ºchi automata as the optimal resolution of nondeterministic choices may require access to future events. This motivates the definition of a good-for-MDP nondeterminisitc B√ºchi automata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.6 (Good-for-MDP [33]). A B√ºchi automaton</head><formula xml:id="formula_12">A is good for MDPs (GFM), if PSat M A (ùë† 0 ) = PSem M A (ùë† 0</formula><p>) holds for all MDPs M and starting states ùë† 0 .</p><p>Note that every ùúî-regular objective can be expressed as a GFM automaton <ref type="bibr" target="#b32">[33]</ref>. A popular class of GFM automata is suitable limit-deterministic B√ºchi automata <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b76">77]</ref>. There are other types of ùúî-automata that are good-for-MDPs. For example, in parity automata, each transition is assigned an integer priority. A run of a parity automaton is accepting if the highest recurring priority is odd. In this paper, we only use GFM B√ºchi automata.</p><p>The satisfaction of an ùúî-regular objective given as a GFM automaton A by an MDP M can be formulated in terms of the accepting maximal end-components of the product M√óA, i.e., the maximal end-components that contain an accepting transition from F √ó . The optimal satisfaction probabilities and strategies can be computed by first computing the accepting maximal end-components of M √ó A and then maximizing the probability to reach states in such components. The optimal strategies are positional on M √ó A and induce finite-memory strategies over M that maximize the satisfaction probability of the learning objective given by A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Reinforcement Learning for Continuing Tasks and Problem Statements</head><p>RL is a sampling-based optimization approach where an agent learns to optimize its strategy by repeatedly interacting with the environment relying on the reinforcements (numerical reward signals) it receives for its actions. We focus on the model-free approach to RL, where the learner computes optimal strategies without explicitly estimating the transition probabilities and rewards. These approaches are asymptotically more spaceefficient <ref type="bibr" target="#b84">[85]</ref> than model-based RL and have been shown to scale well <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b77">78]</ref>. Some prominent model-free RL algorithms for discounted and average reward objectives include Q-learning and TD(ùúÜ) <ref type="bibr" target="#b85">[86]</ref> and Differential Q-learning <ref type="bibr" target="#b92">[93,</ref><ref type="bibr" target="#b93">94]</ref>.</p><p>In some applications, such as running a maze or playing tic-tac-toe-the interaction between the agent and the environment naturally breaks into finite length learning sequences, called episodes. Thus the agent optimizes its strategy by combining its experience over different episodes. We call such tasks episodic. On the other hand, for some applications-such as process control and reactive systems-this interaction continues ad-infinitum and the agent learns over a single lifetime. We call such tasks continuing. This paper develops a model-free RL algorithm for continuing tasks where the learning objective is an ùúî-regular specification given as a GFM automaton. Prior solutions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b70">71]</ref> focused on episodic setting and have proposed a model-free reduction from ùúî-regular objectives to discounted-reward objectives. Several researchers <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b85">86]</ref> have made the case for adopting average reward formulation for continuing tasks due to several limitations of discounted-reward RL in continuing tasks. This paper investigates a model-free reduction from ùúî-regular objectives to average-reward objectives in model-free RL.</p><p>Problem 2.7 (ùúî-Regular to Average Reward Translation). Given an unknown communicating MDP M = (ùëÜ, ùë† 0 , ùê¥,ùëá , ùê¥ùëÉ, ùêø) and a GFM automaton A = (Œ£, ùëÑ, ùëû 0 , ùõø, ùêπ ), the reward translation problem is to design a reward machine R such that an optimal positional strategy maximizing the average reward for M√óR provides a finite memory strategy maximizing the satisfaction probability of L (A) in M.</p><p>We also consider the lexicographic multi-objective optimization in which the goal is to maximize a mean-payoff objective over those policies that satisfy a given liveness specification.</p><p>Problem 2.8 (Lexicographic ùúî-regular and Average Reward). Given an unknown communicating MDP M = (ùëÜ, ùë† 0 , ùê¥,ùëá , ùê¥ùëÉ, ùêø), a GFM automaton A = (Œ£, ùëÑ, ùëû 0 , ùõø, ùêπ ), and a reward function ùúå (ùë†, ùë† ‚Ä≤ ), the problem is to design an algorithm such that the resulting policy maximizes the average reward</p><formula xml:id="formula_13">Avg M ùúé (ùë†) := lim sup ùëÅ ‚Üí‚àû 1 ùëÅ E M ùúé (ùë†) ùëÅ ‚àëÔ∏Å ùëñ=1 ùúå (ùë† ùëñ ‚àí1 , ùëé ùëñ , ùë† ùëñ ) ,</formula><p>among the policies that maximize the satisfaction probability of A in M.</p><p>Next we give an account of challenges with average-reward RL, identify suitable classes of qualitative temporal specifications in Section 3, and then solve Problems 2.7-2.8 respectively in Sections 4-5 under appropriate assumptions on the structure of the underlying MDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Average-Reward Reinforcement Learning</head><p>Given an MDP M, reward machine R, and an objective (discounted or average reward), an optimal strategy can be computed in polynomial time using linear programming <ref type="bibr" target="#b68">[69]</ref>. Similarly, graph-theoretic techniques to find maximal end-components can be combined with linear programming to compute optimal strategies for ùúî-regular objectives <ref type="bibr" target="#b30">[31]</ref>. However, such techniques are not applicable when the transitions or reward structure of the MDP is unknown. The existing average-reward RL algorithms such as Differential Q-learning provide convergence guarantees under the assumption that the MDP M is communicating <ref type="bibr" target="#b93">[94]</ref>. Thus, for the reward translation in Problem 2.7 to be effective, we need the product M √ó A to be communicating. Unfortunately, even when M is communicating, M √ó A may violate the communicating requirement. We tackle this issue in Section 4.</p><p>The communicating assumption requires that, for every pair of states ùë† and ùë† ‚Ä≤ , there is a policy under which the MDP can reach ùë† ‚Ä≤ from ùë† in a finite number of steps with positive probability. Under the communicating assumption, there exists a unique optimal reward rate ùëü ‚òÖ that is independent of the start state. The differential Q-learning algorithm constructs a table of estimates ùëÑ ùë° : ùëÜ √ó ùëà ‚Üí R at each time step ùë°. Let us denote the state-action pair of the MDP at time ùë° by (ùë† ùë° , ùë¢ ùë° ). Then, differential Q-learning updates the table as follows:</p><formula xml:id="formula_14">ùëÑ ùë° +1 (ùë†, ùë¢) := ùëÑ ùë° (ùë†, ùë¢)</formula><p>for all (ùë†, ùë¢) ‚â† (ùë† ùë° , ùë¢ ùë° )</p><formula xml:id="formula_15">ùëÑ ùë° +1 (ùë†, ùë¢) := ùëÑ ùë° (ùë† ùë° , ùë¢ ùë° ) + ùõº ùë° ùõø ùë° , for (ùë†, ùë¢) = (ùë† ùë° , ùë¢ ùë° ),</formula><p>where ùõº ùë° is the step-size at step ùë°, and ùõø ùë° is the temporal difference error defined as</p><formula xml:id="formula_16">ùõø ùë° := ùúå ùë° +1 ‚àí rùë° + max ùë¢ ùëÑ ùë° (ùë† ùë° +1 , ùë¢) ‚àí ùëÑ ùë° (ùë† ùë° , ùë¢ ùë° ) and rùë°+1 := rùë° + ùúÇ ‚Ä¢ ùõº ùë° ‚Ä¢ ùõø ùë° ,</formula><p>where ùúå ùë° +1 is the reward received for the transition (ùë† ùë° , ùë¢ ùë° , ùë† ùë° +1 ), rùë° is a scalar estimate of the optimal reward rate ùëü ‚òÖ , and ùúÇ is a positive constant. It is shown in <ref type="bibr" target="#b93">[94]</ref> that rùë° converges to ùëü ‚òÖ under the following assumptions: (a) the MDP is communicating; (b) the associated Bellman equation has a unique solution up to a constant; (c) the step sizes ùõº ùë° decrease appropriately with ùë°;</p><p>(d) all state-action pairs are updated infinitely often; and (e) the ratio of the update frequency of the most-updated state-action pair to the least-updated one is finite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Specifications Suitable for Continual Learning</head><p>In this section, we study and characterize temporal logic specifications that express various classes of tasks. We then identify those suitable for continual reinforcement learning and provide solutions to Problems 2.7-2.8 in subsequent sections. Two important classes of temporal logic specifications are safety and liveness properties <ref type="bibr" target="#b1">[2]</ref>.</p><p>Loosely speaking, safety specifications ensure that something bad never happens, while liveness specifications guarantee that something good eventually happens. Below, we briefly recall the formal definition of safety and then focus on liveness.</p><p>Definition 3.1 (Safety and Liveness). Let ùë§ be a sequence over alphabet Œ£ and let ùë§ (ùëñ) represent the prefix of ùë§ of length ùëñ + 1; i.e., ùë• (ùëñ) = (ùë• 0 , ùë• 1 , . . . , ùë• ùëñ ).</p><p>‚Ä¢ A language L ‚äÜ Œ£ ùúî is a safety specification if, and only if, for all ùúî-sequences ùë§ ‚àà Œ£ ùúî not in L, some prefix of ùë§ cannot be extended to an ùúî-sequence in the language. ‚Ä¢ A language L ‚äÜ Œ£ ùúî is a liveness specification if, and only if, we can extend any finite sequence in Œ£ * to an ùúî-sequence in the language. In other words, the set {ùë§ (ùëñ) :</p><formula xml:id="formula_17">ùë§ ‚àà L} is Œ£ * . For instance ùúì 1 = ùëé ‚à® F ùëè is a liveness specification.</formula><p>Definition 3.2 (Absolute Liveness). A language L ‚äÜ Œ£ ùúî is an absolute liveness specification if, and only if, L is non-empty and, for every finite sequence w ‚àà Œ£ * and every ùë§ ‚àà L, wùë§ ‚àà L holds. That is, appending an arbitrary finite prefix to an accepted word produces an accepted word. This implies that an LTL specification ùúë is absolute liveness specification if ùúë is satisfiable and ùúë is equivalent to F ùúë. One can observe that ùúì 1 is not an absolute liveness specification since adding the prefix ¬¨ùëé to a trace that does not satisfy F ùëè yields a trace not in the language. However, ùúì 2 = F ùëé is an absolute liveness specification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3.3 (Stable Specification</head><p>). A language L ‚äÜ Œ£ ùúî is a stable specification if, and only if, it is closed under suffixes; that is, if ùë• is in L, then every suffix of ùë• is also in L. Since deleting the first element from ùëé(¬¨ùëé) ùúî results in a sequence that does not satisfy ùúì 2 , ùúì 2 is not stable. However, ùúì 3 = G ùëé ‚à® G F ùëè is stable. Moreover, a language is a fairness specification if and only if it is both a stable specification and absolute liveness specification, e.g., ùúì 4 = G F ùëé. One can easily conclude that none of the specifications ùúì 1 ,ùúì 2 , and ùúì 3 are fairness specifications.</p><p>We give a solution for the translation in Problem 2.7 for absolute liveness specifications <ref type="bibr" target="#b82">[83]</ref>. Adding a prefix to a trace for average reward objectives should not change the average value associated with the trace. This is aligned with the satisfaction of absolute liveness specifications. Moreover, since absolute liveness specifications cannot be rejected for any finite word, they preserve the continual nature of the learning procedure. In the following, we focus on the characteristics of absolute liveness specifications.</p><p>The set of absolute liveness specifications is closed under union and intersection. This proofs mentioned below rely on the property ùúë ‚â° F ùúë in LTL notation. The more general ùúî-regular proofs use ùúë ‚â° Œ£ * ùúë. In addition, closure under intersection requires an "asterisk:" the intersection is an absolute liveness specification unless it is empty. Suppose ùúë 1 is equivalent to F ùúë 1 and ùúë 2 is equivalent to F ùúë 2 . Then, Moreover, since for every LTL specification ùúë, F ùúë is equivalent to F F ùúë,</p><formula xml:id="formula_18">ùúë 1 ‚à® ùúë 2 ‚â° (F ùúë 1 ) ‚à® (F ùúë 2 ) ‚â° F(ùúë 1 ‚à® ùúë 2 ) .</formula><formula xml:id="formula_19">ùúë 1 ‚àß ùúë 2 ‚â° (F F ùúë 1 ) ‚àß (F F ùúë 2 ) ‚â° F((F ùúë 1 ) ‚àß (F ùúë 2 )) ‚â° F(ùúë 1 ‚àß ùúë 2 ) .</formula><p>Lemma 2.1 of <ref type="bibr" target="#b82">[83]</ref>, which says that the complement of a stable specification different from Œ£ ùúî is an absolute liveness specification and vice versa, can be used to show that the stable specifications are also closed under union and intersection. The case when either stable specification is ‚ä§ is easily proved. Because of these closure properties, 1‚â§ùëò ‚â§ùëõ (F G ùëé ùëò ) ‚àß (G F ùëè ùëò ) (where each ùëé ùëò and ùëè ùëò is an atomic proposition) is an absolute liveness specification of Rabin index ùëõ. This means that there is no upper bound to the number of priorities required by a parity automaton accepting an absolute liveness specification <ref type="bibr" target="#b91">[92]</ref>. The simplest absolute liveness specification is ‚ä§, which requires one priority. The automata that accept absolute liveness and stable specifications have special properties discussed next. Lemma 3.4. Let A be a deterministic ùúî-automaton with initial state ùë† 0 . A accepts an absolute liveness specification if, and only if, the language accepted from any reachable state ùë† of A contains the language accepted from ùë† 0 .</p><p>Proof. Suppose A accepts an absolute liveness specification. Let ùë¢ be a word that takes the automaton from ùë† 0 to ùë†. If ùë§ is accepted from ùë† 0 , so is ùë¢ùë§. This means that ùë§ is accepted from ùë† because A is deterministic and the unique run of ùë¢ùë§ goes through ùë†. Now suppose that the language accepted from any reachable state of A contains the language accepted from ùë† 0 . If ùë¢ takes A from ùë† 0 to ùë† and ùë§ is accepted from ùë† 0 , then ùë§ is accepted from ùë†, which means that ùë¢ùë§ is accepted from ùë† 0 . Therefore, A accepts an absolute liveness specification. ‚ñ°</p><p>The "only-if" part of Lemma 3.4 does not apply to nondeterministic automata-witness the two-state automaton for the absolute liveness specification F G ùëé, in which the language of the accepting state is the language of G ùëé. We have a similar result for stable specifications. Lemma 3.5. Let A be an ùúî-automaton with initial state ùë† 0 . A accepts a stable specification if, and only if, the language accepted from any reachable state ùë† of A is contained in the language accepted from ùë† 0 .</p><p>Proof. Suppose A accepts a stable specification. Let ùë§ be a word accepted from ùë† and ùë¢ be a word that takes the automaton from ùë† 0 to ùë†. Then ùë¢ùë§ is accepted from ùë† 0 . This means that the suffix ùë§ of ùë¢ùë§ is accepted from ùë† 0 because A accepts a stable specification. Now suppose that the language accepted from any reachable state of A is contained in the language accepted from ùë† 0 . If ùë¢ takes A from ùë† 0 to ùë† on some accepting run of ùë¢ùë§, then ùë§ is accepted from ùë†, which means that ùë§ is accepted from ùë† 0 . Therefore, A accepts a stable specification. ‚ñ°</p><p>Note that Lemma 3.5 does not require determinism. Combined with Lemma 3.4, it proves that a deterministic automaton accepts a fairness specification if, and only if, all its states are language-equivalent. This implies that a fairness specification accepted by a deterministic (B√ºchi) automaton is accepted by a strongly connected deterministic (B√ºchi) automaton. Any reachable sink SCC of a deterministic automaton for a fairness specification is, in itself, an automaton for the specification. The above lemmas show that checking whether a specification is stable or an absolute liveness specification is reducible to checking language containment for deterministic ùúî-automata. To solve Problem 2.7, we make the following assumption.</p><p>Assumption 1. Given an MDP M and ùúî-automaton A, we assume that: 1) M is communicating; 2) A is a GFM automaton; and 3) A accepts an absolute liveness specification.</p><p>We also study relaxing the communicating assumption on M to weakly communicating, which requires strengthening the specification to fairness in order to retain correctness and convergence guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Average-Reward RL for Qualitative Objectives</head><p>This section provides a solution for Problem 2.7. Let us fix a communicating MDP M = (ùëÜ, ùë† 0 , ùê¥,ùëá , ùê¥ùëÉ, ùêø) and an absolute liveness GFM property A = (Œ£, ùëÑ, ùëû 0 , ùõø, ùêπ ) for the rest of this section. Our goal is to construct a reward machine R such that we can use an off-the-shelf average reward RL on M √ó R to compute an optimal strategy of M against A. Since the optimal strategies are not positional on M but rather positional on M√óA, it is natural to assume that the reward machine R takes the structure of A with a reward function providing positive reinforcement with every accepting transition. Unfortunately, even for absolute liveness GFM automata A, the product M √ó A with a communicating MDP M may not be communicating.</p><formula xml:id="formula_20">ùëû 0 ùëû 1 ùëû 2 ùëé ¬¨ùëé ‚ä§ ùëé ¬¨ùëé Fig. 1. Automaton for (F G ùëé)‚à®(F G ¬¨ùëé).</formula><p>Example 4.1. Assume a communicating MDP M with at least one state labeled ùëé or ¬¨ùëé, and the absolute liveness property ùúë = (F G ùëé) ‚à® (F G ¬¨ùëé) with its automaton shown in Fig. <ref type="figure">1</ref>. Observe that any run that visits one of the two accepting states cannot visit the other one. Hence, the product does not satisfy the communicating property.</p><p>Reward Machine Construction. Let A = (Œ£, ùëÑ, ùëû 0 , ùõø, ùêπ ) be an absolute liveness GFM B√ºchi automaton. Consider R A = (Œ£ ùúñ , ùëÑ, ùëû 0 , ùõø ‚Ä≤ , ùúå) where ùõø ‚Ä≤ (ùëû, ùëé) = ùõø (ùëû, ùëé) for all ùëé ‚àà Œ£ and ùúñ transitions reset to the starting state, i.e. ùõø ‚Ä≤ (ùëû, ùúñ) = ùëû 0 . Note that by adding the reset (ùúñ) action from every state of R to its initial state, the graph structure of M is strongly connected. The reward function ùúå : ùëÑ √ó (Œ£ ‚à™ {ùúñ}) √ó ùëÑ‚ÜíR is such that</p><formula xml:id="formula_21">ùúå (ùëû, ùëé, ùëû ‚Ä≤ ) = Ô£± Ô£¥ Ô£¥ Ô£¥ Ô£≤ Ô£¥ Ô£¥ Ô£¥ Ô£≥ ùëê if ùëé = ùúñ 1 if (ùëû, ùëé, ùëû ‚Ä≤ ) ‚àà ùêπ 0 otherwise.</formula><p>Remark 4.2. Intuitively, adding reset transitions that should only be taken a finite number of times corresponds to transforming the GFM B√ºchi automaton into a nondeterministic parity automaton with three priorities, with the reset transitions rejecting at the highest priority. The parity automaton accepts the same language as the given B√ºchi automaton because an accepting run only takes finitely many reset transitions. Hence, a word whose run uses reset transitions consists of a finite prefix followed by a word accepted by the B√ºchi automaton. Since the latter accepts an absolute liveness property, adding a prefix to an accepted word gives another accepted word. One can then observe that adding transitions to an automaton results in an automaton that simulates the given automaton. Finally, one invokes [33, <ref type="bibr">Lemma 1]</ref> to conclude that the nondeterministic parity automaton is GFM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 4.3 (Preservation of Communication).</head><p>For a communicating MDP M and reward machine R A for an absolute liveness GFM automaton A, we have that the product M√óR A is communicating. Proof. To show that M√óR A is communicating, we need to show that for arbitrary states (ùë†, ùëû), (ùë† ‚Ä≤ , ùëû ‚Ä≤ ) ‚àà ùëÜ √óùëÑ reachable from the initial state (ùë† 0 , ùëû 0 ), there is a strategy that can reach (ùë† ‚Ä≤ , ùëû ‚Ä≤ ) from (ùë†, ùëû) with positive probability. Note that since M is communicating, it is possible to reach (ùë† 0 , ùëû ‚Ä≤ ) from (ùë†, ùëû) for some ùëû ‚Ä≤ of R A using a strategy to reach ùë† 0 from ùë† in M. We can then use a reset (ùúñ) action in R A to reach the state (ùë† 0 , ùëû 0 ). Since (ùë† ‚Ä≤ , ùëû ‚Ä≤ ) is reachable from the initial state (ùë† 0 , ùëû 0 ), we have a strategy to reach (ùë† ‚Ä≤ , ùëû ‚Ä≤ ) from (ùë†, ùëû) with positive probability. ‚ñ° Lemma 4.4 (Average and Probability). There exists a ùëê * &lt; 0 such that for all ùëê &lt; ùëê * , positional strategies that maximize the average reward on M √ó R A will maximize the satisfaction probability of A.</p><p>Proof. The proof is in three parts.</p><p>(1) First observe that if ùëê &lt; 0, then for any average-reward optimal strategy in M √ó R A , the expected average reward is non-negative. This is so because all other actions except ùúñ actions provide non-negative rewards. Hence, any strategy that takes ùúñ actions only finitely often, results in a non-negative average reward. (2) Let Œ† * be the set of positional strategies in M√óR A such that the ùúñ actions are taken only finitely often, i.e. no BSCC of the corresponding Markov chain contains an ùúñ transition. Let Œ† ùúñ be the set of remaining positional strategies, i.e., the set of positional strategies that visit an ùúñ transition infinitely often. Let 0&lt;ùëù min &lt;1 be a lower bound on the expected long-run frequency of the ùúñ transitions among all strategies in Œ† ùúñ . Let ùëê * = ‚àí1/ùëù min . Observe that for every policy ùúé ‚Ä≤ ‚àà Œ† ùúñ , the expected average reward is negative and cannot be an optimal strategy in M √ó R A . To see that, let 0 &lt; ùëù ‚â§ 1 be the the long-run frequency of the ùúñ transitions for ùúé and let 0 ‚â§ ùëû &lt; 1 be the long-run frequency of visiting accepting transitions for ùúé. The average reward for ùúé is</p><formula xml:id="formula_22">Avg M √ó R A ùúé (ùë† 0 , ùëû 0 ) = ùëù ‚Ä¢ ùëê + ùëû ‚Ä¢ 1 + (1 ‚àí ùëù ‚àí ùëû) ‚Ä¢ 0 ‚â§ ùëù ‚Ä¢ ùëê + ùëû ‚Ä¢ 1 + (1 ‚àí ùëù ‚àí ùëû) ‚Ä¢ 1 = ùëù ‚Ä¢ ùëê + (1 ‚àí ùëù) ‚â§ ùëù ‚Ä¢ ùëê * + (1 ‚àí ùëù) = ‚àíùëù/ùëù min + (1 ‚àí ùëù) ‚â§ ‚àí1 + (1 ‚àí ùëù) ‚â§ ‚àíùëù.</formula><p>Since every optimal policy must have a non-negative average reward, no policy in Œ† ùúñ is optimal for ùëê &lt; ùëê * .</p><p>(3) Now consider an optimal policy ùúé * in Œ† * . We show that this policy also optimizes the probability of satisfaction of A. There are two cases to consider. (a) If the expected average reward of ùúé * is 0, then under no strategy it is possible to reach an accepting transitions (positive reward transition) in M √ó R A . Hence, every policy is optimal in M against A, and so is ùúé * . (b) If the expected average reward of ùúé * is positive, then notice that for every BSCC of the Markov chain of M √ó R A under ùúé * , the average reward is the same. This is so because otherwise, there is a positional policy that reaches the BSCC with the optimal average from all the other BSCCs with lower averages, contradicting the optimality of ùúé * . Since for an optimal policy ùúé * , every BSCC provides the same positive average, every BSCC must contain an accepting transition. Hence, every run of the MDP M under ùúé * will eventually dwell in an accepting component and in the process will see a finitely many ùúñ (reset) transitions. For any such given run ùëü , consider the the suffix ùëü ‚Ä≤ of the run after the last ùúñ transition is taken and let ùëü = ùë§ùëü ‚Ä≤ for some finite run ùë§. Since ùêø(ùëü ‚Ä≤ ) is an accepting word in A, and since A is an absolute liveness property any arbitrary prefix ùë§ ‚Ä≤ to this run ùëü ‚Ä≤ is also accepting. This implies that the original run ùëü is also accepting for A. It follows that for such a strategy ùúé * , the probability of satisfaction of A is 1, making ùúé * an optimal policy for M against A. ‚ñ° Since our translation from ùúî-regular objective to reward machines is model-free, the following theorem is immediate.</p><p>Theorem 4.5 (Convergence of Model-free RL). Differential ùëÑ-learning algorithm for maximizing average reward objective on M √ó R A will converge to a strategy maximizing the probability of satisfaction of A for a suitable value of ùëê. Moreover, the product construction M √ó R A can be done on-the-fly and it is model-free.</p><p>As an example, consider the property F G ùëé and an MDP with two states and all transitions between states are available as deterministic actions (Fig. <ref type="figure">2</ref> and Fig. <ref type="figure">3</ref>). Only one of the states is labeled ùëé. An infinite memory strategy could see ùëé for one step, reset, then see two ùëés, reset, then see three ùëés and so forth. This strategy will produce the same average value as the positional strategy which sees ùëé forever without resetting. However, the infinite memory strategy will fail the property while the positional one will not.</p><p>Shaping Rewards via Hard Resets. For a B√ºchi automaton A, we say that its state ùëû ‚àà ùëÑ is coaccessible if there exists a path starting from that state to an accepting transition. If a state is not coaccessible then any run of the product M √ó A that ends in such a state will never be accepting, and hence one can safely redirect all of its outgoing (even better, incoming) transitions to the initial state with reward ùëê (a hard reset). Such hard resets will promote speedy learning by reducing the time spent in such states during unsuccessful explorations, and at the same time adding these resets does not make a non-accepting run accepting or vice versa. Lemma 4.3, Lemma 4.4, and Theorem 4.5 continue to hold with such hard resets. Introducing hard resets is a reward shaping procedure in that it is a reward transformation <ref type="bibr" target="#b66">[67]</ref> under which optimal strategies remain invariant.</p><formula xml:id="formula_23">ùëû 0 ùëû 1 ùëû 2 ùëé ¬¨ùëé ‚ä§ ùëé ‚ä§</formula><p>Extension to weakly communicating MDPs. To relax further the communicating assumption on the MDP, we can apply our method to weakly communicating MDPs but the class of specifications will be more restricted.</p><p>The set of weakly-communicating MDPs is the most general set of MDPs such that there currently exists a learning algorithm that can, using a single stream of experience, guarantee to identify a policy that achieves the optimal average reward rate in the MDP <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b94">95]</ref>. We show how to apply our method in the following for weakly communicating MDPs and fairness specifications. Proof. The proof directly follows the proof of Lemma 4.3. Since we have a weakly-communicating MDP we can partition the states of the MDP into two subsets: the first set is all the transient states under stationary policies, and the second set are the set of states where every pair of states are reachable from each other under a stationary policy. Since the sub-MDP resulting from the second set is communicating, and a fairness property is also an absolute liveness property, then based on Lemma 4.3, the sub-product MDP is communicating. On the other hand, all of the states in the first set are transient, and the fairness property is closed under the addition and deletion of prefixes then eventually we reach a state of the communicating sub-product MDP which leads to the weakly communicating property of the product MDP. ‚ñ°</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Average-Reward RL for Lexicographic Objectives</head><p>This section provides a solution for Problem 2.8. We first note that an optimal policy for the Problem 2.8 may require infinite memory. Consider the example in Figure <ref type="figure">4</ref>. The ùúî-regular specification G F ùëé can be satisfied surely by staying in the state ùëé. Likewise, the maximum possible average reward of 1 can be obtained by staying in the state labeled ¬¨ùëé. Surprisingly, there is an infinite memory policy that achieves both. This infinite memory policy visits the state labeled ¬¨ùëé ùëò times, then visits the state labeled ùëé, where ùëò increases towards infinity forever. Any finite memory policy that visits ùëé infinitely often with probability 1 must visit the state labeled ùëé with positive expected frequency in steady state, say ùëì &gt; 0, and thus achieves a suboptimal external average reward of at most (1 ‚àí ùëì ) &lt; 1. However, one can find finite memory policies that achieve an average reward within ùúÄ of the optimal average reward for any ùúÄ &gt; 0 while maximizing the probability of satisfaction of the ùúî-regular specification. Namely, consider the policy which visits the state labeled ¬¨ùëé ùëò times, then visits the state labeled ùëé for a fixed ùëò. This In the following, we show that for any ùúÄ &gt; 0, there always exist a finite-memory policy that is ùúÄ-optimal for Problem 2.8 and show how this policy can be constructed. Our approach to solve Problem 2.8 reduces the task of producing a lexicographically optimal strategy to one of solving a single average reward problem on an MDP, which can then be solved by any off-the-shelf RL algorithm for average reward on communicating MDPs, e.g. Differential Q-learning. The resulting policy is ùúÄ-optimal and finite memory. We then perform a transformation of this learned policy to yield an infinite memory optimal policy. We define our average reward reduction via a reward machine. This reward machine augments the usual product with an additional bit. When this bit is zero, the agent receives reward based on the external reward function. With probability ùõΩ on every step, this bit flips to one. When this bit is one, the agent is incentivized to find an accepting edge and is given a punishing reward of ùúå (ùë†, ùë† ‚Ä≤ ) + ùëê 1 where ùëê 1 is a constant such that ùëê 1 + max ùë†,ùë† ‚Ä≤ ùúå (ùë†, ùë† ‚Ä≤ ) &lt; min ùë†,ùë† ‚Ä≤ ùúå (ùë†, ùë† ‚Ä≤ ) on every step. When the agent finds an accepting edge, the bit flips back to zero. To ensure communication, we add two types of resets, one which resets the automaton state back to its initial state (ùúñ 1 ), and one which sets the extra bit to zero (ùúñ 2 ). Both of these incur some large penalty of ùëê 2 . We refer to the case when the bit is zero as "the first layer" and the case when the bit is one as "the second layer". Intuitively, the agent spends most of its time collecting the external average reward, but is occasionally called upon with probability ùõΩ to prove that it can see an accepting edge. This construction is shown pictorially in Figure <ref type="figure">5</ref>. We now define the reward machine formally. Let A = (Œ£, ùëÑ, ùëû 0 , ùõø, ùêπ ) be a GFM for an absolute liveness specification and let ùúå (ùë†, ùë† ‚Ä≤ ) be a given reward function. We construct a probabilistic reward machine</p><formula xml:id="formula_24">R A‚Ä¢ùúå = (Œ£ ‚Ä≤ , ùëÑ √ó {0, 1}, (ùëû 0 , 0), ùõø ‚Ä≤ , ùúå ‚Ä≤ ) where Œ£ ‚Ä≤ = (Œ£ √ó ùëÜ √ó ùëÜ √ó ùëÑ) ‚à™ {ùúñ 1 , ùúñ 2 },</formula><p>by applying a positional policy optimal for the external reward to the second layer. Since ùúé is optimal, it must obtain a value of ùëè + ùë£ * , which obtains an external average reward of ùë£ * on M. If this policy takes resets infinitely often, then it takes the reset transition immediately upon reaching the second layer. This is due to the penalty ùëè: if a trajectory of length ùëá in the second layer that ends in a reset obtains a value of ùëá (ùëè + ùë£) + ùëê, then taking the reset first, and followed by the same trajectory in the first layer obtains a value of ùëá ùë£ + ùëê ‚â• ùëá (ùëè + ùë£) + ùëê, where ùë£ is the average reward collected along this trajectory. Thus, this policy obtains a value of ùëÖ ùúé = (1 ‚àí ùõΩ)ùë£ 1 + ùõΩùëê in the extended product, which is maximized when ùë£ 1 = ùë£ * . Since the resets are internal to the agent, this policy collects an external average reward of ùë£ * on M. We have shown that under any 0 &lt; ùõΩ * &lt; 1 and ùëê * &lt; 0 when the ùúî-regular objective is satisfied with probability 0, an optimal policy on the extended product collects an external average reward of ùë£ * ‚â• ùë£ * ‚àí ùúÄ for every ùúÄ &gt; 0.</p><p>(2) We now consider the case where the ùúî-regular objective is satisfied with probability 1 under some policy.</p><p>Let ùê∫ ‚äÜ (M √ó A) √ó (M √ó A) be the set of transitions such that for every ùë° ‚àà ùê∫, there exists a policy that satisfies A with probability 1 and visits ùë° infinitely often. In other words, ùê∫ is the maximal set of transitions that can be visited infinitely often without diminishing the probability of satisfying A. Consider a lexicographically optimal strategy. This strategy cannot visit any transition outside of ùê∫ infinitely often by definition and achieves an external average reward of ùë£ * . This implies that there exists a set of positional policies restricted to ùê∫ that achieves an average reward of at least ùë£ * , since there exists optimal positional policies for average reward on MDPs. We now extend these policies to the extended product by applying this policy in the first layer and applying a policy which visits an accepting edge in the second layer without taking resets infinitely often and leaves ùê∫ finitely many times, with probability 1. This second layer policy exists due to the definition of ùê∫. We call this set of policies ùúé * , and abuse notion by treating ùúé * as if it is a single policy for the rest of the proof. We will select 0 &lt; ùõΩ * &lt; 1 and ùëê * &lt; 0 such that ùúé * is optimal in the extended product, and is ùúÄ-optimal on M. Consider all policies Œ† 1 that are positional on the extended product, satisfy the specification with probability 1, and visit the first layer infinitely often with probability 1. We say that the average reward that such a policy ùúé ‚àà Œ† 1 obtains on the extended product for a particular ùõΩ is ùëÖ ùúé (ùõΩ). Let ùë£ ùúé be the external average reward that ùúé obtains when ùõΩ = 0. Then, we have that (1 ‚àí ùõΩ)ùë£ ùúé + ùõΩùëè ùëì ùúé (ùõΩ) ‚â§ ùëÖ ùúé (ùõΩ) ‚â§ ùë£ ùúé where ùëì ùúé (ùõΩ) is an upper bound on the expected time it takes to return to a state in steady state after a transition from the first to the second layer occurs. The first inequality follows by breaking up the average reward into uninterrupted reward collected in the first layer in steady state, and reward collected while waiting to return to the state that was left by transitioning to the second layer in steady state. The second inequality follows from the fact that the reward in the second layer is always less than the first. Note that ùëì ùúé (ùõΩ) is monotonically increasing in ùõΩ. Consider a policy ùúé ‚àà Œ† 1 such that ùë£ ùúé &lt; ùë£ ùúé * = ùë£ * . Then we have that for</p><formula xml:id="formula_25">ùõΩ ‚â§ min{ 1 2 , ùë£ * ‚àíùë£ ùúé ùë£ * ‚àíùëè ùëì ùúé * ( 1 2 ) }, ùëÖ ùúé (ùõΩ) ‚â§ ùë£ ùúé ‚â§ (1 ‚àí ùõΩ)ùë£ ùúé * + ùõΩùëè ùëì ùúé * (ùõΩ) ‚â§ (1 ‚àí ùõΩ)ùë£ ùúé * + ùõΩùëè ùëì ùúé * ( 1 2 ) ‚â§ ùëÖ ùúé * (ùõΩ)</formula><p>where the third inequality follows from the monotonicity of ùëì ùúé * (ùõΩ). Since there are only finitely many policies in Œ† 1 , one can find a 0 &lt; ùõΩ thresh &lt; 1 that satisfies the above inequality for all policies in Œ† 1 .</p><p>Thus, for all ùõΩ &lt; ùõΩ thresh , ùúé * is an optimal average reward policy on the extended product amongst all the policies in Œ† 1 . Note that the external average reward collected on M by any policy ùúé for a fixed ùõΩ, denoted by Rùúé (ùõΩ), is larger than the average reward collected on the extended product ùëÖ ùúé (ùõΩ) because all of the rewards on the extended product are less than or equal to the external rewards. By selecting</p><formula xml:id="formula_26">ùõΩ * = min{ùõΩ thresh , ùúÄ ùë£ * ‚àíùëè ùëì ùúé * ( 1 2 ) } &gt; 0, we have that ùë£ * ‚àí ùúÄ ‚â§ (1 ‚àí ùõΩ)ùë£ ùúé * + ùõΩùëè ùëì ùúé * ( 1 2 ) ‚â§ ùëÖ ùúé * (ùõΩ) ‚â§ Rùúé * (ùõΩ),</formula><p>so ùúé * is ùúÄ-optimal for all 0 &lt; ùõΩ &lt; ùõΩ * . We will now select ùëê * &lt; 0 such that there is an optimal policy in Œ† 1 . We denote all positional policies by Œ†. Note that all policies in Œ†\Œ† 1 either remain in the second layer forever, obtaining an average reward of ùëè &lt; min ùë†,ùë† ‚Ä≤ ùúå (ùë†, ùë† ‚Ä≤ ) which is less than the average reward for all policies Œ† 1 , or take reset transitions infinitely often. Policies which select reset transitions infinitely often obtain an average reward of at most (1 ‚àí ùõΩ) max ùë†,ùë† ‚Ä≤ ùúå (ùë†, ùë† ‚Ä≤ ) + ùõΩùëê. One can then select ùëê * ‚â§ ùëè ùõΩ ‚àí</p><p>1‚àíùõΩ ùõΩ max ùë†,ùë† ‚Ä≤ ùúå (ùë†, ùë† ‚Ä≤ ) &lt; 0 to ensure that (1 ‚àí ùõΩ) max ùë†,ùë† ‚Ä≤ ùúå (ùë†, ùë† ‚Ä≤ ) + ùõΩùëê ‚â§ ùëè. Since all policies in Œ† 1 obtain an average reward on the extended product greater than ùëè, there is an optimal policy ùúé * in Œ† 1 for ùëê &lt; ùëê * with the previously fixed ùõΩ. This ùúé * is an optimal policy on the extended product under these parameter selections and is ùúÄ-optimal when applied to M. ‚ñ° Theorem 5.4 (Lexicographic Reward Machine Optimality). Let ùõΩ (ùëñ) : N ‚Üí (0, 1) be a sequence such that lim ùëñ‚Üí‚àû ùõΩ (ùëñ) = 0. There exists a threshold ùõΩ * &gt; 0 such that for all 0 &lt; ùõΩ &lt; ùõΩ * there exists a ùëê * &lt; 0 such that for all ùëê &lt; ùëê * all positional policies that maximize the average reward on M √ó R A‚Ä¢ùúå have the following property: if we transform this fixed, finite memory policy by setting ùõΩ = ùõΩ (ùëñ) on timestep ùëñ, then this transformed policy is optimal for Problem 2.8 and infinite memory.</p><p>Proof. We first note that this policy is infinite memory: it must keep track of ùõΩ (ùëñ), which requires an increasing number bits to represent exactly as ùõΩ (ùëñ) decreases. Thus, as ùëñ goes to infinity, the amount of memory required grows in an unbounded manner. For the rest of the proof, we follow the results and proof of Theorem 5.3. Let ùõΩ * = ùõΩ thresh as defined in the proof of Theorem 5.3, and let</p><formula xml:id="formula_27">ùëê * ‚â§ ùëè ùõΩ ‚àí 1 ‚àí ùõΩ ùõΩ max ùë†,ùë† ‚Ä≤ ùúå (ùë†, ùë† ‚Ä≤ ) &lt; 0.</formula><p>We have that there is a set of optimal positional policies ùúé * on the extended product that satisfy A with the maximum probability and accumulate an external average reward Rùúé * (ùõΩ) ‚â• (1 ‚àí ùõΩ)ùë£ * + ùõΩùëè ùëì ùúé * ( <ref type="formula">1</ref>2 ) where ùë£ * is the external average reward obtained under a lexicographically optimal strategy. We now substitute ùõΩ with ùõΩ (ùëñ). We have that </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>We implemented the reduction<ref type="foot" target="#foot_2">1</ref> with hard resets presented in Section 4. As described, we do not build the product MDP explicitly, and instead compose it on-the-fly by keeping track of the MDP and automaton states  We compare our proposed reduction, the reduction of <ref type="bibr" target="#b31">[32]</ref> with Q-learning, and the reduction of <ref type="bibr" target="#b10">[11]</ref> with Q-learning. Episodic resetting is not used.</p><p>independently. We use Differential Q-learning <ref type="bibr" target="#b93">[94]</ref> to learn optimal, positional average reward strategies. For our experiments, we have collected a set of communicating MDPs with absolute liveness specifications. <ref type="foot" target="#foot_3">2</ref>We compare with two previous approaches for translating ùúî-regular languages to rewards: the method of <ref type="bibr" target="#b31">[32]</ref> with Q-learning and the method of <ref type="bibr" target="#b10">[11]</ref> with Q-learning. The method of <ref type="bibr" target="#b31">[32]</ref> translates a GFM B√ºchi automaton into a reachability problem through a suitable parameter ùúÅ . This reachability problem can be solved with discounted RL by rewarding reaching the target state and using a large enough discount factor. The method of <ref type="bibr" target="#b10">[11]</ref> uses a state dependent discount factor ùõæ ùêµ and a GFM B√ºchi automaton. By using a suitable ùõæ ùêµ and large enough discount factor, one can learn optimal strategies for the ùúî-regular objective.</p><p>RQ1. How do previous approaches perform in the continuing setting? The methods of <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref> may produce product MDPs that are not communicating (see <ref type="bibr">Example 4.1)</ref>. This means that a single continuing run of the MDP may not explore all relevant states and actions. Thus, previous methods are not guaranteed to converge in this setting. We studied if this behavior affects these prior methods in practice. As a baseline, we include our proposed approach. Instead of tuning hyperparameters for each method, where hyperparameters that lead to convergence may not exist, we take a sampling approach. We select a wide distribution over hyperparameters for each method and sample 200 hyperparameter combinations for each method and example. We then train for 10 million steps on each combination. The selected hyperparameter distribution is ùõº ‚àº D (0.01, 0.5), ùúÄ ‚àº D (0.01, 1.0), ùëê ‚àº D <ref type="bibr" target="#b0">(1,</ref><ref type="bibr">200)</ref>, ùúÇ ‚àº D (0.01, 0.5), ùúÅ ‚àº D (0.5, 0.995), ùõæ ùêµ ‚àº D (0.5, 0.995), and ùõæ ‚àº D (0.99, 0.99999) where D (ùëé, ùëè) is a log-uniform distribution from ùëé to ùëè. The end points of these distributions and the training amount are selected by finding hyperparameters which lead to convergence in the episodic setting for these methods.</p><p>Figure <ref type="figure" target="#fig_7">6</ref> shows the resulting distribution over runs. A distribution entirely at 0 <ref type="bibr" target="#b0">(1)</ref> indicates that all sampled runs produced strategies that satisfy the specification with probability 0 (1). For many examples, prior approaches have no successful hyperparameter combinations, with distributions centered entirely at 0. However, our proposed approach always have some hyperparameters that lead to optimal, probability 1, strategies, as indicated by the tails of the distributions touching the probability 1 region of the plot. indicates results from Q-learning with reduction from <ref type="bibr" target="#b31">[32]</ref>, while superscript ‚Ä° indicates Q-learning with reduction from <ref type="bibr" target="#b10">[11]</ref>.</p><p>Results for ‚Ä† and ‚Ä° require episodic resetting. All hyperparameters are tuned by hand.</p><p>RQ2. How does our method compare to previous approaches when we allow episodic setting? By allowing episodic resetting, we can now find hyperparameters for previous methods that lead to convergence. We tuned all hyperparameters by hand to minimize training time, while verifying with a model checker that the produced strategies are optimal. Table <ref type="table" target="#tab_0">1</ref> shows learning times, as well as hyperparameters for our reduction. We report the number of states reachable in the MDP and the product, learning times averaged over 5 runs, the reset penalty ùëê, the ùúÄ-greedy exploration rate ùúÄ, the learning rates ùõº and ùúÇ of the Differential Q-learning, as well as the number of training steps. Note that we do not do any episodic resetting when training with our reduction. This means that the RL agent must learn to recover from mistakes during training, while previous approaches are periodically reset to a good initial state. Our reduction using Differential Q-learning is competitive with previous approaches while not being reliant on episodic resetting.</p><p>RQ3. How our approach works in multi-objective settings? As shown in Table <ref type="table" target="#tab_2">2</ref>, we successfully satisfied the absolute liveness property with probability one across all case studies. We report the number of states reachable in the MDP and the product, the learning times averaged over five runs, the average reward over five runs, the reset penalty ùëê and ùõΩ, the ùúÄ-greedy exploration rate ùúÄ, the learning rates ùõº and ùúÇ of Differential Q-learning, as well as the number of training steps. Note that, also in the multi-objective setting, we do not perform any episodic resetting when training with our reduction. We achieved average rewards that are almost optimal in all cases (the optimal value for the first case study is 1 and for the rest is 0).   <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref>. These logics have equivalent automaton and reward machine representations that have catalyzed a series of efforts on defining novel reward shaping functions to accelerate the convergence of RL algorithms subject to formal specifications <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">40]</ref>. These methods leverage the graph structure of the automaton to provide an artificial reward signal to the agent. More recently, dynamic reward shaping using LTL ùëì has been introduced as a means to both learn the transition values of a given reward machine and leverage these values for reward shaping and transfer learning <ref type="bibr" target="#b90">[91]</ref>. There has also been work on learning or synthesizing the entire structure of such reward machines from agent interactions with the environment by leveraging techniques from satisfiability and active grammatical inference <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b96">97,</ref><ref type="bibr" target="#b97">98]</ref>. Other data-driven methods have also been developed recently for satisfying specifications over finite traces. Data-driven distributionally robust policy synthesis approaches are presented in <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b74">75]</ref>. Data-driven construction of abstractions with correctness guarantees has also been studied <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b72">73]</ref>, which enables policy synthesis against temporal specifications. Data-driven computation of resilience with respect to finite temporal logic specifications is studied in <ref type="bibr" target="#b71">[72]</ref>.</p><p>Formal Specifications over Infinite Traces. For the infinite-trace settings, LTL has been extensively used to verify specifications and synthesize policies formally using the mathematical model of a system <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b98">99]</ref>. Considering the generality of the available results in terms of structure of the underlying MDP, most of the research focuses on discounted reward structures. Despite the simplicity of discounted Markov decision problems, the discounted reward structure (unlike average reward) prioritizes the transient response of the system. However, application of the average reward objective because of the restriction over the structure of the MDP is limited. The work <ref type="bibr" target="#b21">[22]</ref> proposes a policy iteration algorithm for satisfying specifications of the form G F ùúô ‚àß ùúì for a communicating MDP almost surely. The work <ref type="bibr" target="#b2">[3]</ref> proposes a value iteration algorithm for solving the average reward problem for multichain MDPs, where the algorithm first computes the optimal value for each of strongly connected components and then weighted reachability to find the optimal policy. The work <ref type="bibr">[4]</ref> provides a linear program for policy synthesis of multichain MDPs with steady-state constraints. The work <ref type="bibr" target="#b56">[57]</ref> provides a regret-free learning algorithm for policy synthesis that is based on identifying the structure of the underlying MDP using data with a certain confidence.</p><p>RL for Formal Specifications over Infinite Traces. In the last few years, researchers have started developing data-driven policy synthesis techniques in order to satisfy temporal specifications. There is a large body of literature in safe reinforcement learning (RL) (see e.g. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b69">70]</ref>). The problem of learning a policy to maximize the satisfaction probability of a temporal specification using discounted RL is studied recently <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b70">71]</ref>.</p><p>The work <ref type="bibr" target="#b31">[32]</ref> uses a parameterized augmented MDP to provide an RL-based policy synthesis for finite MDPs with unknown transition probabilities. It shows that the optimal policy obtained by RL for the reachability probability on the augmented MDP gives a policy for the MDP with a suitable convergence guarantee. In <ref type="bibr" target="#b10">[11]</ref> authors provide a path-dependent discounting mechanism for the RL algorithm based on a limit-deterministic B√ºchi automaton (LDBA) representation of the underlying ùúî-regular specification, and prove convergence of their approach on finite MDPs when the discounting factor goes to one. An LDBA is also leveraged in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b67">68]</ref> for discounted-reward model-free RL in both continuous-and discrete-state MDPs. The LDBA is used to define a reward function that incentivizes the agent to visit all accepting components of the automaton. These works use episodic discounted RL with discount factor close to one to solve the policy synthesis problem. There are two issues with the foregoing approaches. First, because of the episodic nature of the algorithms they are not applicable in continuing settings. Second, because of high discount factors in practice these algorithm are difficult to converge. On the other hand, recent work on reward shaping for average reward RL has been explored based on safety specifications to be satisfied by the synthesized policy <ref type="bibr" target="#b42">[43]</ref>. In contrast to the solution proposed in this paper, the preceding approach requires knowledge of the graph structure of the underlying MDP and does not account for absolute liveness specifications.</p><p>Average-Reward RL. There is a rich history of studies in average reward RL <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b53">54]</ref>. Lack of stopping criteria for multichain MDPs affect the generality of model-free RL algorithms. Therefore, all model-free RL algorithms put some restrictions on the structure of MDP (e.g. ergodicity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b95">96]</ref> or communicating property). The closest line of work to this work is to use average reward objective for safe RL. The work <ref type="bibr" target="#b81">[82]</ref> proposes a model-based RL algorithm for maximizing average reward objective with safety constraint for communicating MDPs. It is worth noting that in multichain setting, the state-of-the-art learning algorithms use model-based RL algorithms. The work <ref type="bibr" target="#b49">[50]</ref> studies satisfaction of ùúî-regular specifications using data-driven approaches. The authors introduce an algorithm where the optimality of the policy is conditioned to not leaving the corresponding maximal end component which leads to a sub-optimal solution. The authors provide PAC analysis for the algorithm as well.</p><p>Multi-objective Probabilistic Verification. Chatterjee, Majumadar, and Henzinger <ref type="bibr" target="#b16">[17]</ref> considered MDPs with multiple discounted reward objectives. In the presence of multiple objectives, the trade-offs between different objectives can be characterized as Pareto curves. The authors of <ref type="bibr" target="#b16">[17]</ref> showed that every Pareto optimal point can be achieved by a memoryless strategy and the Pareto curve can be approximated in polynomial time. Moreover, the problem of checking the existence of a strategy that realizes a value vector can be decided in polynomial time. These multi-objective optimization problems were studied in the context of multiple long-run average objectives by Chatterjee <ref type="bibr" target="#b15">[16]</ref>. He showed that the Pareto curve can be approximated in polynomial time in the size of the MDP for irreducible MDPs and in polynomial space in the size of the MDP for general MDPs. Additionally, the problem of checking the existence of a strategy that guarantees values for different objectives to be equal to a given vector is in polynomial time for irreducible MDPs and in NP for general MDPs. Etessami et al. <ref type="bibr" target="#b23">[24]</ref> were the first to study the multi-objective model-checking problem for MDPs with ùúî-regular objectives. Given probability intervals for the satisfaction of various properties, they developed a polynomial-time (in the size of the MDP) algorithm to decide the existence of such a strategy. They also showed that, in general, such strategies may require both randomization and memory. That paper also studies the approximation of the Pareto curve with respect to a set of ùúî-regular properties in time polynomial in the size of the MDP. Forejt et al. <ref type="bibr" target="#b24">[25]</ref> studied quantitative multi-objective optimization over MDPs that combines ùúî-regular and quantitative objectives. Those algorithms are implemented in the probabilistic model checker PRISM <ref type="bibr" target="#b50">[51]</ref>. Multi-objective optimization on interval MDPs is studied by Monir et al. <ref type="bibr" target="#b62">[63]</ref> to provide a Lyapunov-based policy synthesis approach.</p><p>Multi-objective Reinforcement Learning. There has been substantial work on lexicographic objectives in RL, including lexicographic discounted objectives <ref type="bibr" target="#b83">[84]</ref>, lexicographic ùúî-regular objectives <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, and a combination of safety and discounted objectives <ref type="bibr" target="#b8">[9]</ref>. Hahn et al. <ref type="bibr" target="#b35">[36]</ref> considered the general class of ùúî-regular objectives with discounted rewards in model-free RL. However, to the best of our knowledge, this paper is the first work that considers lexicographic ùúî-regular objectives with average objectives.</p><p>Average-Reward RL for Formal Specifications. Despite significant progress in data-driven approaches for satisfying ùúî-regular specifications, there remains a gap in the use of average-reward, model-free RL algorithms for satisfying temporal logic specifications. Our preliminary work presented at AAMAS 2022 <ref type="bibr" target="#b44">[45]</ref> aimed to address this gap by proposing a model-free average-reward RL algorithm tailored to a subclass of LTL specifications known as absolute liveness specifications. We argue that this subclass captures a broad range of practically relevant specifications and is particularly well-suited to the average-reward RL setting. This manuscript builds on <ref type="bibr" target="#b44">[45]</ref> and extends the results in the following directions: (a) we generalize our results from communicating MDPs to the broader class of weakly communicating MDPs; (b) we incorporate a lexicographic multi-objective optimization framework, wherein the goal is to maximize a mean-payoff objective among policies that satisfy a given liveness specification; (c) we provide novel automata-theoretic characterizations of absolute liveness and stable specifications; and (d) we conduct an experimental evaluation demonstrating the effectiveness of the proposed lexicographic multi-objective approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This work addressed the problem of synthesizing policies that satisfy a given absolute liveness ùúî-regular specification while maximizing a mean-payoff objective in the continuing setting. Our first key contribution is a model-free translation from ùúî-regular specifications to an average-reward objective, enabling the use of off-the-shelf average-reward reinforcement learning algorithms. In contrast to existing approaches that rely on discounted, episodic learning, which require environment resets and may be infeasible in many real-world settings, our approach learns optimal policies in a single, life-long episode without resetting.</p><p>Our second contribution is a solution to a lexicographic multi-objective optimization problem, where the goal is to maximize a mean-payoff objective among the set of policies that satisfy a given liveness specification. We provide convergence guarantees under the assumption that the underlying Markov Decision Process is communicating. Importantly, our solutions are model-free and do not require access to the environment's transition structure or its graph representation. This removes the common assumption in prior work that synthesis requires the computation of end components in the product MDP.</p><p>We implemented our approach using Differential Q-learning and evaluated it on a range of case studies. The experimental results demonstrate that our method reliably converges to optimal strategies under the stated assumptions and outperforms existing approaches in the continuing setting. These results support the important and often overlooked hypothesis that average-reward RL is better suited to continuing tasks than discounted RL. As future work, we plan to explore the use of function approximation, with the aim of enabling average-reward RL to achieve the same level of success for continuing tasks as discounted RL has achieved in episodic settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 2 . 2 (</head><label>22</label><figDesc>Probabilistic Reward Machines). A probabilistic reward machine is a tuple R = (Œ£ ùúñ , ùëà √ó ùëà ùëù , (ùë¢ 0 , ùë¢ ùëù 0 ), ùõø ùëü ,ùëá ùëù , ùúå) where ‚Ä¢ Œ£ ùúñ = (Œ£ ‚à™ {ùúñ}) with Œ£ being a finite alphabet and ùúñ indicating a silent transition, ‚Ä¢ ùëà and ùëà ùëù are two finite sets of states, ‚Ä¢ (ùë¢ 0 , ùë¢ ùëù 0 ) ‚àà ùëà √ó ùëà ùëù is the starting state,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>JAIR, Vol. 1, Article . Publication date: May 2025.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Automaton of F G ùëé, dashed lines represent resets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Lemma 4 . 6 (</head><label>46</label><figDesc>Preservation of Weak Communication). For a weakly communicating MDP M and reward machine R A for a fairness GFM automaton A, the product M√óR A is weakly communicating.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 ùõΩ(Fig. 5 .Fig. 4 .</head><label>154</label><figDesc>Fig. 5. Picture of the construction of the probabilistic reward machine in (5.1). Left: Two layers corresponding to ùëè ‚àà {0, 1}. Right: Probabilistic changes of the additional bit ùëè. The ùúñ-transitions are excluded for a better pictorial presentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( 1 ‚àí 2 )</head><label>12</label><figDesc>lim ùëñ‚Üí‚àû Rùúé * (ùõΩ (ùëñ)) ‚â• lim ùëñ‚Üí‚àû ùõΩ (ùëñ))ùë£ * + ùõΩ (ùëñ)ùëè ùëì ùúé * ( 1 = ùë£ * , so ùúé * is a lexicographically optimal policy. ‚ñ° These results can be extended to weakly communicating MDPs in straightforward fashion. Lemma 5.5 (Preservation of Communication). For a weakly-communicating MDP M and reward machine R A‚Ä¢ùúå over a fairness specification, the resulting product M √ó R A‚Ä¢ùúå is weakly-communicating. Proof. The proof follows directly from Lemma 4.6 and Lemma 5.1. As stated for Lemma 4.6 we can partition the states of the MDP into two sets and the sub-product MDP resulted from the second set with the fairness specification is communicating based on Lemma 5.1. Moreover, since all of the states of the first set, see the proof of Lemma 4.6, are transient the resulting product MDP is weakly communicating. ‚ñ°</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>a d v e r s e f r o z e n S m a l l f r o z e n L a r g e w i n d y w i n d y S t o c h g r i d 5 x 5 i s h i f t d o u b l e g r i d b u s y R i n</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Comparison of the distributions of probability of satisfaction of learned policies across sampled hyperparameters in the continuing setting. For each distribution, the mean is shown as a circle, and the maximum and minimum are shown as vertical bars. We compare our proposed reduction, the reduction of<ref type="bibr" target="#b31">[32]</ref> with Q-learning, and the reduction of<ref type="bibr" target="#b10">[11]</ref> with Q-learning. Episodic resetting is not used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Learning results and comparison. Hyperparameters used for our reduction are shown. Blank entries indicate that default values are used. The default parameters are ùëê = ‚àí1, ùúÄ = 0.1, ùõº = 0.1, and ùúÇ = 0.1. Times are in seconds. Superscript ‚Ä†</figDesc><table><row><cell>Name adverse frozenSmall frozenLarge windy windyStoch grid5x5 ishift doublegrid busyRingMC2 busyRingMC4 2592 15426 6.08 states prod. time time  ‚Ä† time  ‚Ä° 202 507 8.51 7.09 12.56 -150 ùëê 16 64 0.99 20.23 9.88 64 256 4.07 3.88 8.79 123 366 1.40 1.81 2.61 130 390 2.97 3.91 2.53 25 100 0.62 1.12 1.02 4 29 0.03 0.01 0.02 1296 5183 16.43 3.45 3.09 -2 72 288 0.03 0.03 0.03 3.94 2.33</cell><cell>ùúÄ 0.95 0.5 0.05 ùõº ùúÇ 0.2 0.02 0.02 0.5 0.5 0.5 0.05 0.01 0.01 0.01</cell><cell>train-steps 10M 500k 3M 1M 2M 200k 10k 12M 10k 1.5M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Learning results for multi-objective case studies. The default parameters are tol = 0, ùõΩ = 0.05, ep-n = 1, ep-l = 1000000, ùõº = 0.01, ùúÇ = 0.01, ùúÄ = 0.1. Times are in seconds. All hyperparameters are tuned by hand.Learning for Formal Specifications over Finite Traces. The development and use of formal reward structures for RL have witnessed increased interest in recent years. For episodic RL, logics have been developed over finite traces of the agent's behavior, including LTL ùëì and Linear Dynamic Logic (LDL ùëì )</figDesc><table><row><cell>7 Related Work</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">‚Ä¢ Kazemi et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">JAIR, Vol. 1, Article . Publication date: May 2025.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2">The implementation is available at https://plv.colorado.edu/mungojerrie/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3">Case studies are available at https://plv.colorado.edu/mungojerrie/aamas22.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research of S. Soudjani is supported by the following grants: EIC 101070802 and ERC 101089047. This research was also supported in part by the National Science Foundation (NSF) through CAREER Award CCF-2146563. Ashutosh Trivedi holds the position of Royal Society Wolfson Visiting Fellow and gratefully acknowledges the support of the Wolfson Foundation and the Royal Society for this fellowship.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ùõø ‚Ä≤ ((ùëû, ùëè),(ùëô, ùë†, ùë† ‚Ä≤ , ùëû ‚Ä≤ )) ((ùëû ‚Ä≤ , ùëè</p><p>ùõΩ ùëè = 0, ùëè ‚Ä≤ = 1, ùëû ‚Ä≤ ‚àà ùõø (ùëû, ùëô), ùëô ‚àâ {ùúñ 1 , ùúñ 2 } 1 ‚àí ùõΩ ùëè = 0, ùëè ‚Ä≤ = 0, ùëû ‚Ä≤ ‚àà ùõø (ùëû, ùëô), ùëô ‚àâ {ùúñ 1 , ùúñ 2 } 1 ùëè = 1, ùëè ‚Ä≤ = 1, ùëû ‚Ä≤ ‚àà ùõø (ùëû, ùëô), (ùëû, ùëô, ùëû ‚Ä≤ ) ‚àâ ùêπ, ùëô ‚àâ {ùúñ 1 , ùúñ 2 } 1 ùëè = 1, ùëè ‚Ä≤ = 0, ùëû ‚Ä≤ ‚àà ùõø (ùëû, ùëô), (ùëû, ùëô, ùëû ‚Ä≤ ) ‚àà ùêπ, ùëô ‚àâ {ùúñ 1 , ùúñ 2 } 1 ùëû ‚â† ùëû 0 , ùëû ‚Ä≤ = ùëû 0 , ùëè ‚Ä≤ = ùëè, ùëô = ùúñ 1 1 ùëû ‚Ä≤ = ùëû, ùëè = 1, ùëè ‚Ä≤ = 0, ùëô = ùúñ 2 0 otherwise <ref type="bibr">(5.1)</ref> and ùúå ‚Ä≤ ((ùëû, ùëè), (ùëô, ùë†, ùë† ‚Ä≤ , ùëû ‚Ä≤ ), (ùëû ‚Ä≤ , ùëè ‚Ä≤ ))</p><p>We now show a few results before proceeding to the main theorems.</p><p>Lemma 5.1 (Preservation of Communication). For a communicating MDP M and reward machine R A‚Ä¢ùúå defined above, the resulting product M √ó R A‚Ä¢ùúå is communicating.</p><p>Proof. The claim holds due to the additions of ùúñ-transitions {ùúñ 1 , ùúñ 2 } that resets both the automaton (ùúñ 1 ) and the extra bit ùëè to zero (ùúñ 2 ). ‚ñ° To develop our formal proof in Theorem 5.3, we make use of the following observation. Lemma 5.2 (Binary Probability of Satisfaction). The probability of satisfaction of an absolute liveness specification is either 0 or 1 in a communicating MDP.</p><p>Proof. To show this result, we just need to show that if the probability of satisfaction of the specification is positive, then the probability of satisfaction must be 1. This follows from the fact that if the probability of satisfaction is positive, then there is a winning end-component, and this end-component can be reached with probability 1 due to the communicating nature of the product. ‚ñ° Theorem 5.3 (Lexicographic Reward Machine ùúÄ-optimality). Let ùë£ * be the external average reward obtained under a lexicographically optimal strategy for Problem 2.8. For every ùúÄ &gt; 0, there exists a threshold ùõΩ * &gt; 0 such that for all 0 &lt; ùõΩ &lt; ùõΩ * there exists a ùëê * &lt; 0 such that for all ùëê &lt; ùëê * all positional policies that maximize the average reward on M √ó R A‚Ä¢ùúå maximize the probability of satisfaction of A on M and achieves an average reward ùë£ such that ùë£ ‚â• ùë£ * ‚àí ùúÄ. We call such a policy ùúÄ-optimal. Additionally, these policies are finite memory policies on M.</p><p>Proof. We first note that since the memory required in the resulting policies is defined by R A‚Ä¢ùúå with fixed parameters, the resulting policies are finite memory. From Lemma 5.2, we can proceed by considering two cases: the ùúî-regular objective is satisfied with probability 0 or 1.</p><p>(1) We first consider the case where the ùúî-regular objective is satisfied with probability 0 under any policy, i.e., there is no policy that can satisfy the specification with positive probability. We will show that any 0 &lt; ùõΩ * &lt; 1 and ùëê * &lt; 0 works. Since the lexicographically optimal policy only needs to maximize the external average reward, there is an optimal policy that is positional on M that achieves an external average reward ùë£ * . Consider an optimal policy ùúé on the extended product M √ó R </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Reproducibility Checklist for JAIR</head><p>Select the answers that apply to your research -one per item.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All articles:</head><p>( Articles reporting on computational experiments:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does this paper include computational experiments? [yes]</head><p>If yes, please complete the list below.</p><p>(1) All source code required for conducting experiments is included in an online appendix or will be made publicly available upon publication of the paper. The online appendix follows best practices for source code readability and documentation as well as for long-term accessibility.</p><p>[yes] (2) The source code comes with a license that allows free usage for reproducibility purposes. [yes] (3) The source code comes with a license that allows free usage for research purposes in general. [yes] (4) Raw, unaggregated data from all experiments is included in an online appendix or will be made publicly available upon publication of the paper. The online appendix follows best practices for long-term accessibility.</p><p>[NA] (5) The unaggregated data comes with a license that allows free usage for reproducibility purposes. [NA] <ref type="bibr" target="#b5">(6)</ref> The unaggregated data comes with a license that allows free usage for research purposes in general. [NA] ( <ref type="formula">7</ref>) If an algorithm depends on randomness, then the method used for generating random numbers and for setting seeds is described in a way sufficient to allow replication of results.</p><p>[yes] (8) The execution environment for experiments, the computing infrastructure (hardware and software) used for running them, is described, including GPU/CPU makes and models; amount of memory (cache and RAM); make and version of operating system; names and versions of relevant software libraries and frameworks.</p><p>[yes] (9) The evaluation metrics used in experiments are clearly explained and their choice is explicitly motivated.</p><p>[yes] (10) The number of algorithm runs used to compute each result is reported. [yes] (11) Reported results have not been "cherry-picked" by silently ignoring unsuccessful or unsatisfactory experiments.</p><p>[yes] (12) Analysis of results goes beyond single-dimensional summaries of performance (e.g., average, median) to include measures of variation, confidence, or other distributional information.</p><p>[yes] (13) All (hyper-) parameter settings for the algorithms/methods used in experiments have been reported, along with the rationale or method for determining them. (1) All newly introduced data sets are included in an online appendix or will be made publicly available upon publication of the paper. The online appendix follows best practices for long-term accessibility with a license that allows free usage for research purposes. [yes/partially/no/NA] (2) The newly introduced data set comes with a license that allows free usage for reproducibility purposes.</p><p>[yes/partially/no] (3) The newly introduced data set comes with a license that allows free usage for research purposes in general.</p><p>[yes/partially/no] (4) All data sets drawn from the literature or other public sources (potentially including authors <ref type="bibr">'</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Politex: Regret bounds for policy iteration using expert prediction</title>
		<author>
			<persName><forename type="first">Yasin</forename><surname>Abbasi-Yadkori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kush</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gell√©rt</forename><surname>Weisz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3692" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Defining Liveness</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Process. Lett</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="181" to="185" />
			<date type="published" when="1985-10">1985. Oct. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Value iteration for long-run average reward in Markov decision processes</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnendu</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Przemys≈Çaw</forename><surname>Daca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Aided Verification</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017-01">Jan K≈ôet√≠nsk·ª≥, and Tobias Meggendorfer. 2017</date>
			<biblScope unit="page" from="201" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Steady-state planning in expected reward multichain mdps</title>
		<author>
			<persName><forename type="first">Andre</forename><surname>George K Atia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ismail</forename><surname>Beckus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Alkhouri</surname></persName>
		</author>
		<author>
			<persName><surname>Velasquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="1029" to="1082" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Principles of Model Checking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Katoen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambuj</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><surname>Tewari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1205.2661</idno>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Formal methods for control synthesis: An optimization perspective</title>
		<author>
			<persName><forename type="first">Calin</forename><surname>Belta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadra</forename><surname>Sadraddini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics, and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="115" to="140" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>Annual Review of Control</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Stochastic hybrid systems: theory and safety critical applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Henk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Blom</surname></persName>
		</author>
		<author>
			<persName><surname>Lygeros</surname></persName>
		</author>
		<author>
			<persName><surname>Everdij</surname></persName>
		</author>
		<author>
			<persName><surname>Loizou</surname></persName>
		</author>
		<author>
			<persName><surname>Kyriakopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">337</biblScope>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model-Free Learning of Safe yet Effective Controllers</title>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Alper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Bozkurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Pajic</surname></persName>
		</author>
		<idno type="DOI">10.1109/CDC45484.2021.9683634</idno>
		<ptr target="https://doi.org/10.1109/CDC45484.2021.9683634" />
	</analytic>
	<monogr>
		<title level="m">2021 60th IEEE Conference on Decision and Control (CDC)</title>
				<meeting><address><addrLine>Austin, TX, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-12-14">2021. December 14-17, 2021</date>
			<biblScope unit="page" from="6560" to="6565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Control synthesis from linear temporal logic specifications using model-free reinforcement learning</title>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Alper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Bozkurt</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Michael M Zavlanos</surname></persName>
		</author>
		<author>
			<persName><surname>Pajic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10349" to="10355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Control synthesis from linear temporal logic specifications using model-free reinforcement learning</title>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Alper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Bozkurt</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Michael M Zavlanos</surname></persName>
		</author>
		<author>
			<persName><surname>Pajic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10349" to="10355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Verification of Markov decision processes using learning algorithms</title>
		<author>
			<persName><forename type="first">Tom√°≈°</forename><surname>Br√°zdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnendu</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Chmelik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vojtƒõch</forename><surname>Forejt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>K≈ôet√≠nsk·ª≥</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Ujma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automated Technology for Verification and Analysis (ATVA)</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="98" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Synthesis of LTL formulas from natural language texts: State of the art and research directions</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Brunello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelo</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th International Symposium on Temporal Representation and Reasoning</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finite LTL synthesis as planning</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Camacho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><forename type="middle">A</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Muise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheila</forename><forename type="middle">A</forename><surname>Mcilraith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth International Conference on Automated Planning and Scheduling</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">LTL and Beyond: Formal Languages for Reward Function Specification in Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Camacho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Toro Icarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Anthony</forename><surname>Toryn Q Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheila</forename><forename type="middle">A</forename><surname>Valenzano</surname></persName>
		</author>
		<author>
			<persName><surname>Mcilraith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="6065" to="6073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Markov Decision Processes with Multiple Long-Run Average Objectives</title>
		<author>
			<persName><forename type="first">Krishnendu</forename><surname>Chatterjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FSTTCS 2007: Foundations of Software Technology and Theoretical Computer Science</title>
				<editor>
			<persName><forename type="first">Arvind</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sanjiva</forename><surname>Prasad</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="473" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Markov decision processes with multiple objectives</title>
		<author>
			<persName><forename type="first">Krishnendu</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rupak</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Henzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual symposium on theoretical aspects of computer science</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="325" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Formal verification of probabilistic systems</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfaro</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Ph. D. Dissertation. Stanford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Foundations for restraining bolts: Reinforcement learning with LTLf/LDLf restraining specifications</title>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giacomo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Iocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Favorito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Patrizi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Automated Planning and Scheduling</title>
				<meeting>the International Conference on Automated Planning and Scheduling</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="128" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Synthesis for LTL and LDL on finite traces</title>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giacomo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Vardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Average-reward model-free reinforcement learning: a systematic review and literature mapping</title>
		<author>
			<persName><forename type="first">Vektor</forename><surname>Dewanto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Eshragh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Roosta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08920</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimal control of Markov decision processes with linear temporal logic constraints</title>
		<author>
			<persName><forename type="first">Xuchu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calin</forename><surname>Belta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Control</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="1244" to="1257" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Exploration-exploitation in constrained MDPs</title>
		<author>
			<persName><forename type="first">Yonathan</forename><surname>Efroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Pirotta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02189</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-objective Model Checking of Markov Decision Processes</title>
		<author>
			<persName><forename type="first">K</forename><surname>Etessami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Vardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yannakakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tools and Algorithms for the Construction and Analysis of Systems</title>
				<editor>
			<persName><forename type="first">Orna</forename><surname>Grumberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Huth</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="50" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Quantitative Multi-objective Verification for Probabilistic Systems</title>
		<author>
			<persName><forename type="first">Marta</forename><surname>Vojtech Forejt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gethin</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tools and Algorithms for the Construction and Analysis of Systems</title>
				<editor>
			<persName><forename type="first">Parosh</forename><surname>Aziz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Abdulla</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Rustan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Leino</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="112" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Probably Approximately Correct MDP Learning and Control With Temporal Logic Constraints</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ufuk</forename><surname>Topcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Robotics: Science and Systems</title>
				<meeting>Robotics: Science and Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reinforcement Learning with Non-Markovian Rewards</title>
		<author>
			<persName><forename type="first">Maor</forename><surname>Gaon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Brafman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3980" to="3987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A comprehensive survey on safe reinforcement learning</title>
		<author>
			<persName><forename type="first">Javier</forename><surname>Garcƒ±a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Fern√°ndez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1437" to="1480" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Formal Multi-Objective Synthesis of Continuous-State MDPs</title>
		<author>
			<persName><forename type="first">Sofie</forename><surname>Haesaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petter</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Systems Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1765" to="1770" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust dynamic programming for temporal logic control of stochastic systems</title>
		<author>
			<persName><forename type="first">Sofie</forename><surname>Haesaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Control</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="2496" to="2511" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lazy Probabilistic Model Checking without Determinisation</title>
		<author>
			<persName><forename type="first">Ernst</forename><surname>Moritz Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Schewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Turrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Concurrency Theory (CONCUR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="354" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Omega-regular objectives in model-free reinforcement learning</title>
		<author>
			<persName><forename type="first">Ernst</forename><surname>Moritz Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Schewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Somenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Wojtczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS)</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="395" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Good-for-MDPs automata for probabilistic analysis and reinforcement learning</title>
		<author>
			<persName><forename type="first">Ernst</forename><surname>Moritz Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Schewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Somenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Wojtczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Tools and Algorithms for the Construction and Analysis of Systems</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="306" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Model-Free Reinforcement Learning for Lexicographic Omega-Regular Objectives</title>
		<author>
			<persName><forename type="first">Ernst</forename><surname>Moritz Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Schewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Somenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Wojtczak</surname></persName>
		</author>
		<idno>FM 2021. 142-159. LNCS 13047</idno>
	</analytic>
	<monogr>
		<title level="m">Formal Methods</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Model-free reinforcement learning for lexicographic omega-regular objectives</title>
		<author>
			<persName><forename type="first">Ernst</forename><surname>Moritz Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Schewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Somenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Wojtczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International symposium on formal methods</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="142" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Omega-regular reward machines</title>
		<author>
			<persName><forename type="first">Ernst</forename><surname>Moritz Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Schewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Somenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Wojtczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI 2023</title>
				<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="972" to="979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Certified reinforcement learning with logic guidance</title>
		<author>
			<persName><forename type="first">Mohammadhosein</forename><surname>Hasanbeig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Abate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kroening</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00778</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reinforcement learning for temporal logic control synthesis with probabilistic satisfaction guarantees</title>
		<author>
			<persName><forename type="first">Mohammadhosein</forename><surname>Hasanbeig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiannis</forename><surname>Kantaros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Abate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kroening</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">J</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Insup</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Decision and Control (CDC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5338" to="5343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Using reward machines for high-level task specification and decomposition in reinforcement learning</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Toro Icarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toryn</forename><surname>Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Valenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheila</forename><surname>Mcilraith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2107" to="2116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Using reward machines for high-level task specification and decomposition in reinforcement learning</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Toro Icarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toryn</forename><surname>Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Valenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheila</forename><surname>Mcilraith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2107" to="2116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Toro Icarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toryn</forename><forename type="middle">Q</forename><surname>Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Anthony</forename><surname>Valenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheila</forename><forename type="middle">A</forename><surname>Mcilraith</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.03950" />
		<title level="m">Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning. CoRR abs/2010</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page">3950</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Formal synthesis of stochastic systems via control barrier certificates</title>
		<author>
			<persName><forename type="first">Pushpak</forename><surname>Jagtap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Majid</forename><surname>Zamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Control</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Temporal-Logic-Based Reward Shaping for Continuing Reinforcement Learning Tasks</title>
		<author>
			<persName><forename type="first">Yuqian</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suda</forename><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ufuk</forename><surname>Topcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<publisher>Good Systems-Published Research</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Data-driven abstraction-based control synthesis</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rupak</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Salamati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wooding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Analysis: Hybrid Systems</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">101467</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Translating omega-regular specifications to average objectives for model-free reinforcement learning</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Somenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Velasquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems</title>
				<meeting>the 21st International Conference on Autonomous Agents and Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="732" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Assume-guarantee reinforcement learning</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Somenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Velasquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="21223" to="21231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Formal policy synthesis for continuous-state systems via reinforcement learning</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Integrated Formal Methods</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Data-Driven Distributionally Robust Control for Interacting Agents under Logical Constraints</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Bahari Kordabad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleftherios</forename><forename type="middle">E</forename><surname>Vlahakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Lindemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Gros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimos</forename><forename type="middle">V</forename><surname>Dimarogonas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control (under review</title>
		<imprint>
			<date type="published" when="2025">2025. 2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Synthesis for robots: Guarantees and feedback for robot behavior</title>
		<author>
			<persName><forename type="first">Hadas</forename><surname>Kress-Gazit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morteza</forename><surname>Lahijanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasumathi</forename><surname>Raman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics, and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="211" to="236" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>Annual Review of Control</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Finite-Memory Near-Optimal Learning for Markov Decision Processes with Long-Run Average Reward</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kret√≠nsk√Ω</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1149" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">PRISM 4.0: Verification of Probabilistic Real-time Systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Aided Verification (CAV)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6806</biblScope>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Formal controller synthesis for continuous-space MDPs via model-free reinforcement learning</title>
		<author>
			<persName><forename type="first">Abolfazl</forename><surname>Lavaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Somenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Majid</forename><surname>Zamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Cyber-Physical Systems (ICCPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="98" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Automated verification and synthesis of stochastic hybrid systems: A survey</title>
		<author>
			<persName><forename type="first">Abolfazl</forename><surname>Lavaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Abate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Majid</forename><surname>Zamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page">110617</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Average reward reinforcement learning: Foundations, algorithms, and empirical results</title>
		<author>
			<persName><surname>Sridhar Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="159" to="195" />
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Symbolic Qualitative Control for Stochastic Systems via Finite Parity Games</title>
		<author>
			<persName><forename type="first">Rupak</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Mallik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne-Kathrin</forename><surname>Schmuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IFAC-PapersOnLine</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="127" to="132" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Symbolic Controller Synthesis for B√ºChi Specifications on Stochastic Systems</title>
		<author>
			<persName><forename type="first">Rupak</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Mallik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hybrid Systems: Computation and Control (HSCC) (Sydney, New South Wales, Australia)</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Article 14, 11 pages</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Regret-Free Reinforcement Learning for LTL Specifications</title>
		<author>
			<persName><forename type="first">Rupak</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Salamati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2025">2025. 2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Necessary and Sufficient Certificates for Almost Sure Reachability</title>
		<author>
			<persName><forename type="first">Rupak</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Vr Sathiyanarayana</surname></persName>
		</author>
		<author>
			<persName><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Systems Letters</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A hierarchy of temporal properties</title>
		<author>
			<persName><forename type="first">Zohar</forename><surname>Manna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Pnueli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth annual ACM symposium on Principles of distributed computing</title>
				<meeting>the ninth annual ACM symposium on Principles of distributed computing</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="377" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Resource management with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Hongzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishai</forename><surname>Menache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikanth</forename><surname>Kandula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM workshop on hot topics in networks</title>
				<meeting>the 15th ACM workshop on hot topics in networks</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="50" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Azade Nova, et al. 2021. A graph placement methodology for fast chip design</title>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Yazgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><forename type="middle">Wenjie</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ebrahim</forename><surname>Songhori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young-Joon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Pathak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">594</biblScope>
			<biblScope unit="page" from="207" to="212" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Human-level control through reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015-02">2015. Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Lyapunov-Based Policy Synthesis for Multi-Objective Interval MDPs</title>
		<author>
			<persName><forename type="first">Negar</forename><surname>Monir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Sch√∂n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IFAC-PapersOnLine</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Discounted Reinforcement Learning is Not an Optimization Problem</title>
		<author>
			<persName><forename type="first">R</forename><surname>Abhishek Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niko</forename><surname>Shariff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yasui</surname></persName>
		</author>
		<author>
			<persName><surname>Sutton</surname></persName>
		</author>
		<idno>abs/1910.02140</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Sadegh Soudjani, and Alessandro Abate. 2025. Data-Driven Yet Formal Policy Synthesis for Stochastic Nonlinear Dynamical Systems. Learning for Decision and Control</title>
		<author>
			<persName><forename type="first">Mahdi</forename><surname>Nazeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thom</forename><surname>Badings</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Advice-Guided Reinforcement Learning in a non-Markovian Environment</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Neider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Raphael</forename><surname>Gaglione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Gavran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ufuk</forename><surname>Topcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Policy invariance under reward transformations: Theory and application to reward shaping</title>
		<author>
			<persName><forename type="first">Daishi</forename><surname>Andrew Y Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="278" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Reinforcement learning of control policy for linear temporal logic specifications using limit-deterministic B√ºchi automata</title>
		<author>
			<persName><forename type="first">Ryohei</forename><surname>Oura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ami</forename><surname>Sakakibara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshimitsu</forename><surname>Ushio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Systems Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="761" to="766" />
			<date type="published" when="2020-07">2020. July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Markov Decision Processes: Discrete Stochastic Dynamic Programming</title>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1st ed.</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A tour of reinforcement learning: The view from continuous control</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics, and Autonomous Systems</title>
		<imprint>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>Annual Review of Control</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A learning based approach to control synthesis of Markov decision processes for linear temporal logic specifications</title>
		<author>
			<persName><forename type="first">Dorsa</forename><surname>Sadigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Coogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shankar Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjit</forename><forename type="middle">A</forename><surname>Seshia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Decision and Control</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1091" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Temporal logic resilience for dynamical systems</title>
		<author>
			<persName><forename type="first">Adnane</forename><surname>Saoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushpak</forename><surname>Jagtap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Atuomatic Control (under review</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Data-Driven Abstractions via Binary-Tree Gaussian Processes for Formal Verification</title>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Sch√∂n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shammakh</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wooding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IFAC-PapersOnLine</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="115" to="122" />
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Correct-by-Design Control of Parametric Stochastic Systems</title>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Sch√∂n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Birgit</forename><surname>Van Huijgevoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sofie</forename><surname>Haesaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">61st IEEE Conference on Decision and Control (CDC)</title>
				<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Data-Driven Distributionally Robust Safety Verification Using Barrier Certificates and Conditional Mean Embeddings</title>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Sch√∂n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengang</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2024 American Control Conference (ACC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="3417" to="3423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Mastering atari, Go, chess and shogi by planning with a learned model</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">588</biblScope>
			<biblScope unit="page" from="604" to="609" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Limit-deterministic B√ºchi automata for linear temporal logic</title>
		<author>
			<persName><forename type="first">Salomon</forename><surname>Sickert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Esparza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Jaax</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Aided Verification (CAV)</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">Jan K≈ôet√≠nsk·ª≥. 2016</date>
			<biblScope unit="page" from="312" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016-01">2016. Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="page" from="1140" to="1144" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Mastering the game of Go without human knowledge</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="page">354</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Learning in Markov decision processes under constraints</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Ness</surname></persName>
		</author>
		<author>
			<persName><surname>Shroff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12435</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Safety, liveness and fairness in temporal logic</title>
		<author>
			<persName><forename type="first">Sistla</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Formal Aspects of Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="495" to="511" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<author>
			<persName><forename type="first">Joar</forename><surname>Skalse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Abate</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.13769</idno>
		<title level="m">Lexicographic multi-objective reinforcement learning</title>
				<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">PAC model-free reinforcement learning</title>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Alexander L Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wiewiora</surname></persName>
		</author>
		<author>
			<persName><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><surname>Michael L Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
				<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="881" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Model-based average reward reinforcement learning</title>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dokyeong</forename><surname>Ok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="177" to="224" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for robotics: A survey of real-world successes</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Abbatematteo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Mart√≠n-Mart√≠n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2025">2025</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="28694" to="28698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Learning reward machines for partially observable reinforcement learning</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Toro Icarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Waldie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toryn</forename><surname>Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick</forename><surname>Valenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margarita</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheila</forename><surname>Mcilraith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="15523" to="15534" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Automatic verification of probabilistic concurrent finite state programs</title>
		<author>
			<persName><forename type="first">Moshe</forename><forename type="middle">Y</forename><surname>Vardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th Annual Symposium on Foundations of Computer Science (SFCS 1985)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="327" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Dynamic Automaton-Guided Reward Shaping for Monte Carlo Tree Search</title>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Velasquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brett</forename><surname>Bissey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Beckus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ismail</forename><surname>Alkhouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Melcer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Atia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="12015" to="12023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">On ùúî-Regular Sets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="177" />
			<date type="published" when="1979-11">1979. Nov. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16318</idno>
		<title level="m">Learning and Planning in Average-Reward Markov Decision Processes</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Learning and planning in average-reward Markov decision processes</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10653" to="10662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">On Convergence of Average-Reward Off-Policy Control Algorithms in Weakly Communicating MDPs</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OPT 2022: Optimization for Machine Learning</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>NeurIPS 2022 Workshop</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Model-free reinforcement learning in infinite-horizon average-reward Markov decision processes</title>
		<author>
			<persName><forename type="first">Chen-Yu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><forename type="middle">Jafarnia</forename><surname>Jahromi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haipeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiteshi</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10170" to="10180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Joint inference of reward machines and policies for reinforcement learning</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Gavran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yousef</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rupak</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Neider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ufuk</forename><surname>Topcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Automated Planning and Scheduling</title>
				<meeting>the International Conference on Automated Planning and Scheduling</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="590" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Active finite reward automaton inference and reinforcement learning using queries and counterexamples</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ojha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Neider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ufuk</forename><surname>Topcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Cross-Domain Conference for Machine Learning and Knowledge Extraction</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="115" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Predictive runtime monitoring for linear stochastic systems and applications to geofence enforcement for UAVs</title>
		<author>
			<persName><forename type="first">Hansol</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Frew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Sankaranarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Runtime Verification</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="349" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Sim-to-real transfer in deep reinforcement learning for robotics: a survey</title>
		<author>
			<persName><forename type="first">Wenshuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><forename type="middle">Pe√±a</forename><surname>Queralta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomi</forename><surname>Westerlund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE symposium series on computational intelligence (SSCI)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Complex formalism, such as definitions or proofs, is motivated and explained clearly. [yes] (5) The use of mathematical notation and formalism serves the purpose of enhancing clarity and precision; gratuitous use of mathematical formalism</title>
		<imprint/>
	</monogr>
	<note>i.e., use that does not enhance clarity or precision) is avoided</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
