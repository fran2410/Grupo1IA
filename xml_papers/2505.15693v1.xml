<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives MILAD KAZEMI, King&apos;s College London, UK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2025-05-21">21 May 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mateo</forename><surname>Perez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Milad</forename><surname>Kazemi</surname></persName>
							<email>milad.kazemi@kcl.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Fabio</forename><surname>Somenzi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
							<email>sadegh@mpi-sws.org</email>
						</author>
						<author>
							<persName><forename type="first">Ashutosh</forename><surname>Trivedi</surname></persName>
							<email>ashutosh.trivedi@colorado.edu</email>
						</author>
						<author>
							<persName><forename type="first">Alvaro</forename><surname>Velasquez</surname></persName>
							<email>velasquez@colorado.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Colorado Boulder</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">FABIO SOMENZI</orgName>
								<orgName type="institution">University of Colorado Boulder</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
								<orgName type="institution">SADEGH SOUDJANI</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">ASHUTOSH TRIVEDI</orgName>
								<orgName type="institution" key="instit2">University of Colorado Boulder</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">ALVARO VELASQUEZ</orgName>
								<orgName type="institution" key="instit2">University of Colorado Boulder</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">colorado.edu</orgName>
								<orgName type="institution">University of Colorado Boulder</orgName>
								<address>
									<settlement>Colorado</settlement>
									<region>Boulder</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">University of Colorado Boulder</orgName>
								<address>
									<settlement>Colorado</settlement>
									<region>Boulder</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
								<address>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="institution">University of Colorado Boulder</orgName>
								<address>
									<settlement>Colorado</settlement>
									<region>Boulder</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution">University of Colorado Boulder</orgName>
								<address>
									<settlement>Colorado</settlement>
									<region>Boulder</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives MILAD KAZEMI, King&apos;s College London, UK</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-05-21">21 May 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">7A96671CD0FC8AB90BA1308FD7924C0D</idno>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
					<idno type="arXiv">arXiv:2505.15693v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2025-05-26T20:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>JAIR Track: Insert JAIR Track Name Here</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in reinforcement learning (RL) have renewed focus on the design of reward functions that shape agent behavior. Manually designing reward functions is tedious and error-prone. A principled alternative is to specify behaviors in a formal language that can be automatically translated into rewards. Omega-regular languages are a natural choice for this purpose, given their established role in formal verification and synthesis. However, existing methods using omega-regular specifications typically rely on discounted reward RL in episodic settings, with periodic resets. This setup misaligns with the semantics of omega-regular specifications, which describe properties over infinite behavior traces. In such cases, the average reward criterion and the continuing setting-where the agent interacts with the environment over a single, uninterrupted lifetime-are more appropriate.</p><p>To address the challenges of infinite-horizon, continuing tasks, we focus on absolute liveness specifications-a subclass of omega-regular languages that cannot be violated by any finite behavior prefix, making them well-suited to the continuing setting. We present the first model-free RL framework that translates absolute liveness specifications to average-reward objectives. Our approach enables learning in communicating MDPs without episodic resetting. We also introduce a reward structure for lexicographic multi-objective optimization, aiming to maximize an external average-reward objective among the policies that also maximize the satisfaction probability of a given omega-regular specification. Our method guarantees convergence in unknown communicating MDPs and supports on-the-fly reductions that do not require full knowledge of the environment, thus enabling model-free RL. Empirical results show our average-reward approach in continuing setting outperforms discount-based methods across benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reinforcement learning (RL) <ref type="bibr" target="#b85">[86]</ref> is a foundational framework for sequential decision making under uncertainty, where agents learn to optimize their behavior through trial-and-error interaction with the environment. While RL has achieved superhuman performance in domains such as board games <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b78">[79]</ref><ref type="bibr" target="#b79">[80]</ref><ref type="bibr" target="#b80">[81]</ref>, robotics <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b99">100]</ref>, and resource optimization <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref>, much of this success has been in episodic tasks, i.e., settings where interactions naturally reset after a finite sequence, and learning proceeds by leveraging repeated experience across episode. In contrast, many real-world applications-such as autonomous monitoring, industrial process control, or mission planning-are inherently continuing tasks, where the agent must learn over a single, unending interaction with its environment. For such tasks, short-term rewards often fail to capture the long-term behavioral goals of interest. Instead, it is more natural to specify objectives in terms of structured, temporally extended criteria. Formal languages, particularly ğœ”-regular languages recognized by automata over infinite traces <ref type="bibr" target="#b4">[5]</ref>, provide a rich and precise way to express such long-term goals. This has led to increasing interest in integrating formal methods with RL for the synthesis of correct-by-construction policies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b54">55]</ref>, enabling logic-based specifications to guide data-driven learning. This paper explores how model-free average-reward RL, a formulation especially suited for continuing tasks <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b93">94]</ref>, can be used to synthesize policies that satisfy ğœ”-regular objectives.</p><p>Why ğœ”-Regular Objectives? While many successful applications of RL rely on reward functions that depend only on the agent's current state and action, it is often necessary-or more natural-to account for the agent's history when defining learning objectives. This becomes especially important in settings with sparse rewards <ref type="bibr" target="#b65">[66]</ref>, partial observability <ref type="bibr" target="#b88">[89]</ref>, or temporally extended goals <ref type="bibr" target="#b14">[15]</ref>. In such cases, the agent's goal is better expressed as a sequence of desirable or undesirable behaviors rather than as immediate outcomes. Formal language structuresparticularly automata over sequences of observations-have emerged as a powerful tool for specifying these non-Markovian objectives. Rooted in formal verification <ref type="bibr" target="#b89">[90]</ref>, this approach enables the use of automata with a variety of acceptance conditions to capture complex reward structures and behavioral constraints. In some cases, even natural language objectives can be systematically translated into automata representations <ref type="bibr" target="#b12">[13]</ref>.</p><p>This line of work has gained significant traction in the formal synthesis of control policies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55]</ref>, where developers define high-level goals in a formal language and synthesis algorithms compute correct-by-construction policies-eliminating the need for manual, error-prone reward engineering <ref type="bibr" target="#b48">[49]</ref>. In this paper, we adopt this paradigm to synthesize policies using model-free RL for a class of non-Markovian objectives defined by ğœ”-regular languages <ref type="bibr" target="#b4">[5]</ref>. They capture infinite sequences of semantically meaningful observations and provide a rich formalism for specifying long-term behavioral goals.</p><p>To operationalize such specifications within RL, we introduce nondeterministic reward machines, which encode the reward structure of ğœ”-regular automata. By constructing a product between the agent's environmentmodeled as a Markov Decision Process (MDP)-and the automaton, we reduce the synthesis problem to a Markovian task that can be addressed using standard model-free algorithms. This builds on the broader use of reward machines <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b40">41]</ref>, which represent reward functions as automata over histories of semantically meaningful events. These machines provide a structured and interpretable way to incorporate temporal logic into RL and enable the transformation of non-Markovian objectives into equivalent Markovian formulations over an augmented state space <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b97">98]</ref>. This transformation allows existing RL techniques to be applied to long-horizon, history-dependent tasks without modification.</p><p>Limitations of Discounted Reward in Continuing Tasks. Despite the widespread use of discounted reward formulations in RL, they are often ill-suited for continuing tasks. The discounted return, defined as the sum of future rewards scaled by a discount factor, ensures mathematical tractability by bounding the total return over infinite horizons. However, this formulation inherently prioritizes short-term gains over long-term performance, which misaligns with the objectives of many continuing systems. In practice, achieving a suitable approximation of long-run behavior requires setting the discount factor very close to one. Yet, doing so weakens the contraction properties of learning algorithms, leading to slow convergence and potential instability.</p><p>These issues are particularly pronounced in continuing environments, where no natural episode boundaries exist. While discounted RL has shown remarkable success in episodic domains <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b78">79]</ref>, its solutions are sensitive to initial state distributions-an undesirable property in continuing settings where learning should be state-agnostic over the long run. Furthermore, discounted formulations are often incompatible with function approximation techniques commonly used in large-scale RL, as highlighted by recent studies <ref type="bibr" target="#b63">[64]</ref>. Given the growing reliance on large neural architectures in modern RL, the limitations of discounted RL motivate the need for alternative formulations better aligned with the demands of continuing tasks. These limitations motivate the need to explore average-reward RL as a more principled alternative in continuing settings.</p><p>Average-Reward RL. A natural alternative to discounting in continuing environments is to optimize the agent's average reward over time. This formulation aligns more directly with long-term performance objectives, making it particularly well-suited for tasks without episodic resets. However, average-reward RL introduces unique challenges. Unlike discounted RL-where the discount factor ensures convergence through a builtin contraction-average-reward methods rely on structural properties of the underlying MDP. In particular, the convergence of model-free average-reward algorithms <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b95">96]</ref> typically requires the MDP to be communicating, meaning every state is reachable from every other state under some policy. This communicating assumption is especially important-and potentially problematic-when synthesizing policies for ğœ”-regular specifications. The synthesis process involves constructing a product MDP between the agent's environment and the automaton representing the specification. While the original MDP may satisfy the communicating property, the product MDP may not, creating a barrier to applying average-reward RL directly in such settings.</p><p>Absolute Liveness Properties. Absolute liveness specifications form an important subclass of ğœ”-regular specifications and are prevalent throughout the temporal property hierarchy <ref type="bibr" target="#b58">[59]</ref>. These specifications are prefixindependent-their satisfaction is unaffected by the addition of arbitrary finite prefixes to an infinite word-making them particularly well-suited for continuing tasks. Moreover, when the underlying MDP is communicating, absolute liveness specifications are satisfied with probability either zero or one.</p><p>A key observation is that many temporally extended objectives, including those expressed in Linear Temporal Logic (LTL), can be naturally framed in terms of absolute liveness semantics. In particular, a satisfiable LTL formula ğœ‘ is considered an absolute liveness property, if it is expressively equivalent to the formula Fğœ‘, where Fğœ‘ denotes eventual satisfaction of ğœ‘. Thus, the eventual satisfaction semantics of an arbitrary ğœ”-regular or LTL specification ğœ‘ can be captured by the absolute liveness property Fğœ‘, further motivating their use in long-horizon learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>The main contribution of this paper is an average-reward model-free reinforcement learning algorithm for synthesizing policies that satisfy a given absolute liveness ğœ”-regular specification. Our approach preserves the communicating property in the product construction, thereby enabling the learning of optimal policies without requiring episodic resetting. Although we assume the underlying MDP is communicating, a naive synchronization of the MDP with the automaton does not, in general, preserve this property. To address this, we introduce a reward machine construction and an augmented specification that guarantee the synchronized product MDP remains communicating.</p><p>To the best of our knowledge, this is the first framework that provides a formal translation from ğœ”-regular objectives to average-reward RL with convergence guarantees. Preliminary results were reported at the AAMAS conference <ref type="bibr" target="#b44">[45]</ref>; the current paper extends that work with complete proofs, generalizations (weakly communicating MDPs), and novel results on multi-objective synthesis. In particular, we explore scenarios in which multiple optimal policies satisfy the given specification. Motivated by this, we propose a lexicographic multi-objective optimization framework, where the goal is to maximize an external average-reward objective among policies that already maximize the satisfaction probability of a given ğœ”-regular specification. We establish convergence guarantees for this formulation in the setting of unknown communicating MDPs and absolute liveness objectives. Importantly, our reductions are on-the-fly and do not require full knowledge of the environment, enabling seamless integration with standard model-free RL algorithms.</p><p>This paper makes the following primary contributions:</p><p>(1) We present average-reward model-free RL algorithms for: (a) synthesizing policies that satisfy a given absolute liveness ğœ”-regular specification, and (b) solving lexicographic multi-objective optimizations in which the goal is to maximize a mean-payoff objective among the set of satisfying policies. <ref type="bibr" target="#b1">(2)</ref> We study the structure of ğœ”-regular specifications, including safety, liveness, absolute liveness, fairness, and stability. We introduce novel automata-theoretic characterizations of absolute liveness (Lemma 3.5) and stable specifications (Lemma 3.4), which may be of independent interest. <ref type="bibr" target="#b2">(3)</ref> We analyze the convergence of our algorithms and demonstrate that naive synchronization of a communicating MDP with a specification automaton can break the communicating property. To address this, we construct reward machines that preserve communication, enabling optimal learning without episodic resets. (4) Our work is the first to establish a formal reduction from ğœ”-regular objectives to average-reward RL in both single-and multi-objective settings with convergence guarantees. (5) We generalize our prior results from communicating MDPs <ref type="bibr" target="#b44">[45]</ref> to the broader class of weakly communicating MDPs, thereby extending the applicability of our framework to environments where some states may not be mutually reachable, yet still support meaningful long-run average-reward optimization. <ref type="bibr" target="#b5">(6)</ref> We implement our proposed reduction using Differential Q-learning and evaluate it on a suite of communicating MDPs with absolute liveness specifications. Unlike prior methods, our approach composes the product MDP on-the-fly without requiring episodic resetting. Empirical results show that our method reliably converges to optimal policies in the continuing setting, even when prior approaches fail due to non-communicating product MDPs. In the episodic setting, our approach remains competitive in training time while requiring no resets, highlighting its robustness and practical effectiveness.</p><p>Organization. The paper is organized as follows. Section 2 includes the preliminaries, qualitative and quantitative objectives, RL for continuing tasks, and the problem statements. Section 3 provides our results on qualitative objectives suitable for continual learning. Section 4 establishes our theoretical results on average-reward RL for qualitative objectives. Section 5 presents our algorithm for average-reward RL with lexicographic objectives, where the primary goal is to satisfy a qualitative specification and the secondary goal is to optimize a quantitative average-reward objective. In Section 6, we test the performance of our approach on different case studies and compare it against prior techniques. Section 7 discusses related work before concluding in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries and Problem Statement</head><p>This section presents the foundational concepts and formalizes the problem setting. We begin with Markov Decision Processes (MDPs) as the underlying model of the environment in Subsection 2.1. In Subsection 2.2, we introduce probabilistic reward machines, which serve as quantitative objectives for RL-based policy synthesis. Subsection 2.3 covers ğœ”-regular specifications as qualitative objectives for policy synthesis. We then formulate the formal problem statements in the context of reinforcement learning for continuing tasks in Subsection 2.4, and conclude by discussing the challenges of average-reward RL in Subsection 2.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Environment: Markov Decision Processes</head><p>Let D (ğ‘†) be the set of distributions over a given set ğ‘†.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.1 (Markov decision process (MDP)</head><p>). An MDP M is a tuple (ğ‘†, ğ‘  0 , ğ´,ğ‘‡ , ğ´ğ‘ƒ, ğ¿) where â€¢ ğ‘† is a finite set of states, with ğ‘  0 âˆˆ ğ‘† as the initial state;</p><p>â€¢ ğ´ is a finite set of actions;</p><p>â€¢ ğ‘‡ : ğ‘† Ã— ğ´ â‡ D (ğ‘†) is the probabilistic transition function;</p><p>â€¢ ğ´ğ‘ƒ is the set of atomic propositions; and</p><p>â€¢ ğ¿ : ğ‘† â†’ 2 ğ´ğ‘ƒ is the labeling function. For any state ğ‘  âˆˆ ğ‘†, we let ğ´(ğ‘ ) denote the set of actions that can be selected in state ğ‘ . An MDP is a Markov chain if ğ´(ğ‘ ) is a singleton for all ğ‘  âˆˆ ğ‘†.</p><p>Runs. For states ğ‘ , ğ‘  â€² âˆˆ ğ‘† and ğ‘ âˆˆ ğ´(ğ‘ ), ğ‘‡ (ğ‘ , ğ‘) (ğ‘  â€² ) equals the conditional probability ğ‘ (ğ‘  â€² | ğ‘ , ğ‘), which is the probability of jumping to state ğ‘  â€² from state ğ‘  under action ğ‘. A run of M is an ğœ”-word âŸ¨ğ‘  0 , ğ‘ 1 , ğ‘  1 , . . .âŸ© âˆˆ ğ‘† Ã— (ğ´Ã—ğ‘†) ğœ” such that ğ‘ (ğ‘  ğ‘–+1 | ğ‘  ğ‘– , ğ‘ ğ‘–+1 ) &gt; 0 for all ğ‘– â‰¥ 0. A finite run is a finite such sequence. For a run ğ‘Ÿ = âŸ¨ğ‘  0 , ğ‘ 1 , ğ‘  1 , . . .âŸ© we define the corresponding labeled run as ğ¿(ğ‘Ÿ ) = âŸ¨ğ¿(ğ‘  0 ), ğ¿(ğ‘  1 ), . . .âŸ© âˆˆ (2 ğ´ğ‘ƒ ) ğœ” . We write Runs M (FRuns M ) for the set of (finite) runs of the MDP M and Runs M (ğ‘ ) (FRuns M (ğ‘ )) for the set of (finite) runs of the MDP M from ğ‘ . We write last(ğ‘Ÿ ) for the last state of a finite run ğ‘Ÿ . The superscript M will be dropped when clear from context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strategies or Policies.</head><p>A strategy in M is a function ğœ : FRuns â†’ D (ğ´) such that supp(ğœ (ğ‘Ÿ )) âŠ† ğ´(last(ğ‘Ÿ )), where supp(ğ‘‘) denotes the support of the distribution ğ‘‘. A memory skeleton is a tuple ğ‘€ ğ”° = (ğ‘€, Î£, ğ‘š 0 , ğ›¼ ğ”° ) where ğ‘€ is a finite set of memory states, Î£ is a finite alphabet, ğ‘š 0 âˆˆ ğ‘€ is the initial state, and ğ›¼ ğ”° : ğ‘€ Ã— Î£ â†’ ğ‘€ is the memory update function. We define the extended memory update function Î±ğ”° : ğ‘€Ã—Î£ * â†’ ğ‘€ inductively in the usual way. A finite-memory strategy for M over a memory skeleton ğ‘€ ğ”° is a Mealy machine (ğ‘€ ğ”° , ğ›¼ ğ” ) where ğ›¼ ğ” : ğ‘†Ã—ğ‘€ â†’ D (ğ´) is the next-action function that suggests the next action based on the MDP and memory state. The semantics of a finite-memory strategy (ğ‘€, ğ›¼ ğ” ) is given as a strategy ğœ : FRuns â†’ D (ğ´) such that, for every ğ‘Ÿ âˆˆ FRuns, we have that ğœ (ğ‘Ÿ ) = ğ›¼ ğ” (last(ğ‘Ÿ ), Î±ğ”° (ğ‘š 0 , ğ¿(ğ‘Ÿ ))).</p><p>A strategy ğœ is pure if ğœ (ğ‘Ÿ ) is a point distribution for all runs ğ‘Ÿ âˆˆ FRuns M and is mixed (short for strictly mixed) if supp(ğœ (ğ‘Ÿ )) = ğ´(last(ğ‘Ÿ )) for all runs ğ‘Ÿ âˆˆ FRuns M . Let Runs M ğœ (ğ‘ ) denote the subset of runs Runs M (ğ‘ ) from initial state ğ‘  that follow strategy ğœ. Let Î  M be the set of all strategies. We say that ğœ is stationary if last(ğ‘Ÿ ) = last(ğ‘Ÿ â€² ) implies ğœ (ğ‘Ÿ ) = ğœ (ğ‘Ÿ â€² ) for all runs ğ‘Ÿ, ğ‘Ÿ â€² âˆˆ FRuns M . A stationary strategy can be given as a function ğœ : ğ‘† â†’ D (ğ´). A strategy is positional if it is both pure and stationary.</p><p>Probability Space. An MDP M under strategy ğœ results in a Markov chain M ğœ . If ğœ is a finite-memory strategy, then M ğœ is a finite-state Markov chain. The behavior of an MDP M under a strategy ğœ and starting state ğ‘  âˆˆ ğ‘† is defined on the probability space (Runs M ğœ (ğ‘ ), F Runs M ğœ (ğ‘  ) , Pr M ğœ (ğ‘ )) over the set of infinite runs of ğœ with starting state ğ‘ . F Runs M ğœ (ğ‘  ) denotes the sigma-algebra on these runs generated by the underlying MDP, and Pr M ğœ (ğ‘ ) is the probability distribution over these runs constructed inductively. Given a random variable ğ‘“ : Runs M â†’ R, we denote by E M ğœ (ğ‘ ){ğ‘“ } the expectation of ğ‘“ over the runs of M starting from ğ‘  that follow ğœ. Structural Properties of MDPs. A sub-MDP of M is an MDP M â€² = (ğ‘† â€² , ğ´ â€² ,ğ‘‡ â€² , ğ´ğ‘ƒ, ğ¿ â€² ), where ğ‘† â€² âŠ‚ ğ‘†, ğ´ â€² âŠ† ğ´ is such that ğ´ â€² (ğ‘ ) âŠ† ğ´(ğ‘ ) for all ğ‘  âˆˆ ğ‘† â€² , and ğ‘‡ â€² and ğ¿ â€² are ğ‘‡ and ğ¿ restricted to ğ‘† â€² and ğ´ â€² . An end-component <ref type="bibr" target="#b17">[18]</ref> of an MDP M is a sub-MDP M â€² that is closed under the transitions in ğ‘‡ â€² and such that for every state pair ğ‘ , ğ‘  â€² âˆˆ ğ‘† â€² there is a strategy that can reach ğ‘  â€² from ğ‘  with positive probability. A maximal end-component is an end-component that is maximal under set-inclusion. Every state ğ‘  of an MDP M belongs to at most one maximal end-component. A bottom strongly connected component (BSCC) of a Markov chain is any of its end-components.</p><p>An MDP M is communicating if it is equal to its (only) maximal end-component. An MDP M is weakly communicating if its state space can be decomposed into two sets: in the first set, each state is reachable from every other state in the set under some strategy; in the second set, all states are transient under all strategies, meaning that the probability of starting from ğ‘  in this set and returning to ğ‘  is less than one under any strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Quantitative Objectives: Probabilistic Reward Machines</head><p>In the classical RL literature, the learning objective is specified using Markovian reward functions, i.e., a function ğœŒ : ğ‘† Ã— ğ´ Ã— ğ‘† â†’ R assigning utility to state-action pairs. A rewardful MDP is a tuple M = (ğ‘†, ğ‘  0 , ğ´,ğ‘‡ , ğœŒ), where ğ‘†, ğ‘  0 , ğ´, and ğ‘‡ are defined as for MDPs, and ğœŒ is a Markovian reward function. A rewardful MDP M under a strategy ğœ determines a sequence of random rewards ğœŒ (ğ‘‹ ğ‘– âˆ’1 , ğ‘Œ ğ‘– , ğ‘‹ ğ‘– ) ğ‘– â‰¥1 , where ğ‘‹ ğ‘– and ğ‘Œ ğ‘– are the random variables denoting the ğ‘–-th state and action, respectively. For ğœ† âˆˆ [0, 1), the discounted reward under ğœ is defined as:</p><formula xml:id="formula_0">Disct (ğœ†) M ğœ (ğ‘ ) := lim ğ‘ â†’âˆ E M ğœ (ğ‘ ) ğ‘ âˆ‘ï¸ ğ‘–=1 ğœ† ğ‘– âˆ’1 ğœŒ (ğ‘‹ ğ‘– âˆ’1 , ğ‘Œ ğ‘– , ğ‘‹ ğ‘– ) ,</formula><p>while the average reward is defined as</p><formula xml:id="formula_1">Avg M ğœ (ğ‘ ) := lim sup ğ‘ â†’âˆ 1 ğ‘ E M ğœ (ğ‘ ) ğ‘ âˆ‘ï¸ ğ‘–=1 ğœŒ (ğ‘‹ ğ‘– âˆ’1 , ğ‘Œ ğ‘– , ğ‘‹ ğ‘– ) .</formula><p>For an objective Reward M âˆˆ{Disct (ğœ†) M , Avg M } and state ğ‘ , we define the optimal reward Reward M * (ğ‘ ) as sup ğœ âˆˆÎ  M Reward M ğœ (ğ‘ ). A strategy ğœ is optimal for Reward M if Reward M ğœ (ğ‘ )=Reward M * (ğ‘ ) for all ğ‘  âˆˆ ğ‘†. Optimal reward and strategies for these objectives can be computed in polynomial time <ref type="bibr" target="#b68">[69]</ref>.</p><p>Often, complex learning objectives cannot be expressed using Markovian reward functions. A recent trend is to express learning objectives using finite-state reward machines <ref type="bibr" target="#b38">[39]</ref>. For the objectives we consider, we require a more expressive variant of reward machines having probabilistic transitions, nondeterministic transitions, and spurious transitions labeled with ğœ–. We call them probabilistic reward machines with the understanding that any of these three types of transitions could be absent. For instance, we retrieve the definition of nondeterministic reward machine as in <ref type="bibr" target="#b44">[45]</ref> if any activated transition is taken with probability one. The use of ğœ–-transitions is to streamline the presentation of the technical result of this paper. â€¢ ğ›¿ ğ‘Ÿ : ğ‘ˆ Ã— Î£ ğœ– â†’ 2 ğ‘ˆ is the transition relation (that allows nondeterminism),</p><p>â€¢ ğ‘‡ ğ‘ : ğ‘ˆ ğ‘ â‡ D (ğ‘ˆ ğ‘ ) is the probabilistic transition function, and</p><formula xml:id="formula_2">â€¢ ğœŒ : ğ‘ˆ Ã— ğ‘ˆ ğ‘ Ã— Î£ ğœ– Ã— ğ‘ˆ Ã— ğ‘ˆ ğ‘ â†’ R is the reward function. To simplify the notation we let ğ‘ˆ R = ğ‘ˆ Ã— ğ‘ˆ ğ‘ and ğ‘¢ R 0 = (ğ‘¢ 0 , ğ‘¢ ğ‘ 0 ). Definition 2.3 (Product (rewardful) MDP). Given an MDP M = (ğ‘†, ğ‘  0 , ğ´,ğ‘‡ , ğ´ğ‘ƒ, ğ¿), a reward machine R = (Î£ ğœ– , ğ‘ˆ R , ğ‘¢ R 0 , ğ›¿ ğ‘Ÿ ,ğ‘‡ ğ‘ , ğœŒ)</formula><p>with the alphabet Î£ = 2 ğ´ğ‘ƒ , and the labeling function ğ¿ : ğ‘† â†’ Î£, their product</p><formula xml:id="formula_3">M Ã— R = (ğ‘†Ã—ğ‘ˆ R , ğ‘  0 Ã—ğ‘¢ R 0 , (ğ´Ã—ğ‘ˆ R ) âˆª {ğœ–},ğ‘‡ Ã— , ğœŒ Ã— ) is a rewardful MDP where ğ‘‡ Ã— :(ğ‘†Ã—ğ‘ˆ R ) Ã— ((ğ´Ã—ğ‘ˆ R ) âˆª {ğœ–}) â†’ D (ğ‘†Ã—ğ‘ˆ R ) is such that ğ‘‡ Ã— ((ğ‘ , (ğ‘¢, ğ‘¢ ğ‘ )), ğ›¼) ((ğ‘  â€² , (ğ‘¢ â€² , ğ‘¢ ğ‘ â€² ))) = ï£± ï£´ ï£´ ï£´ ï£² ï£´ ï£´ ï£´ ï£³ ğ‘‡ (ğ‘ , ğ‘) (ğ‘  â€² ) â€¢ ğ‘‡ ğ‘ (ğ‘¢ ğ‘ ) (ğ‘¢ ğ‘ â€² ) if ğ›¼ = (ğ‘, (ğ‘¢ â€² , ğ‘¢ ğ‘ â€² )) and (ğ‘¢, ğ¿(ğ‘ ), ğ‘¢ â€² ) âˆˆ ğ›¿ ğ‘Ÿ 1 if ğ›¼ = ğœ–, ğ‘  = ğ‘  â€² , and ğ›¿ (ğ‘¢, ğœ–, ğ‘¢ â€² ) âˆˆ ğ›¿ ğ‘Ÿ 0 otherwise. and ğœŒ Ã— : (ğ‘†Ã—ğ‘ˆ R ) Ã— ((ğ´Ã—ğ‘ˆ R ) âˆª {ğœ–}) Ã— (ğ‘†Ã—ğ‘ˆ R ) â†’ R is defined such that ğœŒ Ã— ((ğ‘ , ğ‘¢ R ), ğ›¼, (ğ‘  â€² , ğ‘¢ R â€² )) = ğœŒ (ğ‘¢ R , ğ¿(ğ‘ ), ğ‘¢ R â€² ) if ğ›¼ = (ğ‘, ğ‘¢ R â€² ) ğœŒ (ğ‘¢ R , ğœ–, ğ‘¢ R â€² ) if ğ›¼ = ğœ–.</formula><p>A deterministic reward machine is retrieved from the above definition by setting | ğ›¿ ğ‘Ÿ (ğ‘¢, ğ‘) |â‰¤ 1 and ğ‘‡ ğ‘ (ğ‘¢ ğ‘ ) âˆˆ {0, 1} for all ğ‘¢ âˆˆ ğ‘ˆ , ğ‘ âˆˆ Î£ ğœ– , and ğ‘¢ ğ‘ âˆˆ ğ‘ˆ ğ‘ . For technical convenience, we assume that MÃ—R contains only states reachable from (ğ‘  0 , ğ‘¢ R 0 ). For both discounted and average objectives, the optimal strategies of MÃ—R are positional on MÃ—R. Moreover, these positional strategies characterize a finite memory strategy (with memory skeleton based on the states of R and the next-action function based on the positional strategy) over M maximizing the learning objective given by R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Qualitative Objectives: Omega-Regular Specifications</head><p>Formal specification languages, such as ğœ”-automata and logic, provide a rigorous and unambiguous mechanism to express learning objectives for continuing tasks. There is a growing trend <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b70">71]</ref> toward expressing learning objectives in RL using linear temporal logic (LTL) and ğœ”-regular languages (which strictly generalize LTL). We will describe ğœ”-regular languages by good-for-MDP BÃ¼chi automata <ref type="bibr" target="#b32">[33]</ref>.</p><p>Linear Temporal Logic (LTL) <ref type="bibr" target="#b4">[5]</ref> is a temporal logic that is often used to specify objectives in human-readable form. Given a set of atomic propositions ğ´ğ‘ƒ, the LTL formulae over ğ´ğ‘ƒ can be defined via the following grammar:</p><formula xml:id="formula_4">ğœ‘ := ğ‘ | Â¬ğœ‘ | ğœ‘ âˆ¨ ğœ‘ | X ğœ‘ | ğœ‘ U ğœ‘,<label>(2.1)</label></formula><p>where ğ‘ âˆˆ ğ´ğ‘ƒ, using negation Â¬, disjunction âˆ¨, next X, and until U operators. Additional operators are introduced as abbreviations:</p><formula xml:id="formula_5">âŠ¤ def = ğ‘ âˆ¨ Â¬ğ‘; âŠ¥ def = Â¬âŠ¤; ğœ‘ âˆ§ ğœ“ def = Â¬(Â¬ğœ‘ âˆ¨ Â¬ğœ“ ); ğœ‘ â†’ ğœ“ def = Â¬ğœ‘ âˆ¨ ğœ“ ; F ğœ‘ def = âŠ¤ U ğœ‘; and G ğœ‘ def = Â¬ F Â¬ğœ‘. We write ğ‘¤ |= ğœ‘ if ğœ”-word ğ‘¤ over 2 ğ´ğ‘ƒ satisfies LTL formula ğœ‘.</formula><p>The satisfaction relation is defined inductively <ref type="bibr" target="#b4">[5]</ref>. We will provide details of classes of specifications including safety, liveness, and fairness in Subsection 3.</p><p>Nondeterministic BÃ¼chi automata are finite state acceptors for all ğœ”-regular languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.4 (BÃ¼chi Automata). A (nondeterministic)</head><p>BÃ¼chi automaton is a tuple A = (Î£, ğ‘„, ğ‘ 0 , ğ›¿, ğ¹ ), where â€¢ Î£ is a finite alphabet,</p><p>â€¢ ğ‘„ is a finite set of states,</p><p>â€¢ ğ‘ 0 âˆˆ ğ‘„ is the initial state,</p><p>â€¢ ğ›¿ : ğ‘„ Ã— Î£ â†’ 2 ğ‘„ is the transition function, and</p><formula xml:id="formula_6">â€¢ ğ¹ âŠ‚ ğ‘„ Ã— Î£ Ã— ğ‘„ is the set of accepting transitions. A run ğ‘Ÿ of A on ğ‘¤ âˆˆ Î£ ğœ” is an ğœ”-word âŸ¨ğ‘Ÿ 0 , ğ‘¤ 0 , ğ‘Ÿ 1 , ğ‘¤ 1 , . . .âŸ© âˆˆ (ğ‘„ Ã— Î£) ğœ” such that ğ‘Ÿ 0 = ğ‘ 0 and, for ğ‘– &gt; 0, ğ‘Ÿ ğ‘– âˆˆ ğ›¿ (ğ‘Ÿ ğ‘– âˆ’1 , ğ‘¤ ğ‘– âˆ’1 ). Each triple (ğ‘Ÿ ğ‘– âˆ’1 , ğ‘¤ ğ‘– âˆ’1 , ğ‘Ÿ ğ‘– ) is a transition of A.</formula><p>We write Inf (ğ‘Ÿ ) for the set of transitions that appear infinitely often in the run ğ‘Ÿ . A run ğ‘Ÿ of A is accepting if Inf(ğ‘Ÿ ) âˆ© ğ¹ â‰  âˆ…. The language L (A) of A is the subset of words in Î£ ğœ” with accepting runs in A. A language is ğœ”-regular if it is accepted by a BÃ¼chi automaton.</p><p>Good-for-MDP BÃ¼chi Automata. Given an MDP M and a specification ğœ‘ represented with an ğœ”-automaton A = (Î£, ğ‘„, ğ‘ 0 , ğ›¿, ğ¹ ), we want to compute an optimal strategy satisfying the objective. We define the satisfaction probability of ğœ from starting state ğ‘  as</p><formula xml:id="formula_7">PSem M A (ğ‘ , ğœ) = Pr M ğœ (ğ‘ ) ğ‘Ÿ âˆˆ Runs M ğœ (ğ‘ ) : ğ¿(ğ‘Ÿ ) âˆˆ L (A)</formula><p>. The optimal satisfaction probability PSem M A (ğ‘ ) for specification A is defined as sup ğœ âˆˆÎ  M Pr M ğœ (ğ‘ , ğœ) and we say that ğœ âˆˆ Î  M is an optimal strategy for</p><formula xml:id="formula_8">A if PSem M A (ğ‘ , ğœ) (ğ‘ ) = PSem M A (ğ‘ ).</formula><p>Definition 2.5 (Product MDP). Given an MDP M = (ğ‘†, ğ‘  0 , ğ´,ğ‘‡ , ğ´ğ‘ƒ, ğ¿) and automaton A = (Î£, ğ‘„, ğ‘ 0 , ğ›¿, ğ¹ ) with alphabet Î£ = 2 ğ´ğ‘ƒ , the product M Ã— A = (ğ‘† Ã— ğ‘„, (ğ‘  0 , ğ‘ 0 ), ğ´ Ã— ğ‘„,ğ‘‡ Ã— , ğ¹ Ã— ) is an MDP with initial state (ğ‘  0 , ğ‘ 0 ) and accepting transitions ğ¹ Ã— where ğ‘‡ Ã— :</p><formula xml:id="formula_9">(ğ‘† Ã— ğ‘„) Ã— (ğ´ Ã— ğ‘„) â‡ D (ğ‘† Ã— ğ‘„) is defined by ğ‘‡ Ã— ((ğ‘ , ğ‘), (ğ‘, ğ‘ â€² )) ((ğ‘  â€² , ğ‘ â€² )) = ğ‘‡ (ğ‘ , ğ‘) (ğ‘  â€² ) if (ğ‘, ğ¿(ğ‘ ), ğ‘ â€² ) âˆˆ ğ›¿ 0 otherwise.</formula><p>The accepting transition set ğ¹ Ã— âŠ† (ğ‘† Ã— ğ‘„) Ã— (ğ´ Ã— ğ‘„) Ã— (ğ‘† Ã— ğ‘„) is defined by ((ğ‘ , ğ‘), (ğ‘, ğ‘ â€² ), (ğ‘  â€² , ğ‘ â€² )) âˆˆ ğ¹ Ã— if, and only if, (ğ‘, ğ¿(ğ‘ ), ğ‘ â€² ) âˆˆ ğ¹ and ğ‘‡ (ğ‘ , ğ‘) (ğ‘  â€² ) &gt; 0. A strategy ğœ Ã— on the product defines a strategy ğœ on the MDP with the same value, and vice versa. Note that for a stationary ğœ Ã— , the strategy ğœ may need memory. End-components and runs of the product MDP are defined just like for MDPs.</p><p>A run</p><formula xml:id="formula_10">ğ‘Ÿ of MÃ—A is accepting if Inf(ğ‘Ÿ ) âˆ© ğ¹ Ã— â‰  âˆ….</formula><p>The syntactic satisfaction probability is the probability of accepting runs:</p><formula xml:id="formula_11">PSat M A ((ğ‘ , ğ‘), ğœ Ã— ) = Pr MÃ— A ğœ Ã— (ğ‘ , ğ‘) ğ‘Ÿ âˆˆ Runs MÃ— A ğœ Ã— (ğ‘ , ğ‘) : Inf(ğ‘Ÿ ) âˆ© ğ¹ Ã— â‰  âˆ… .</formula><p>Similarly, we define PSat M A (ğ‘ ) as the optimal probability over the product, i.e., sup ğœ Ã— PSat M A ((ğ‘ , ğ‘ 0 ), ğœ Ã— ) . For a deterministic A the equality PSat M A (ğ‘ ) = PSem M A (ğ‘ ) holds; however, this is not guaranteed for nondeterministic BÃ¼chi automata as the optimal resolution of nondeterministic choices may require access to future events. This motivates the definition of a good-for-MDP nondeterminisitc BÃ¼chi automata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.6 (Good-for-MDP [33]). A BÃ¼chi automaton</head><formula xml:id="formula_12">A is good for MDPs (GFM), if PSat M A (ğ‘  0 ) = PSem M A (ğ‘  0</formula><p>) holds for all MDPs M and starting states ğ‘  0 .</p><p>Note that every ğœ”-regular objective can be expressed as a GFM automaton <ref type="bibr" target="#b32">[33]</ref>. A popular class of GFM automata is suitable limit-deterministic BÃ¼chi automata <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b76">77]</ref>. There are other types of ğœ”-automata that are good-for-MDPs. For example, in parity automata, each transition is assigned an integer priority. A run of a parity automaton is accepting if the highest recurring priority is odd. In this paper, we only use GFM BÃ¼chi automata.</p><p>The satisfaction of an ğœ”-regular objective given as a GFM automaton A by an MDP M can be formulated in terms of the accepting maximal end-components of the product MÃ—A, i.e., the maximal end-components that contain an accepting transition from F Ã— . The optimal satisfaction probabilities and strategies can be computed by first computing the accepting maximal end-components of M Ã— A and then maximizing the probability to reach states in such components. The optimal strategies are positional on M Ã— A and induce finite-memory strategies over M that maximize the satisfaction probability of the learning objective given by A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Reinforcement Learning for Continuing Tasks and Problem Statements</head><p>RL is a sampling-based optimization approach where an agent learns to optimize its strategy by repeatedly interacting with the environment relying on the reinforcements (numerical reward signals) it receives for its actions. We focus on the model-free approach to RL, where the learner computes optimal strategies without explicitly estimating the transition probabilities and rewards. These approaches are asymptotically more spaceefficient <ref type="bibr" target="#b84">[85]</ref> than model-based RL and have been shown to scale well <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b77">78]</ref>. Some prominent model-free RL algorithms for discounted and average reward objectives include Q-learning and TD(ğœ†) <ref type="bibr" target="#b85">[86]</ref> and Differential Q-learning <ref type="bibr" target="#b92">[93,</ref><ref type="bibr" target="#b93">94]</ref>.</p><p>In some applications, such as running a maze or playing tic-tac-toe-the interaction between the agent and the environment naturally breaks into finite length learning sequences, called episodes. Thus the agent optimizes its strategy by combining its experience over different episodes. We call such tasks episodic. On the other hand, for some applications-such as process control and reactive systems-this interaction continues ad-infinitum and the agent learns over a single lifetime. We call such tasks continuing. This paper develops a model-free RL algorithm for continuing tasks where the learning objective is an ğœ”-regular specification given as a GFM automaton. Prior solutions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b70">71]</ref> focused on episodic setting and have proposed a model-free reduction from ğœ”-regular objectives to discounted-reward objectives. Several researchers <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b85">86]</ref> have made the case for adopting average reward formulation for continuing tasks due to several limitations of discounted-reward RL in continuing tasks. This paper investigates a model-free reduction from ğœ”-regular objectives to average-reward objectives in model-free RL.</p><p>Problem 2.7 (ğœ”-Regular to Average Reward Translation). Given an unknown communicating MDP M = (ğ‘†, ğ‘  0 , ğ´,ğ‘‡ , ğ´ğ‘ƒ, ğ¿) and a GFM automaton A = (Î£, ğ‘„, ğ‘ 0 , ğ›¿, ğ¹ ), the reward translation problem is to design a reward machine R such that an optimal positional strategy maximizing the average reward for MÃ—R provides a finite memory strategy maximizing the satisfaction probability of L (A) in M.</p><p>We also consider the lexicographic multi-objective optimization in which the goal is to maximize a mean-payoff objective over those policies that satisfy a given liveness specification.</p><p>Problem 2.8 (Lexicographic ğœ”-regular and Average Reward). Given an unknown communicating MDP M = (ğ‘†, ğ‘  0 , ğ´,ğ‘‡ , ğ´ğ‘ƒ, ğ¿), a GFM automaton A = (Î£, ğ‘„, ğ‘ 0 , ğ›¿, ğ¹ ), and a reward function ğœŒ (ğ‘ , ğ‘  â€² ), the problem is to design an algorithm such that the resulting policy maximizes the average reward</p><formula xml:id="formula_13">Avg M ğœ (ğ‘ ) := lim sup ğ‘ â†’âˆ 1 ğ‘ E M ğœ (ğ‘ ) ğ‘ âˆ‘ï¸ ğ‘–=1 ğœŒ (ğ‘  ğ‘– âˆ’1 , ğ‘ ğ‘– , ğ‘  ğ‘– ) ,</formula><p>among the policies that maximize the satisfaction probability of A in M.</p><p>Next we give an account of challenges with average-reward RL, identify suitable classes of qualitative temporal specifications in Section 3, and then solve Problems 2.7-2.8 respectively in Sections 4-5 under appropriate assumptions on the structure of the underlying MDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Average-Reward Reinforcement Learning</head><p>Given an MDP M, reward machine R, and an objective (discounted or average reward), an optimal strategy can be computed in polynomial time using linear programming <ref type="bibr" target="#b68">[69]</ref>. Similarly, graph-theoretic techniques to find maximal end-components can be combined with linear programming to compute optimal strategies for ğœ”-regular objectives <ref type="bibr" target="#b30">[31]</ref>. However, such techniques are not applicable when the transitions or reward structure of the MDP is unknown. The existing average-reward RL algorithms such as Differential Q-learning provide convergence guarantees under the assumption that the MDP M is communicating <ref type="bibr" target="#b93">[94]</ref>. Thus, for the reward translation in Problem 2.7 to be effective, we need the product M Ã— A to be communicating. Unfortunately, even when M is communicating, M Ã— A may violate the communicating requirement. We tackle this issue in Section 4.</p><p>The communicating assumption requires that, for every pair of states ğ‘  and ğ‘  â€² , there is a policy under which the MDP can reach ğ‘  â€² from ğ‘  in a finite number of steps with positive probability. Under the communicating assumption, there exists a unique optimal reward rate ğ‘Ÿ â˜… that is independent of the start state. The differential Q-learning algorithm constructs a table of estimates ğ‘„ ğ‘¡ : ğ‘† Ã— ğ‘ˆ â†’ R at each time step ğ‘¡. Let us denote the state-action pair of the MDP at time ğ‘¡ by (ğ‘  ğ‘¡ , ğ‘¢ ğ‘¡ ). Then, differential Q-learning updates the table as follows:</p><formula xml:id="formula_14">ğ‘„ ğ‘¡ +1 (ğ‘ , ğ‘¢) := ğ‘„ ğ‘¡ (ğ‘ , ğ‘¢)</formula><p>for all (ğ‘ , ğ‘¢) â‰  (ğ‘  ğ‘¡ , ğ‘¢ ğ‘¡ )</p><formula xml:id="formula_15">ğ‘„ ğ‘¡ +1 (ğ‘ , ğ‘¢) := ğ‘„ ğ‘¡ (ğ‘  ğ‘¡ , ğ‘¢ ğ‘¡ ) + ğ›¼ ğ‘¡ ğ›¿ ğ‘¡ , for (ğ‘ , ğ‘¢) = (ğ‘  ğ‘¡ , ğ‘¢ ğ‘¡ ),</formula><p>where ğ›¼ ğ‘¡ is the step-size at step ğ‘¡, and ğ›¿ ğ‘¡ is the temporal difference error defined as</p><formula xml:id="formula_16">ğ›¿ ğ‘¡ := ğœŒ ğ‘¡ +1 âˆ’ rğ‘¡ + max ğ‘¢ ğ‘„ ğ‘¡ (ğ‘  ğ‘¡ +1 , ğ‘¢) âˆ’ ğ‘„ ğ‘¡ (ğ‘  ğ‘¡ , ğ‘¢ ğ‘¡ ) and rğ‘¡+1 := rğ‘¡ + ğœ‚ â€¢ ğ›¼ ğ‘¡ â€¢ ğ›¿ ğ‘¡ ,</formula><p>where ğœŒ ğ‘¡ +1 is the reward received for the transition (ğ‘  ğ‘¡ , ğ‘¢ ğ‘¡ , ğ‘  ğ‘¡ +1 ), rğ‘¡ is a scalar estimate of the optimal reward rate ğ‘Ÿ â˜… , and ğœ‚ is a positive constant. It is shown in <ref type="bibr" target="#b93">[94]</ref> that rğ‘¡ converges to ğ‘Ÿ â˜… under the following assumptions: (a) the MDP is communicating; (b) the associated Bellman equation has a unique solution up to a constant; (c) the step sizes ğ›¼ ğ‘¡ decrease appropriately with ğ‘¡;</p><p>(d) all state-action pairs are updated infinitely often; and (e) the ratio of the update frequency of the most-updated state-action pair to the least-updated one is finite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Specifications Suitable for Continual Learning</head><p>In this section, we study and characterize temporal logic specifications that express various classes of tasks. We then identify those suitable for continual reinforcement learning and provide solutions to Problems 2.7-2.8 in subsequent sections. Two important classes of temporal logic specifications are safety and liveness properties <ref type="bibr" target="#b1">[2]</ref>.</p><p>Loosely speaking, safety specifications ensure that something bad never happens, while liveness specifications guarantee that something good eventually happens. Below, we briefly recall the formal definition of safety and then focus on liveness.</p><p>Definition 3.1 (Safety and Liveness). Let ğ‘¤ be a sequence over alphabet Î£ and let ğ‘¤ (ğ‘–) represent the prefix of ğ‘¤ of length ğ‘– + 1; i.e., ğ‘¥ (ğ‘–) = (ğ‘¥ 0 , ğ‘¥ 1 , . . . , ğ‘¥ ğ‘– ).</p><p>â€¢ A language L âŠ† Î£ ğœ” is a safety specification if, and only if, for all ğœ”-sequences ğ‘¤ âˆˆ Î£ ğœ” not in L, some prefix of ğ‘¤ cannot be extended to an ğœ”-sequence in the language. â€¢ A language L âŠ† Î£ ğœ” is a liveness specification if, and only if, we can extend any finite sequence in Î£ * to an ğœ”-sequence in the language. In other words, the set {ğ‘¤ (ğ‘–) :</p><formula xml:id="formula_17">ğ‘¤ âˆˆ L} is Î£ * . For instance ğœ“ 1 = ğ‘ âˆ¨ F ğ‘ is a liveness specification.</formula><p>Definition 3.2 (Absolute Liveness). A language L âŠ† Î£ ğœ” is an absolute liveness specification if, and only if, L is non-empty and, for every finite sequence w âˆˆ Î£ * and every ğ‘¤ âˆˆ L, wğ‘¤ âˆˆ L holds. That is, appending an arbitrary finite prefix to an accepted word produces an accepted word. This implies that an LTL specification ğœ‘ is absolute liveness specification if ğœ‘ is satisfiable and ğœ‘ is equivalent to F ğœ‘. One can observe that ğœ“ 1 is not an absolute liveness specification since adding the prefix Â¬ğ‘ to a trace that does not satisfy F ğ‘ yields a trace not in the language. However, ğœ“ 2 = F ğ‘ is an absolute liveness specification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3.3 (Stable Specification</head><p>). A language L âŠ† Î£ ğœ” is a stable specification if, and only if, it is closed under suffixes; that is, if ğ‘¥ is in L, then every suffix of ğ‘¥ is also in L. Since deleting the first element from ğ‘(Â¬ğ‘) ğœ” results in a sequence that does not satisfy ğœ“ 2 , ğœ“ 2 is not stable. However, ğœ“ 3 = G ğ‘ âˆ¨ G F ğ‘ is stable. Moreover, a language is a fairness specification if and only if it is both a stable specification and absolute liveness specification, e.g., ğœ“ 4 = G F ğ‘. One can easily conclude that none of the specifications ğœ“ 1 ,ğœ“ 2 , and ğœ“ 3 are fairness specifications.</p><p>We give a solution for the translation in Problem 2.7 for absolute liveness specifications <ref type="bibr" target="#b82">[83]</ref>. Adding a prefix to a trace for average reward objectives should not change the average value associated with the trace. This is aligned with the satisfaction of absolute liveness specifications. Moreover, since absolute liveness specifications cannot be rejected for any finite word, they preserve the continual nature of the learning procedure. In the following, we focus on the characteristics of absolute liveness specifications.</p><p>The set of absolute liveness specifications is closed under union and intersection. This proofs mentioned below rely on the property ğœ‘ â‰¡ F ğœ‘ in LTL notation. The more general ğœ”-regular proofs use ğœ‘ â‰¡ Î£ * ğœ‘. In addition, closure under intersection requires an "asterisk:" the intersection is an absolute liveness specification unless it is empty. Suppose ğœ‘ 1 is equivalent to F ğœ‘ 1 and ğœ‘ 2 is equivalent to F ğœ‘ 2 . Then, Moreover, since for every LTL specification ğœ‘, F ğœ‘ is equivalent to F F ğœ‘,</p><formula xml:id="formula_18">ğœ‘ 1 âˆ¨ ğœ‘ 2 â‰¡ (F ğœ‘ 1 ) âˆ¨ (F ğœ‘ 2 ) â‰¡ F(ğœ‘ 1 âˆ¨ ğœ‘ 2 ) .</formula><formula xml:id="formula_19">ğœ‘ 1 âˆ§ ğœ‘ 2 â‰¡ (F F ğœ‘ 1 ) âˆ§ (F F ğœ‘ 2 ) â‰¡ F((F ğœ‘ 1 ) âˆ§ (F ğœ‘ 2 )) â‰¡ F(ğœ‘ 1 âˆ§ ğœ‘ 2 ) .</formula><p>Lemma 2.1 of <ref type="bibr" target="#b82">[83]</ref>, which says that the complement of a stable specification different from Î£ ğœ” is an absolute liveness specification and vice versa, can be used to show that the stable specifications are also closed under union and intersection. The case when either stable specification is âŠ¤ is easily proved. Because of these closure properties, 1â‰¤ğ‘˜ â‰¤ğ‘› (F G ğ‘ ğ‘˜ ) âˆ§ (G F ğ‘ ğ‘˜ ) (where each ğ‘ ğ‘˜ and ğ‘ ğ‘˜ is an atomic proposition) is an absolute liveness specification of Rabin index ğ‘›. This means that there is no upper bound to the number of priorities required by a parity automaton accepting an absolute liveness specification <ref type="bibr" target="#b91">[92]</ref>. The simplest absolute liveness specification is âŠ¤, which requires one priority. The automata that accept absolute liveness and stable specifications have special properties discussed next. Lemma 3.4. Let A be a deterministic ğœ”-automaton with initial state ğ‘  0 . A accepts an absolute liveness specification if, and only if, the language accepted from any reachable state ğ‘  of A contains the language accepted from ğ‘  0 .</p><p>Proof. Suppose A accepts an absolute liveness specification. Let ğ‘¢ be a word that takes the automaton from ğ‘  0 to ğ‘ . If ğ‘¤ is accepted from ğ‘  0 , so is ğ‘¢ğ‘¤. This means that ğ‘¤ is accepted from ğ‘  because A is deterministic and the unique run of ğ‘¢ğ‘¤ goes through ğ‘ . Now suppose that the language accepted from any reachable state of A contains the language accepted from ğ‘  0 . If ğ‘¢ takes A from ğ‘  0 to ğ‘  and ğ‘¤ is accepted from ğ‘  0 , then ğ‘¤ is accepted from ğ‘ , which means that ğ‘¢ğ‘¤ is accepted from ğ‘  0 . Therefore, A accepts an absolute liveness specification. â–¡</p><p>The "only-if" part of Lemma 3.4 does not apply to nondeterministic automata-witness the two-state automaton for the absolute liveness specification F G ğ‘, in which the language of the accepting state is the language of G ğ‘. We have a similar result for stable specifications. Lemma 3.5. Let A be an ğœ”-automaton with initial state ğ‘  0 . A accepts a stable specification if, and only if, the language accepted from any reachable state ğ‘  of A is contained in the language accepted from ğ‘  0 .</p><p>Proof. Suppose A accepts a stable specification. Let ğ‘¤ be a word accepted from ğ‘  and ğ‘¢ be a word that takes the automaton from ğ‘  0 to ğ‘ . Then ğ‘¢ğ‘¤ is accepted from ğ‘  0 . This means that the suffix ğ‘¤ of ğ‘¢ğ‘¤ is accepted from ğ‘  0 because A accepts a stable specification. Now suppose that the language accepted from any reachable state of A is contained in the language accepted from ğ‘  0 . If ğ‘¢ takes A from ğ‘  0 to ğ‘  on some accepting run of ğ‘¢ğ‘¤, then ğ‘¤ is accepted from ğ‘ , which means that ğ‘¤ is accepted from ğ‘  0 . Therefore, A accepts a stable specification. â–¡</p><p>Note that Lemma 3.5 does not require determinism. Combined with Lemma 3.4, it proves that a deterministic automaton accepts a fairness specification if, and only if, all its states are language-equivalent. This implies that a fairness specification accepted by a deterministic (BÃ¼chi) automaton is accepted by a strongly connected deterministic (BÃ¼chi) automaton. Any reachable sink SCC of a deterministic automaton for a fairness specification is, in itself, an automaton for the specification. The above lemmas show that checking whether a specification is stable or an absolute liveness specification is reducible to checking language containment for deterministic ğœ”-automata. To solve Problem 2.7, we make the following assumption.</p><p>Assumption 1. Given an MDP M and ğœ”-automaton A, we assume that: 1) M is communicating; 2) A is a GFM automaton; and 3) A accepts an absolute liveness specification.</p><p>We also study relaxing the communicating assumption on M to weakly communicating, which requires strengthening the specification to fairness in order to retain correctness and convergence guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Average-Reward RL for Qualitative Objectives</head><p>This section provides a solution for Problem 2.7. Let us fix a communicating MDP M = (ğ‘†, ğ‘  0 , ğ´,ğ‘‡ , ğ´ğ‘ƒ, ğ¿) and an absolute liveness GFM property A = (Î£, ğ‘„, ğ‘ 0 , ğ›¿, ğ¹ ) for the rest of this section. Our goal is to construct a reward machine R such that we can use an off-the-shelf average reward RL on M Ã— R to compute an optimal strategy of M against A. Since the optimal strategies are not positional on M but rather positional on MÃ—A, it is natural to assume that the reward machine R takes the structure of A with a reward function providing positive reinforcement with every accepting transition. Unfortunately, even for absolute liveness GFM automata A, the product M Ã— A with a communicating MDP M may not be communicating.</p><formula xml:id="formula_20">ğ‘ 0 ğ‘ 1 ğ‘ 2 ğ‘ Â¬ğ‘ âŠ¤ ğ‘ Â¬ğ‘ Fig. 1. Automaton for (F G ğ‘)âˆ¨(F G Â¬ğ‘).</formula><p>Example 4.1. Assume a communicating MDP M with at least one state labeled ğ‘ or Â¬ğ‘, and the absolute liveness property ğœ‘ = (F G ğ‘) âˆ¨ (F G Â¬ğ‘) with its automaton shown in Fig. <ref type="figure">1</ref>. Observe that any run that visits one of the two accepting states cannot visit the other one. Hence, the product does not satisfy the communicating property.</p><p>Reward Machine Construction. Let A = (Î£, ğ‘„, ğ‘ 0 , ğ›¿, ğ¹ ) be an absolute liveness GFM BÃ¼chi automaton. Consider R A = (Î£ ğœ– , ğ‘„, ğ‘ 0 , ğ›¿ â€² , ğœŒ) where ğ›¿ â€² (ğ‘, ğ‘) = ğ›¿ (ğ‘, ğ‘) for all ğ‘ âˆˆ Î£ and ğœ– transitions reset to the starting state, i.e. ğ›¿ â€² (ğ‘, ğœ–) = ğ‘ 0 . Note that by adding the reset (ğœ–) action from every state of R to its initial state, the graph structure of M is strongly connected. The reward function ğœŒ : ğ‘„ Ã— (Î£ âˆª {ğœ–}) Ã— ğ‘„â†’R is such that</p><formula xml:id="formula_21">ğœŒ (ğ‘, ğ‘, ğ‘ â€² ) = ï£± ï£´ ï£´ ï£´ ï£² ï£´ ï£´ ï£´ ï£³ ğ‘ if ğ‘ = ğœ– 1 if (ğ‘, ğ‘, ğ‘ â€² ) âˆˆ ğ¹ 0 otherwise.</formula><p>Remark 4.2. Intuitively, adding reset transitions that should only be taken a finite number of times corresponds to transforming the GFM BÃ¼chi automaton into a nondeterministic parity automaton with three priorities, with the reset transitions rejecting at the highest priority. The parity automaton accepts the same language as the given BÃ¼chi automaton because an accepting run only takes finitely many reset transitions. Hence, a word whose run uses reset transitions consists of a finite prefix followed by a word accepted by the BÃ¼chi automaton. Since the latter accepts an absolute liveness property, adding a prefix to an accepted word gives another accepted word. One can then observe that adding transitions to an automaton results in an automaton that simulates the given automaton. Finally, one invokes [33, <ref type="bibr">Lemma 1]</ref> to conclude that the nondeterministic parity automaton is GFM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 4.3 (Preservation of Communication).</head><p>For a communicating MDP M and reward machine R A for an absolute liveness GFM automaton A, we have that the product MÃ—R A is communicating. Proof. To show that MÃ—R A is communicating, we need to show that for arbitrary states (ğ‘ , ğ‘), (ğ‘  â€² , ğ‘ â€² ) âˆˆ ğ‘† Ã—ğ‘„ reachable from the initial state (ğ‘  0 , ğ‘ 0 ), there is a strategy that can reach (ğ‘  â€² , ğ‘ â€² ) from (ğ‘ , ğ‘) with positive probability. Note that since M is communicating, it is possible to reach (ğ‘  0 , ğ‘ â€² ) from (ğ‘ , ğ‘) for some ğ‘ â€² of R A using a strategy to reach ğ‘  0 from ğ‘  in M. We can then use a reset (ğœ–) action in R A to reach the state (ğ‘  0 , ğ‘ 0 ). Since (ğ‘  â€² , ğ‘ â€² ) is reachable from the initial state (ğ‘  0 , ğ‘ 0 ), we have a strategy to reach (ğ‘  â€² , ğ‘ â€² ) from (ğ‘ , ğ‘) with positive probability. â–¡ Lemma 4.4 (Average and Probability). There exists a ğ‘ * &lt; 0 such that for all ğ‘ &lt; ğ‘ * , positional strategies that maximize the average reward on M Ã— R A will maximize the satisfaction probability of A.</p><p>Proof. The proof is in three parts.</p><p>(1) First observe that if ğ‘ &lt; 0, then for any average-reward optimal strategy in M Ã— R A , the expected average reward is non-negative. This is so because all other actions except ğœ– actions provide non-negative rewards. Hence, any strategy that takes ğœ– actions only finitely often, results in a non-negative average reward. (2) Let Î  * be the set of positional strategies in MÃ—R A such that the ğœ– actions are taken only finitely often, i.e. no BSCC of the corresponding Markov chain contains an ğœ– transition. Let Î  ğœ– be the set of remaining positional strategies, i.e., the set of positional strategies that visit an ğœ– transition infinitely often. Let 0&lt;ğ‘ min &lt;1 be a lower bound on the expected long-run frequency of the ğœ– transitions among all strategies in Î  ğœ– . Let ğ‘ * = âˆ’1/ğ‘ min . Observe that for every policy ğœ â€² âˆˆ Î  ğœ– , the expected average reward is negative and cannot be an optimal strategy in M Ã— R A . To see that, let 0 &lt; ğ‘ â‰¤ 1 be the the long-run frequency of the ğœ– transitions for ğœ and let 0 â‰¤ ğ‘ &lt; 1 be the long-run frequency of visiting accepting transitions for ğœ. The average reward for ğœ is</p><formula xml:id="formula_22">Avg M Ã— R A ğœ (ğ‘  0 , ğ‘ 0 ) = ğ‘ â€¢ ğ‘ + ğ‘ â€¢ 1 + (1 âˆ’ ğ‘ âˆ’ ğ‘) â€¢ 0 â‰¤ ğ‘ â€¢ ğ‘ + ğ‘ â€¢ 1 + (1 âˆ’ ğ‘ âˆ’ ğ‘) â€¢ 1 = ğ‘ â€¢ ğ‘ + (1 âˆ’ ğ‘) â‰¤ ğ‘ â€¢ ğ‘ * + (1 âˆ’ ğ‘) = âˆ’ğ‘/ğ‘ min + (1 âˆ’ ğ‘) â‰¤ âˆ’1 + (1 âˆ’ ğ‘) â‰¤ âˆ’ğ‘.</formula><p>Since every optimal policy must have a non-negative average reward, no policy in Î  ğœ– is optimal for ğ‘ &lt; ğ‘ * .</p><p>(3) Now consider an optimal policy ğœ * in Î  * . We show that this policy also optimizes the probability of satisfaction of A. There are two cases to consider. (a) If the expected average reward of ğœ * is 0, then under no strategy it is possible to reach an accepting transitions (positive reward transition) in M Ã— R A . Hence, every policy is optimal in M against A, and so is ğœ * . (b) If the expected average reward of ğœ * is positive, then notice that for every BSCC of the Markov chain of M Ã— R A under ğœ * , the average reward is the same. This is so because otherwise, there is a positional policy that reaches the BSCC with the optimal average from all the other BSCCs with lower averages, contradicting the optimality of ğœ * . Since for an optimal policy ğœ * , every BSCC provides the same positive average, every BSCC must contain an accepting transition. Hence, every run of the MDP M under ğœ * will eventually dwell in an accepting component and in the process will see a finitely many ğœ– (reset) transitions. For any such given run ğ‘Ÿ , consider the the suffix ğ‘Ÿ â€² of the run after the last ğœ– transition is taken and let ğ‘Ÿ = ğ‘¤ğ‘Ÿ â€² for some finite run ğ‘¤. Since ğ¿(ğ‘Ÿ â€² ) is an accepting word in A, and since A is an absolute liveness property any arbitrary prefix ğ‘¤ â€² to this run ğ‘Ÿ â€² is also accepting. This implies that the original run ğ‘Ÿ is also accepting for A. It follows that for such a strategy ğœ * , the probability of satisfaction of A is 1, making ğœ * an optimal policy for M against A. â–¡ Since our translation from ğœ”-regular objective to reward machines is model-free, the following theorem is immediate.</p><p>Theorem 4.5 (Convergence of Model-free RL). Differential ğ‘„-learning algorithm for maximizing average reward objective on M Ã— R A will converge to a strategy maximizing the probability of satisfaction of A for a suitable value of ğ‘. Moreover, the product construction M Ã— R A can be done on-the-fly and it is model-free.</p><p>As an example, consider the property F G ğ‘ and an MDP with two states and all transitions between states are available as deterministic actions (Fig. <ref type="figure">2</ref> and Fig. <ref type="figure">3</ref>). Only one of the states is labeled ğ‘. An infinite memory strategy could see ğ‘ for one step, reset, then see two ğ‘s, reset, then see three ğ‘s and so forth. This strategy will produce the same average value as the positional strategy which sees ğ‘ forever without resetting. However, the infinite memory strategy will fail the property while the positional one will not.</p><p>Shaping Rewards via Hard Resets. For a BÃ¼chi automaton A, we say that its state ğ‘ âˆˆ ğ‘„ is coaccessible if there exists a path starting from that state to an accepting transition. If a state is not coaccessible then any run of the product M Ã— A that ends in such a state will never be accepting, and hence one can safely redirect all of its outgoing (even better, incoming) transitions to the initial state with reward ğ‘ (a hard reset). Such hard resets will promote speedy learning by reducing the time spent in such states during unsuccessful explorations, and at the same time adding these resets does not make a non-accepting run accepting or vice versa. Lemma 4.3, Lemma 4.4, and Theorem 4.5 continue to hold with such hard resets. Introducing hard resets is a reward shaping procedure in that it is a reward transformation <ref type="bibr" target="#b66">[67]</ref> under which optimal strategies remain invariant.</p><formula xml:id="formula_23">ğ‘ 0 ğ‘ 1 ğ‘ 2 ğ‘ Â¬ğ‘ âŠ¤ ğ‘ âŠ¤</formula><p>Extension to weakly communicating MDPs. To relax further the communicating assumption on the MDP, we can apply our method to weakly communicating MDPs but the class of specifications will be more restricted.</p><p>The set of weakly-communicating MDPs is the most general set of MDPs such that there currently exists a learning algorithm that can, using a single stream of experience, guarantee to identify a policy that achieves the optimal average reward rate in the MDP <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b94">95]</ref>. We show how to apply our method in the following for weakly communicating MDPs and fairness specifications. Proof. The proof directly follows the proof of Lemma 4.3. Since we have a weakly-communicating MDP we can partition the states of the MDP into two subsets: the first set is all the transient states under stationary policies, and the second set are the set of states where every pair of states are reachable from each other under a stationary policy. Since the sub-MDP resulting from the second set is communicating, and a fairness property is also an absolute liveness property, then based on Lemma 4.3, the sub-product MDP is communicating. On the other hand, all of the states in the first set are transient, and the fairness property is closed under the addition and deletion of prefixes then eventually we reach a state of the communicating sub-product MDP which leads to the weakly communicating property of the product MDP. â–¡</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Average-Reward RL for Lexicographic Objectives</head><p>This section provides a solution for Problem 2.8. We first note that an optimal policy for the Problem 2.8 may require infinite memory. Consider the example in Figure <ref type="figure">4</ref>. The ğœ”-regular specification G F ğ‘ can be satisfied surely by staying in the state ğ‘. Likewise, the maximum possible average reward of 1 can be obtained by staying in the state labeled Â¬ğ‘. Surprisingly, there is an infinite memory policy that achieves both. This infinite memory policy visits the state labeled Â¬ğ‘ ğ‘˜ times, then visits the state labeled ğ‘, where ğ‘˜ increases towards infinity forever. Any finite memory policy that visits ğ‘ infinitely often with probability 1 must visit the state labeled ğ‘ with positive expected frequency in steady state, say ğ‘“ &gt; 0, and thus achieves a suboptimal external average reward of at most (1 âˆ’ ğ‘“ ) &lt; 1. However, one can find finite memory policies that achieve an average reward within ğœ€ of the optimal average reward for any ğœ€ &gt; 0 while maximizing the probability of satisfaction of the ğœ”-regular specification. Namely, consider the policy which visits the state labeled Â¬ğ‘ ğ‘˜ times, then visits the state labeled ğ‘ for a fixed ğ‘˜. This In the following, we show that for any ğœ€ &gt; 0, there always exist a finite-memory policy that is ğœ€-optimal for Problem 2.8 and show how this policy can be constructed. Our approach to solve Problem 2.8 reduces the task of producing a lexicographically optimal strategy to one of solving a single average reward problem on an MDP, which can then be solved by any off-the-shelf RL algorithm for average reward on communicating MDPs, e.g. Differential Q-learning. The resulting policy is ğœ€-optimal and finite memory. We then perform a transformation of this learned policy to yield an infinite memory optimal policy. We define our average reward reduction via a reward machine. This reward machine augments the usual product with an additional bit. When this bit is zero, the agent receives reward based on the external reward function. With probability ğ›½ on every step, this bit flips to one. When this bit is one, the agent is incentivized to find an accepting edge and is given a punishing reward of ğœŒ (ğ‘ , ğ‘  â€² ) + ğ‘ 1 where ğ‘ 1 is a constant such that ğ‘ 1 + max ğ‘ ,ğ‘  â€² ğœŒ (ğ‘ , ğ‘  â€² ) &lt; min ğ‘ ,ğ‘  â€² ğœŒ (ğ‘ , ğ‘  â€² ) on every step. When the agent finds an accepting edge, the bit flips back to zero. To ensure communication, we add two types of resets, one which resets the automaton state back to its initial state (ğœ– 1 ), and one which sets the extra bit to zero (ğœ– 2 ). Both of these incur some large penalty of ğ‘ 2 . We refer to the case when the bit is zero as "the first layer" and the case when the bit is one as "the second layer". Intuitively, the agent spends most of its time collecting the external average reward, but is occasionally called upon with probability ğ›½ to prove that it can see an accepting edge. This construction is shown pictorially in Figure <ref type="figure">5</ref>. We now define the reward machine formally. Let A = (Î£, ğ‘„, ğ‘ 0 , ğ›¿, ğ¹ ) be a GFM for an absolute liveness specification and let ğœŒ (ğ‘ , ğ‘  â€² ) be a given reward function. We construct a probabilistic reward machine</p><formula xml:id="formula_24">R Aâ€¢ğœŒ = (Î£ â€² , ğ‘„ Ã— {0, 1}, (ğ‘ 0 , 0), ğ›¿ â€² , ğœŒ â€² ) where Î£ â€² = (Î£ Ã— ğ‘† Ã— ğ‘† Ã— ğ‘„) âˆª {ğœ– 1 , ğœ– 2 },</formula><p>by applying a positional policy optimal for the external reward to the second layer. Since ğœ is optimal, it must obtain a value of ğ‘ + ğ‘£ * , which obtains an external average reward of ğ‘£ * on M. If this policy takes resets infinitely often, then it takes the reset transition immediately upon reaching the second layer. This is due to the penalty ğ‘: if a trajectory of length ğ‘‡ in the second layer that ends in a reset obtains a value of ğ‘‡ (ğ‘ + ğ‘£) + ğ‘, then taking the reset first, and followed by the same trajectory in the first layer obtains a value of ğ‘‡ ğ‘£ + ğ‘ â‰¥ ğ‘‡ (ğ‘ + ğ‘£) + ğ‘, where ğ‘£ is the average reward collected along this trajectory. Thus, this policy obtains a value of ğ‘… ğœ = (1 âˆ’ ğ›½)ğ‘£ 1 + ğ›½ğ‘ in the extended product, which is maximized when ğ‘£ 1 = ğ‘£ * . Since the resets are internal to the agent, this policy collects an external average reward of ğ‘£ * on M. We have shown that under any 0 &lt; ğ›½ * &lt; 1 and ğ‘ * &lt; 0 when the ğœ”-regular objective is satisfied with probability 0, an optimal policy on the extended product collects an external average reward of ğ‘£ * â‰¥ ğ‘£ * âˆ’ ğœ€ for every ğœ€ &gt; 0.</p><p>(2) We now consider the case where the ğœ”-regular objective is satisfied with probability 1 under some policy.</p><p>Let ğº âŠ† (M Ã— A) Ã— (M Ã— A) be the set of transitions such that for every ğ‘¡ âˆˆ ğº, there exists a policy that satisfies A with probability 1 and visits ğ‘¡ infinitely often. In other words, ğº is the maximal set of transitions that can be visited infinitely often without diminishing the probability of satisfying A. Consider a lexicographically optimal strategy. This strategy cannot visit any transition outside of ğº infinitely often by definition and achieves an external average reward of ğ‘£ * . This implies that there exists a set of positional policies restricted to ğº that achieves an average reward of at least ğ‘£ * , since there exists optimal positional policies for average reward on MDPs. We now extend these policies to the extended product by applying this policy in the first layer and applying a policy which visits an accepting edge in the second layer without taking resets infinitely often and leaves ğº finitely many times, with probability 1. This second layer policy exists due to the definition of ğº. We call this set of policies ğœ * , and abuse notion by treating ğœ * as if it is a single policy for the rest of the proof. We will select 0 &lt; ğ›½ * &lt; 1 and ğ‘ * &lt; 0 such that ğœ * is optimal in the extended product, and is ğœ€-optimal on M. Consider all policies Î  1 that are positional on the extended product, satisfy the specification with probability 1, and visit the first layer infinitely often with probability 1. We say that the average reward that such a policy ğœ âˆˆ Î  1 obtains on the extended product for a particular ğ›½ is ğ‘… ğœ (ğ›½). Let ğ‘£ ğœ be the external average reward that ğœ obtains when ğ›½ = 0. Then, we have that (1 âˆ’ ğ›½)ğ‘£ ğœ + ğ›½ğ‘ ğ‘“ ğœ (ğ›½) â‰¤ ğ‘… ğœ (ğ›½) â‰¤ ğ‘£ ğœ where ğ‘“ ğœ (ğ›½) is an upper bound on the expected time it takes to return to a state in steady state after a transition from the first to the second layer occurs. The first inequality follows by breaking up the average reward into uninterrupted reward collected in the first layer in steady state, and reward collected while waiting to return to the state that was left by transitioning to the second layer in steady state. The second inequality follows from the fact that the reward in the second layer is always less than the first. Note that ğ‘“ ğœ (ğ›½) is monotonically increasing in ğ›½. Consider a policy ğœ âˆˆ Î  1 such that ğ‘£ ğœ &lt; ğ‘£ ğœ * = ğ‘£ * . Then we have that for</p><formula xml:id="formula_25">ğ›½ â‰¤ min{ 1 2 , ğ‘£ * âˆ’ğ‘£ ğœ ğ‘£ * âˆ’ğ‘ ğ‘“ ğœ * ( 1 2 ) }, ğ‘… ğœ (ğ›½) â‰¤ ğ‘£ ğœ â‰¤ (1 âˆ’ ğ›½)ğ‘£ ğœ * + ğ›½ğ‘ ğ‘“ ğœ * (ğ›½) â‰¤ (1 âˆ’ ğ›½)ğ‘£ ğœ * + ğ›½ğ‘ ğ‘“ ğœ * ( 1 2 ) â‰¤ ğ‘… ğœ * (ğ›½)</formula><p>where the third inequality follows from the monotonicity of ğ‘“ ğœ * (ğ›½). Since there are only finitely many policies in Î  1 , one can find a 0 &lt; ğ›½ thresh &lt; 1 that satisfies the above inequality for all policies in Î  1 .</p><p>Thus, for all ğ›½ &lt; ğ›½ thresh , ğœ * is an optimal average reward policy on the extended product amongst all the policies in Î  1 . Note that the external average reward collected on M by any policy ğœ for a fixed ğ›½, denoted by Rğœ (ğ›½), is larger than the average reward collected on the extended product ğ‘… ğœ (ğ›½) because all of the rewards on the extended product are less than or equal to the external rewards. By selecting</p><formula xml:id="formula_26">ğ›½ * = min{ğ›½ thresh , ğœ€ ğ‘£ * âˆ’ğ‘ ğ‘“ ğœ * ( 1 2 ) } &gt; 0, we have that ğ‘£ * âˆ’ ğœ€ â‰¤ (1 âˆ’ ğ›½)ğ‘£ ğœ * + ğ›½ğ‘ ğ‘“ ğœ * ( 1 2 ) â‰¤ ğ‘… ğœ * (ğ›½) â‰¤ Rğœ * (ğ›½),</formula><p>so ğœ * is ğœ€-optimal for all 0 &lt; ğ›½ &lt; ğ›½ * . We will now select ğ‘ * &lt; 0 such that there is an optimal policy in Î  1 . We denote all positional policies by Î . Note that all policies in Î \Î  1 either remain in the second layer forever, obtaining an average reward of ğ‘ &lt; min ğ‘ ,ğ‘  â€² ğœŒ (ğ‘ , ğ‘  â€² ) which is less than the average reward for all policies Î  1 , or take reset transitions infinitely often. Policies which select reset transitions infinitely often obtain an average reward of at most (1 âˆ’ ğ›½) max ğ‘ ,ğ‘  â€² ğœŒ (ğ‘ , ğ‘  â€² ) + ğ›½ğ‘. One can then select ğ‘ * â‰¤ ğ‘ ğ›½ âˆ’</p><p>1âˆ’ğ›½ ğ›½ max ğ‘ ,ğ‘  â€² ğœŒ (ğ‘ , ğ‘  â€² ) &lt; 0 to ensure that (1 âˆ’ ğ›½) max ğ‘ ,ğ‘  â€² ğœŒ (ğ‘ , ğ‘  â€² ) + ğ›½ğ‘ â‰¤ ğ‘. Since all policies in Î  1 obtain an average reward on the extended product greater than ğ‘, there is an optimal policy ğœ * in Î  1 for ğ‘ &lt; ğ‘ * with the previously fixed ğ›½. This ğœ * is an optimal policy on the extended product under these parameter selections and is ğœ€-optimal when applied to M. â–¡ Theorem 5.4 (Lexicographic Reward Machine Optimality). Let ğ›½ (ğ‘–) : N â†’ (0, 1) be a sequence such that lim ğ‘–â†’âˆ ğ›½ (ğ‘–) = 0. There exists a threshold ğ›½ * &gt; 0 such that for all 0 &lt; ğ›½ &lt; ğ›½ * there exists a ğ‘ * &lt; 0 such that for all ğ‘ &lt; ğ‘ * all positional policies that maximize the average reward on M Ã— R Aâ€¢ğœŒ have the following property: if we transform this fixed, finite memory policy by setting ğ›½ = ğ›½ (ğ‘–) on timestep ğ‘–, then this transformed policy is optimal for Problem 2.8 and infinite memory.</p><p>Proof. We first note that this policy is infinite memory: it must keep track of ğ›½ (ğ‘–), which requires an increasing number bits to represent exactly as ğ›½ (ğ‘–) decreases. Thus, as ğ‘– goes to infinity, the amount of memory required grows in an unbounded manner. For the rest of the proof, we follow the results and proof of Theorem 5.3. Let ğ›½ * = ğ›½ thresh as defined in the proof of Theorem 5.3, and let</p><formula xml:id="formula_27">ğ‘ * â‰¤ ğ‘ ğ›½ âˆ’ 1 âˆ’ ğ›½ ğ›½ max ğ‘ ,ğ‘  â€² ğœŒ (ğ‘ , ğ‘  â€² ) &lt; 0.</formula><p>We have that there is a set of optimal positional policies ğœ * on the extended product that satisfy A with the maximum probability and accumulate an external average reward Rğœ * (ğ›½) â‰¥ (1 âˆ’ ğ›½)ğ‘£ * + ğ›½ğ‘ ğ‘“ ğœ * ( <ref type="formula">1</ref>2 ) where ğ‘£ * is the external average reward obtained under a lexicographically optimal strategy. We now substitute ğ›½ with ğ›½ (ğ‘–). We have that </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>We implemented the reduction<ref type="foot" target="#foot_2">1</ref> with hard resets presented in Section 4. As described, we do not build the product MDP explicitly, and instead compose it on-the-fly by keeping track of the MDP and automaton states  We compare our proposed reduction, the reduction of <ref type="bibr" target="#b31">[32]</ref> with Q-learning, and the reduction of <ref type="bibr" target="#b10">[11]</ref> with Q-learning. Episodic resetting is not used.</p><p>independently. We use Differential Q-learning <ref type="bibr" target="#b93">[94]</ref> to learn optimal, positional average reward strategies. For our experiments, we have collected a set of communicating MDPs with absolute liveness specifications. <ref type="foot" target="#foot_3">2</ref>We compare with two previous approaches for translating ğœ”-regular languages to rewards: the method of <ref type="bibr" target="#b31">[32]</ref> with Q-learning and the method of <ref type="bibr" target="#b10">[11]</ref> with Q-learning. The method of <ref type="bibr" target="#b31">[32]</ref> translates a GFM BÃ¼chi automaton into a reachability problem through a suitable parameter ğœ . This reachability problem can be solved with discounted RL by rewarding reaching the target state and using a large enough discount factor. The method of <ref type="bibr" target="#b10">[11]</ref> uses a state dependent discount factor ğ›¾ ğµ and a GFM BÃ¼chi automaton. By using a suitable ğ›¾ ğµ and large enough discount factor, one can learn optimal strategies for the ğœ”-regular objective.</p><p>RQ1. How do previous approaches perform in the continuing setting? The methods of <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref> may produce product MDPs that are not communicating (see <ref type="bibr">Example 4.1)</ref>. This means that a single continuing run of the MDP may not explore all relevant states and actions. Thus, previous methods are not guaranteed to converge in this setting. We studied if this behavior affects these prior methods in practice. As a baseline, we include our proposed approach. Instead of tuning hyperparameters for each method, where hyperparameters that lead to convergence may not exist, we take a sampling approach. We select a wide distribution over hyperparameters for each method and sample 200 hyperparameter combinations for each method and example. We then train for 10 million steps on each combination. The selected hyperparameter distribution is ğ›¼ âˆ¼ D (0.01, 0.5), ğœ€ âˆ¼ D (0.01, 1.0), ğ‘ âˆ¼ D <ref type="bibr" target="#b0">(1,</ref><ref type="bibr">200)</ref>, ğœ‚ âˆ¼ D (0.01, 0.5), ğœ âˆ¼ D (0.5, 0.995), ğ›¾ ğµ âˆ¼ D (0.5, 0.995), and ğ›¾ âˆ¼ D (0.99, 0.99999) where D (ğ‘, ğ‘) is a log-uniform distribution from ğ‘ to ğ‘. The end points of these distributions and the training amount are selected by finding hyperparameters which lead to convergence in the episodic setting for these methods.</p><p>Figure <ref type="figure" target="#fig_7">6</ref> shows the resulting distribution over runs. A distribution entirely at 0 <ref type="bibr" target="#b0">(1)</ref> indicates that all sampled runs produced strategies that satisfy the specification with probability 0 (1). For many examples, prior approaches have no successful hyperparameter combinations, with distributions centered entirely at 0. However, our proposed approach always have some hyperparameters that lead to optimal, probability 1, strategies, as indicated by the tails of the distributions touching the probability 1 region of the plot. indicates results from Q-learning with reduction from <ref type="bibr" target="#b31">[32]</ref>, while superscript â€¡ indicates Q-learning with reduction from <ref type="bibr" target="#b10">[11]</ref>.</p><p>Results for â€  and â€¡ require episodic resetting. All hyperparameters are tuned by hand.</p><p>RQ2. How does our method compare to previous approaches when we allow episodic setting? By allowing episodic resetting, we can now find hyperparameters for previous methods that lead to convergence. We tuned all hyperparameters by hand to minimize training time, while verifying with a model checker that the produced strategies are optimal. Table <ref type="table" target="#tab_0">1</ref> shows learning times, as well as hyperparameters for our reduction. We report the number of states reachable in the MDP and the product, learning times averaged over 5 runs, the reset penalty ğ‘, the ğœ€-greedy exploration rate ğœ€, the learning rates ğ›¼ and ğœ‚ of the Differential Q-learning, as well as the number of training steps. Note that we do not do any episodic resetting when training with our reduction. This means that the RL agent must learn to recover from mistakes during training, while previous approaches are periodically reset to a good initial state. Our reduction using Differential Q-learning is competitive with previous approaches while not being reliant on episodic resetting.</p><p>RQ3. How our approach works in multi-objective settings? As shown in Table <ref type="table" target="#tab_2">2</ref>, we successfully satisfied the absolute liveness property with probability one across all case studies. We report the number of states reachable in the MDP and the product, the learning times averaged over five runs, the average reward over five runs, the reset penalty ğ‘ and ğ›½, the ğœ€-greedy exploration rate ğœ€, the learning rates ğ›¼ and ğœ‚ of Differential Q-learning, as well as the number of training steps. Note that, also in the multi-objective setting, we do not perform any episodic resetting when training with our reduction. We achieved average rewards that are almost optimal in all cases (the optimal value for the first case study is 1 and for the rest is 0).   <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref>. These logics have equivalent automaton and reward machine representations that have catalyzed a series of efforts on defining novel reward shaping functions to accelerate the convergence of RL algorithms subject to formal specifications <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">40]</ref>. These methods leverage the graph structure of the automaton to provide an artificial reward signal to the agent. More recently, dynamic reward shaping using LTL ğ‘“ has been introduced as a means to both learn the transition values of a given reward machine and leverage these values for reward shaping and transfer learning <ref type="bibr" target="#b90">[91]</ref>. There has also been work on learning or synthesizing the entire structure of such reward machines from agent interactions with the environment by leveraging techniques from satisfiability and active grammatical inference <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b96">97,</ref><ref type="bibr" target="#b97">98]</ref>. Other data-driven methods have also been developed recently for satisfying specifications over finite traces. Data-driven distributionally robust policy synthesis approaches are presented in <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b74">75]</ref>. Data-driven construction of abstractions with correctness guarantees has also been studied <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b72">73]</ref>, which enables policy synthesis against temporal specifications. Data-driven computation of resilience with respect to finite temporal logic specifications is studied in <ref type="bibr" target="#b71">[72]</ref>.</p><p>Formal Specifications over Infinite Traces. For the infinite-trace settings, LTL has been extensively used to verify specifications and synthesize policies formally using the mathematical model of a system <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b98">99]</ref>. Considering the generality of the available results in terms of structure of the underlying MDP, most of the research focuses on discounted reward structures. Despite the simplicity of discounted Markov decision problems, the discounted reward structure (unlike average reward) prioritizes the transient response of the system. However, application of the average reward objective because of the restriction over the structure of the MDP is limited. The work <ref type="bibr" target="#b21">[22]</ref> proposes a policy iteration algorithm for satisfying specifications of the form G F ğœ™ âˆ§ ğœ“ for a communicating MDP almost surely. The work <ref type="bibr" target="#b2">[3]</ref> proposes a value iteration algorithm for solving the average reward problem for multichain MDPs, where the algorithm first computes the optimal value for each of strongly connected components and then weighted reachability to find the optimal policy. The work <ref type="bibr">[4]</ref> provides a linear program for policy synthesis of multichain MDPs with steady-state constraints. The work <ref type="bibr" target="#b56">[57]</ref> provides a regret-free learning algorithm for policy synthesis that is based on identifying the structure of the underlying MDP using data with a certain confidence.</p><p>RL for Formal Specifications over Infinite Traces. In the last few years, researchers have started developing data-driven policy synthesis techniques in order to satisfy temporal specifications. There is a large body of literature in safe reinforcement learning (RL) (see e.g. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b69">70]</ref>). The problem of learning a policy to maximize the satisfaction probability of a temporal specification using discounted RL is studied recently <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b70">71]</ref>.</p><p>The work <ref type="bibr" target="#b31">[32]</ref> uses a parameterized augmented MDP to provide an RL-based policy synthesis for finite MDPs with unknown transition probabilities. It shows that the optimal policy obtained by RL for the reachability probability on the augmented MDP gives a policy for the MDP with a suitable convergence guarantee. In <ref type="bibr" target="#b10">[11]</ref> authors provide a path-dependent discounting mechanism for the RL algorithm based on a limit-deterministic BÃ¼chi automaton (LDBA) representation of the underlying ğœ”-regular specification, and prove convergence of their approach on finite MDPs when the discounting factor goes to one. An LDBA is also leveraged in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b67">68]</ref> for discounted-reward model-free RL in both continuous-and discrete-state MDPs. The LDBA is used to define a reward function that incentivizes the agent to visit all accepting components of the automaton. These works use episodic discounted RL with discount factor close to one to solve the policy synthesis problem. There are two issues with the foregoing approaches. First, because of the episodic nature of the algorithms they are not applicable in continuing settings. Second, because of high discount factors in practice these algorithm are difficult to converge. On the other hand, recent work on reward shaping for average reward RL has been explored based on safety specifications to be satisfied by the synthesized policy <ref type="bibr" target="#b42">[43]</ref>. In contrast to the solution proposed in this paper, the preceding approach requires knowledge of the graph structure of the underlying MDP and does not account for absolute liveness specifications.</p><p>Average-Reward RL. There is a rich history of studies in average reward RL <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b53">54]</ref>. Lack of stopping criteria for multichain MDPs affect the generality of model-free RL algorithms. Therefore, all model-free RL algorithms put some restrictions on the structure of MDP (e.g. ergodicity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b95">96]</ref> or communicating property). The closest line of work to this work is to use average reward objective for safe RL. The work <ref type="bibr" target="#b81">[82]</ref> proposes a model-based RL algorithm for maximizing average reward objective with safety constraint for communicating MDPs. It is worth noting that in multichain setting, the state-of-the-art learning algorithms use model-based RL algorithms. The work <ref type="bibr" target="#b49">[50]</ref> studies satisfaction of ğœ”-regular specifications using data-driven approaches. The authors introduce an algorithm where the optimality of the policy is conditioned to not leaving the corresponding maximal end component which leads to a sub-optimal solution. The authors provide PAC analysis for the algorithm as well.</p><p>Multi-objective Probabilistic Verification. Chatterjee, Majumadar, and Henzinger <ref type="bibr" target="#b16">[17]</ref> considered MDPs with multiple discounted reward objectives. In the presence of multiple objectives, the trade-offs between different objectives can be characterized as Pareto curves. The authors of <ref type="bibr" target="#b16">[17]</ref> showed that every Pareto optimal point can be achieved by a memoryless strategy and the Pareto curve can be approximated in polynomial time. Moreover, the problem of checking the existence of a strategy that realizes a value vector can be decided in polynomial time. These multi-objective optimization problems were studied in the context of multiple long-run average objectives by Chatterjee <ref type="bibr" target="#b15">[16]</ref>. He showed that the Pareto curve can be approximated in polynomial time in the size of the MDP for irreducible MDPs and in polynomial space in the size of the MDP for general MDPs. Additionally, the problem of checking the existence of a strategy that guarantees values for different objectives to be equal to a given vector is in polynomial time for irreducible MDPs and in NP for general MDPs. Etessami et al. <ref type="bibr" target="#b23">[24]</ref> were the first to study the multi-objective model-checking problem for MDPs with ğœ”-regular objectives. Given probability intervals for the satisfaction of various properties, they developed a polynomial-time (in the size of the MDP) algorithm to decide the existence of such a strategy. They also showed that, in general, such strategies may require both randomization and memory. That paper also studies the approximation of the Pareto curve with respect to a set of ğœ”-regular properties in time polynomial in the size of the MDP. Forejt et al. <ref type="bibr" target="#b24">[25]</ref> studied quantitative multi-objective optimization over MDPs that combines ğœ”-regular and quantitative objectives. Those algorithms are implemented in the probabilistic model checker PRISM <ref type="bibr" target="#b50">[51]</ref>. Multi-objective optimization on interval MDPs is studied by Monir et al. <ref type="bibr" target="#b62">[63]</ref> to provide a Lyapunov-based policy synthesis approach.</p><p>Multi-objective Reinforcement Learning. There has been substantial work on lexicographic objectives in RL, including lexicographic discounted objectives <ref type="bibr" target="#b83">[84]</ref>, lexicographic ğœ”-regular objectives <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, and a combination of safety and discounted objectives <ref type="bibr" target="#b8">[9]</ref>. Hahn et al. <ref type="bibr" target="#b35">[36]</ref> considered the general class of ğœ”-regular objectives with discounted rewards in model-free RL. However, to the best of our knowledge, this paper is the first work that considers lexicographic ğœ”-regular objectives with average objectives.</p><p>Average-Reward RL for Formal Specifications. Despite significant progress in data-driven approaches for satisfying ğœ”-regular specifications, there remains a gap in the use of average-reward, model-free RL algorithms for satisfying temporal logic specifications. Our preliminary work presented at AAMAS 2022 <ref type="bibr" target="#b44">[45]</ref> aimed to address this gap by proposing a model-free average-reward RL algorithm tailored to a subclass of LTL specifications known as absolute liveness specifications. We argue that this subclass captures a broad range of practically relevant specifications and is particularly well-suited to the average-reward RL setting. This manuscript builds on <ref type="bibr" target="#b44">[45]</ref> and extends the results in the following directions: (a) we generalize our results from communicating MDPs to the broader class of weakly communicating MDPs; (b) we incorporate a lexicographic multi-objective optimization framework, wherein the goal is to maximize a mean-payoff objective among policies that satisfy a given liveness specification; (c) we provide novel automata-theoretic characterizations of absolute liveness and stable specifications; and (d) we conduct an experimental evaluation demonstrating the effectiveness of the proposed lexicographic multi-objective approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This work addressed the problem of synthesizing policies that satisfy a given absolute liveness ğœ”-regular specification while maximizing a mean-payoff objective in the continuing setting. Our first key contribution is a model-free translation from ğœ”-regular specifications to an average-reward objective, enabling the use of off-the-shelf average-reward reinforcement learning algorithms. In contrast to existing approaches that rely on discounted, episodic learning, which require environment resets and may be infeasible in many real-world settings, our approach learns optimal policies in a single, life-long episode without resetting.</p><p>Our second contribution is a solution to a lexicographic multi-objective optimization problem, where the goal is to maximize a mean-payoff objective among the set of policies that satisfy a given liveness specification. We provide convergence guarantees under the assumption that the underlying Markov Decision Process is communicating. Importantly, our solutions are model-free and do not require access to the environment's transition structure or its graph representation. This removes the common assumption in prior work that synthesis requires the computation of end components in the product MDP.</p><p>We implemented our approach using Differential Q-learning and evaluated it on a range of case studies. The experimental results demonstrate that our method reliably converges to optimal strategies under the stated assumptions and outperforms existing approaches in the continuing setting. These results support the important and often overlooked hypothesis that average-reward RL is better suited to continuing tasks than discounted RL. As future work, we plan to explore the use of function approximation, with the aim of enabling average-reward RL to achieve the same level of success for continuing tasks as discounted RL has achieved in episodic settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 2 . 2 (</head><label>22</label><figDesc>Probabilistic Reward Machines). A probabilistic reward machine is a tuple R = (Î£ ğœ– , ğ‘ˆ Ã— ğ‘ˆ ğ‘ , (ğ‘¢ 0 , ğ‘¢ ğ‘ 0 ), ğ›¿ ğ‘Ÿ ,ğ‘‡ ğ‘ , ğœŒ) where â€¢ Î£ ğœ– = (Î£ âˆª {ğœ–}) with Î£ being a finite alphabet and ğœ– indicating a silent transition, â€¢ ğ‘ˆ and ğ‘ˆ ğ‘ are two finite sets of states, â€¢ (ğ‘¢ 0 , ğ‘¢ ğ‘ 0 ) âˆˆ ğ‘ˆ Ã— ğ‘ˆ ğ‘ is the starting state,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>JAIR, Vol. 1, Article . Publication date: May 2025.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Automaton of F G ğ‘, dashed lines represent resets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Lemma 4 . 6 (</head><label>46</label><figDesc>Preservation of Weak Communication). For a weakly communicating MDP M and reward machine R A for a fairness GFM automaton A, the product MÃ—R A is weakly communicating.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 ğ›½(Fig. 5 .Fig. 4 .</head><label>154</label><figDesc>Fig. 5. Picture of the construction of the probabilistic reward machine in (5.1). Left: Two layers corresponding to ğ‘ âˆˆ {0, 1}. Right: Probabilistic changes of the additional bit ğ‘. The ğœ–-transitions are excluded for a better pictorial presentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( 1 âˆ’ 2 )</head><label>12</label><figDesc>lim ğ‘–â†’âˆ Rğœ * (ğ›½ (ğ‘–)) â‰¥ lim ğ‘–â†’âˆ ğ›½ (ğ‘–))ğ‘£ * + ğ›½ (ğ‘–)ğ‘ ğ‘“ ğœ * ( 1 = ğ‘£ * , so ğœ * is a lexicographically optimal policy. â–¡ These results can be extended to weakly communicating MDPs in straightforward fashion. Lemma 5.5 (Preservation of Communication). For a weakly-communicating MDP M and reward machine R Aâ€¢ğœŒ over a fairness specification, the resulting product M Ã— R Aâ€¢ğœŒ is weakly-communicating. Proof. The proof follows directly from Lemma 4.6 and Lemma 5.1. As stated for Lemma 4.6 we can partition the states of the MDP into two sets and the sub-product MDP resulted from the second set with the fairness specification is communicating based on Lemma 5.1. Moreover, since all of the states of the first set, see the proof of Lemma 4.6, are transient the resulting product MDP is weakly communicating. â–¡</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>a d v e r s e f r o z e n S m a l l f r o z e n L a r g e w i n d y w i n d y S t o c h g r i d 5 x 5 i s h i f t d o u b l e g r i d b u s y R i n</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Comparison of the distributions of probability of satisfaction of learned policies across sampled hyperparameters in the continuing setting. For each distribution, the mean is shown as a circle, and the maximum and minimum are shown as vertical bars. We compare our proposed reduction, the reduction of<ref type="bibr" target="#b31">[32]</ref> with Q-learning, and the reduction of<ref type="bibr" target="#b10">[11]</ref> with Q-learning. Episodic resetting is not used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Learning results and comparison. Hyperparameters used for our reduction are shown. Blank entries indicate that default values are used. The default parameters are ğ‘ = âˆ’1, ğœ€ = 0.1, ğ›¼ = 0.1, and ğœ‚ = 0.1. Times are in seconds. Superscript â€ </figDesc><table><row><cell>Name adverse frozenSmall frozenLarge windy windyStoch grid5x5 ishift doublegrid busyRingMC2 busyRingMC4 2592 15426 6.08 states prod. time time  â€  time  â€¡ 202 507 8.51 7.09 12.56 -150 ğ‘ 16 64 0.99 20.23 9.88 64 256 4.07 3.88 8.79 123 366 1.40 1.81 2.61 130 390 2.97 3.91 2.53 25 100 0.62 1.12 1.02 4 29 0.03 0.01 0.02 1296 5183 16.43 3.45 3.09 -2 72 288 0.03 0.03 0.03 3.94 2.33</cell><cell>ğœ€ 0.95 0.5 0.05 ğ›¼ ğœ‚ 0.2 0.02 0.02 0.5 0.5 0.5 0.05 0.01 0.01 0.01</cell><cell>train-steps 10M 500k 3M 1M 2M 200k 10k 12M 10k 1.5M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Learning results for multi-objective case studies. The default parameters are tol = 0, ğ›½ = 0.05, ep-n = 1, ep-l = 1000000, ğ›¼ = 0.01, ğœ‚ = 0.01, ğœ€ = 0.1. Times are in seconds. All hyperparameters are tuned by hand.Learning for Formal Specifications over Finite Traces. The development and use of formal reward structures for RL have witnessed increased interest in recent years. For episodic RL, logics have been developed over finite traces of the agent's behavior, including LTL ğ‘“ and Linear Dynamic Logic (LDL ğ‘“ )</figDesc><table><row><cell>7 Related Work</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">â€¢ Kazemi et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">JAIR, Vol. 1, Article . Publication date: May 2025.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2">The implementation is available at https://plv.colorado.edu/mungojerrie/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3">Case studies are available at https://plv.colorado.edu/mungojerrie/aamas22.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research of S. Soudjani is supported by the following grants: EIC 101070802 and ERC 101089047. This research was also supported in part by the National Science Foundation (NSF) through CAREER Award CCF-2146563. Ashutosh Trivedi holds the position of Royal Society Wolfson Visiting Fellow and gratefully acknowledges the support of the Wolfson Foundation and the Royal Society for this fellowship.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ğ›¿ â€² ((ğ‘, ğ‘),(ğ‘™, ğ‘ , ğ‘  â€² , ğ‘ â€² )) ((ğ‘ â€² , ğ‘</p><p>ğ›½ ğ‘ = 0, ğ‘ â€² = 1, ğ‘ â€² âˆˆ ğ›¿ (ğ‘, ğ‘™), ğ‘™ âˆ‰ {ğœ– 1 , ğœ– 2 } 1 âˆ’ ğ›½ ğ‘ = 0, ğ‘ â€² = 0, ğ‘ â€² âˆˆ ğ›¿ (ğ‘, ğ‘™), ğ‘™ âˆ‰ {ğœ– 1 , ğœ– 2 } 1 ğ‘ = 1, ğ‘ â€² = 1, ğ‘ â€² âˆˆ ğ›¿ (ğ‘, ğ‘™), (ğ‘, ğ‘™, ğ‘ â€² ) âˆ‰ ğ¹, ğ‘™ âˆ‰ {ğœ– 1 , ğœ– 2 } 1 ğ‘ = 1, ğ‘ â€² = 0, ğ‘ â€² âˆˆ ğ›¿ (ğ‘, ğ‘™), (ğ‘, ğ‘™, ğ‘ â€² ) âˆˆ ğ¹, ğ‘™ âˆ‰ {ğœ– 1 , ğœ– 2 } 1 ğ‘ â‰  ğ‘ 0 , ğ‘ â€² = ğ‘ 0 , ğ‘ â€² = ğ‘, ğ‘™ = ğœ– 1 1 ğ‘ â€² = ğ‘, ğ‘ = 1, ğ‘ â€² = 0, ğ‘™ = ğœ– 2 0 otherwise <ref type="bibr">(5.1)</ref> and ğœŒ â€² ((ğ‘, ğ‘), (ğ‘™, ğ‘ , ğ‘  â€² , ğ‘ â€² ), (ğ‘ â€² , ğ‘ â€² ))</p><p>We now show a few results before proceeding to the main theorems.</p><p>Lemma 5.1 (Preservation of Communication). For a communicating MDP M and reward machine R Aâ€¢ğœŒ defined above, the resulting product M Ã— R Aâ€¢ğœŒ is communicating.</p><p>Proof. The claim holds due to the additions of ğœ–-transitions {ğœ– 1 , ğœ– 2 } that resets both the automaton (ğœ– 1 ) and the extra bit ğ‘ to zero (ğœ– 2 ). â–¡ To develop our formal proof in Theorem 5.3, we make use of the following observation. Lemma 5.2 (Binary Probability of Satisfaction). The probability of satisfaction of an absolute liveness specification is either 0 or 1 in a communicating MDP.</p><p>Proof. To show this result, we just need to show that if the probability of satisfaction of the specification is positive, then the probability of satisfaction must be 1. This follows from the fact that if the probability of satisfaction is positive, then there is a winning end-component, and this end-component can be reached with probability 1 due to the communicating nature of the product. â–¡ Theorem 5.3 (Lexicographic Reward Machine ğœ€-optimality). Let ğ‘£ * be the external average reward obtained under a lexicographically optimal strategy for Problem 2.8. For every ğœ€ &gt; 0, there exists a threshold ğ›½ * &gt; 0 such that for all 0 &lt; ğ›½ &lt; ğ›½ * there exists a ğ‘ * &lt; 0 such that for all ğ‘ &lt; ğ‘ * all positional policies that maximize the average reward on M Ã— R Aâ€¢ğœŒ maximize the probability of satisfaction of A on M and achieves an average reward ğ‘£ such that ğ‘£ â‰¥ ğ‘£ * âˆ’ ğœ€. We call such a policy ğœ€-optimal. Additionally, these policies are finite memory policies on M.</p><p>Proof. We first note that since the memory required in the resulting policies is defined by R Aâ€¢ğœŒ with fixed parameters, the resulting policies are finite memory. From Lemma 5.2, we can proceed by considering two cases: the ğœ”-regular objective is satisfied with probability 0 or 1.</p><p>(1) We first consider the case where the ğœ”-regular objective is satisfied with probability 0 under any policy, i.e., there is no policy that can satisfy the specification with positive probability. We will show that any 0 &lt; ğ›½ * &lt; 1 and ğ‘ * &lt; 0 works. Since the lexicographically optimal policy only needs to maximize the external average reward, there is an optimal policy that is positional on M that achieves an external average reward ğ‘£ * . Consider an optimal policy ğœ on the extended product M Ã— R </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Reproducibility Checklist for JAIR</head><p>Select the answers that apply to your research -one per item.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All articles:</head><p>( Articles reporting on computational experiments:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does this paper include computational experiments? [yes]</head><p>If yes, please complete the list below.</p><p>(1) All source code required for conducting experiments is included in an online appendix or will be made publicly available upon publication of the paper. The online appendix follows best practices for source code readability and documentation as well as for long-term accessibility.</p><p>[yes] (2) The source code comes with a license that allows free usage for reproducibility purposes. [yes] (3) The source code comes with a license that allows free usage for research purposes in general. [yes] (4) Raw, unaggregated data from all experiments is included in an online appendix or will be made publicly available upon publication of the paper. The online appendix follows best practices for long-term accessibility.</p><p>[NA] (5) The unaggregated data comes with a license that allows free usage for reproducibility purposes. [NA] <ref type="bibr" target="#b5">(6)</ref> The unaggregated data comes with a license that allows free usage for research purposes in general. [NA] ( <ref type="formula">7</ref>) If an algorithm depends on randomness, then the method used for generating random numbers and for setting seeds is described in a way sufficient to allow replication of results.</p><p>[yes] (8) The execution environment for experiments, the computing infrastructure (hardware and software) used for running them, is described, including GPU/CPU makes and models; amount of memory (cache and RAM); make and version of operating system; names and versions of relevant software libraries and frameworks.</p><p>[yes] (9) The evaluation metrics used in experiments are clearly explained and their choice is explicitly motivated.</p><p>[yes] (10) The number of algorithm runs used to compute each result is reported. [yes] (11) Reported results have not been "cherry-picked" by silently ignoring unsuccessful or unsatisfactory experiments.</p><p>[yes] (12) Analysis of results goes beyond single-dimensional summaries of performance (e.g., average, median) to include measures of variation, confidence, or other distributional information.</p><p>[yes] (13) All (hyper-) parameter settings for the algorithms/methods used in experiments have been reported, along with the rationale or method for determining them. (1) All newly introduced data sets are included in an online appendix or will be made publicly available upon publication of the paper. The online appendix follows best practices for long-term accessibility with a license that allows free usage for research purposes. [yes/partially/no/NA] (2) The newly introduced data set comes with a license that allows free usage for reproducibility purposes.</p><p>[yes/partially/no] (3) The newly introduced data set comes with a license that allows free usage for research purposes in general.</p><p>[yes/partially/no] (4) All data sets drawn from the literature or other public sources (potentially including authors <ref type="bibr">'</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Politex: Regret bounds for policy iteration using expert prediction</title>
		<author>
			<persName><forename type="first">Yasin</forename><surname>Abbasi-Yadkori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kush</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">GellÃ©rt</forename><surname>Weisz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3692" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Defining Liveness</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Process. Lett</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="181" to="185" />
			<date type="published" when="1985-10">1985. Oct. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Value iteration for long-run average reward in Markov decision processes</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnendu</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">PrzemysÅ‚aw</forename><surname>Daca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Aided Verification</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017-01">Jan KÅ™etÃ­nská»³, and Tobias Meggendorfer. 2017</date>
			<biblScope unit="page" from="201" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Steady-state planning in expected reward multichain mdps</title>
		<author>
			<persName><forename type="first">Andre</forename><surname>George K Atia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ismail</forename><surname>Beckus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Alkhouri</surname></persName>
		</author>
		<author>
			<persName><surname>Velasquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="1029" to="1082" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Principles of Model Checking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Katoen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambuj</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><surname>Tewari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1205.2661</idno>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Formal methods for control synthesis: An optimization perspective</title>
		<author>
			<persName><forename type="first">Calin</forename><surname>Belta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadra</forename><surname>Sadraddini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics, and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="115" to="140" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>Annual Review of Control</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Stochastic hybrid systems: theory and safety critical applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Henk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Blom</surname></persName>
		</author>
		<author>
			<persName><surname>Lygeros</surname></persName>
		</author>
		<author>
			<persName><surname>Everdij</surname></persName>
		</author>
		<author>
			<persName><surname>Loizou</surname></persName>
		</author>
		<author>
			<persName><surname>Kyriakopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">337</biblScope>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model-Free Learning of Safe yet Effective Controllers</title>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Alper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Bozkurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Pajic</surname></persName>
		</author>
		<idno type="DOI">10.1109/CDC45484.2021.9683634</idno>
		<ptr target="https://doi.org/10.1109/CDC45484.2021.9683634" />
	</analytic>
	<monogr>
		<title level="m">2021 60th IEEE Conference on Decision and Control (CDC)</title>
				<meeting><address><addrLine>Austin, TX, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-12-14">2021. December 14-17, 2021</date>
			<biblScope unit="page" from="6560" to="6565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Control synthesis from linear temporal logic specifications using model-free reinforcement learning</title>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Alper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Bozkurt</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Michael M Zavlanos</surname></persName>
		</author>
		<author>
			<persName><surname>Pajic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10349" to="10355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Control synthesis from linear temporal logic specifications using model-free reinforcement learning</title>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Alper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Bozkurt</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Michael M Zavlanos</surname></persName>
		</author>
		<author>
			<persName><surname>Pajic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10349" to="10355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Verification of Markov decision processes using learning algorithms</title>
		<author>
			<persName><forename type="first">TomÃ¡Å¡</forename><surname>BrÃ¡zdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnendu</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Chmelik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">VojtÄ›ch</forename><surname>Forejt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>KÅ™etÃ­nská»³</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Ujma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automated Technology for Verification and Analysis (ATVA)</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="98" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Synthesis of LTL formulas from natural language texts: State of the art and research directions</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Brunello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelo</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th International Symposium on Temporal Representation and Reasoning</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finite LTL synthesis as planning</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Camacho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><forename type="middle">A</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Muise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheila</forename><forename type="middle">A</forename><surname>Mcilraith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth International Conference on Automated Planning and Scheduling</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">LTL and Beyond: Formal Languages for Reward Function Specification in Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Camacho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Toro Icarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Anthony</forename><surname>Toryn Q Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheila</forename><forename type="middle">A</forename><surname>Valenzano</surname></persName>
		</author>
		<author>
			<persName><surname>Mcilraith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="6065" to="6073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Markov Decision Processes with Multiple Long-Run Average Objectives</title>
		<author>
			<persName><forename type="first">Krishnendu</forename><surname>Chatterjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FSTTCS 2007: Foundations of Software Technology and Theoretical Computer Science</title>
				<editor>
			<persName><forename type="first">Arvind</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sanjiva</forename><surname>Prasad</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="473" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Markov decision processes with multiple objectives</title>
		<author>
			<persName><forename type="first">Krishnendu</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rupak</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Henzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual symposium on theoretical aspects of computer science</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="325" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Formal verification of probabilistic systems</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfaro</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Ph. D. Dissertation. Stanford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Foundations for restraining bolts: Reinforcement learning with LTLf/LDLf restraining specifications</title>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giacomo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Iocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Favorito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Patrizi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Automated Planning and Scheduling</title>
				<meeting>the International Conference on Automated Planning and Scheduling</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="128" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Synthesis for LTL and LDL on finite traces</title>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giacomo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Vardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Average-reward model-free reinforcement learning: a systematic review and literature mapping</title>
		<author>
			<persName><forename type="first">Vektor</forename><surname>Dewanto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Eshragh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Roosta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08920</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimal control of Markov decision processes with linear temporal logic constraints</title>
		<author>
			<persName><forename type="first">Xuchu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calin</forename><surname>Belta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Control</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="1244" to="1257" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Exploration-exploitation in constrained MDPs</title>
		<author>
			<persName><forename type="first">Yonathan</forename><surname>Efroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Pirotta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02189</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-objective Model Checking of Markov Decision Processes</title>
		<author>
			<persName><forename type="first">K</forename><surname>Etessami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Vardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yannakakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tools and Algorithms for the Construction and Analysis of Systems</title>
				<editor>
			<persName><forename type="first">Orna</forename><surname>Grumberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Huth</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="50" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Quantitative Multi-objective Verification for Probabilistic Systems</title>
		<author>
			<persName><forename type="first">Marta</forename><surname>Vojtech Forejt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gethin</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tools and Algorithms for the Construction and Analysis of Systems</title>
				<editor>
			<persName><forename type="first">Parosh</forename><surname>Aziz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Abdulla</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Rustan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Leino</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="112" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Probably Approximately Correct MDP Learning and Control With Temporal Logic Constraints</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ufuk</forename><surname>Topcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Robotics: Science and Systems</title>
				<meeting>Robotics: Science and Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reinforcement Learning with Non-Markovian Rewards</title>
		<author>
			<persName><forename type="first">Maor</forename><surname>Gaon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Brafman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3980" to="3987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A comprehensive survey on safe reinforcement learning</title>
		<author>
			<persName><forename type="first">Javier</forename><surname>GarcÄ±a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>FernÃ¡ndez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1437" to="1480" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Formal Multi-Objective Synthesis of Continuous-State MDPs</title>
		<author>
			<persName><forename type="first">Sofie</forename><surname>Haesaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petter</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Systems Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1765" to="1770" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust dynamic programming for temporal logic control of stochastic systems</title>
		<author>
			<persName><forename type="first">Sofie</forename><surname>Haesaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Control</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="2496" to="2511" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lazy Probabilistic Model Checking without Determinisation</title>
		<author>
			<persName><forename type="first">Ernst</forename><surname>Moritz Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Schewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Turrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Concurrency Theory (CONCUR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="354" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Omega-regular objectives in model-free reinforcement learning</title>
		<author>
			<persName><forename type="first">Ernst</forename><surname>Moritz Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Schewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Somenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Wojtczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS)</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="395" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Good-for-MDPs automata for probabilistic analysis and reinforcement learning</title>
		<author>
			<persName><forename type="first">Ernst</forename><surname>Moritz Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Schewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Somenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Wojtczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Tools and Algorithms for the Construction and Analysis of Systems</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="306" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Model-Free Reinforcement Learning for Lexicographic Omega-Regular Objectives</title>
		<author>
			<persName><forename type="first">Ernst</forename><surname>Moritz Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Schewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Somenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Wojtczak</surname></persName>
		</author>
		<idno>FM 2021. 142-159. LNCS 13047</idno>
	</analytic>
	<monogr>
		<title level="m">Formal Methods</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Model-free reinforcement learning for lexicographic omega-regular objectives</title>
		<author>
			<persName><forename type="first">Ernst</forename><surname>Moritz Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Schewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Somenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Wojtczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International symposium on formal methods</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="142" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Omega-regular reward machines</title>
		<author>
			<persName><forename type="first">Ernst</forename><surname>Moritz Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Schewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Somenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Wojtczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI 2023</title>
				<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="972" to="979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Certified reinforcement learning with logic guidance</title>
		<author>
			<persName><forename type="first">Mohammadhosein</forename><surname>Hasanbeig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Abate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kroening</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00778</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reinforcement learning for temporal logic control synthesis with probabilistic satisfaction guarantees</title>
		<author>
			<persName><forename type="first">Mohammadhosein</forename><surname>Hasanbeig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiannis</forename><surname>Kantaros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Abate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kroening</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">J</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Insup</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Decision and Control (CDC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5338" to="5343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Using reward machines for high-level task specification and decomposition in reinforcement learning</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Toro Icarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toryn</forename><surname>Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Valenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheila</forename><surname>Mcilraith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2107" to="2116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Using reward machines for high-level task specification and decomposition in reinforcement learning</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Toro Icarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toryn</forename><surname>Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Valenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheila</forename><surname>Mcilraith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2107" to="2116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Toro Icarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toryn</forename><forename type="middle">Q</forename><surname>Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Anthony</forename><surname>Valenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheila</forename><forename type="middle">A</forename><surname>Mcilraith</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.03950" />
		<title level="m">Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning. CoRR abs/2010</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page">3950</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Formal synthesis of stochastic systems via control barrier certificates</title>
		<author>
			<persName><forename type="first">Pushpak</forename><surname>Jagtap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Majid</forename><surname>Zamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Control</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Temporal-Logic-Based Reward Shaping for Continuing Reinforcement Learning Tasks</title>
		<author>
			<persName><forename type="first">Yuqian</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suda</forename><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ufuk</forename><surname>Topcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<publisher>Good Systems-Published Research</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Data-driven abstraction-based control synthesis</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rupak</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Salamati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wooding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Analysis: Hybrid Systems</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">101467</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Translating omega-regular specifications to average objectives for model-free reinforcement learning</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Somenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Velasquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems</title>
				<meeting>the 21st International Conference on Autonomous Agents and Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="732" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Assume-guarantee reinforcement learning</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Somenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Velasquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="21223" to="21231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Formal policy synthesis for continuous-state systems via reinforcement learning</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Integrated Formal Methods</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Data-Driven Distributionally Robust Control for Interacting Agents under Logical Constraints</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Bahari Kordabad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleftherios</forename><forename type="middle">E</forename><surname>Vlahakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Lindemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Gros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimos</forename><forename type="middle">V</forename><surname>Dimarogonas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control (under review</title>
		<imprint>
			<date type="published" when="2025">2025. 2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Synthesis for robots: Guarantees and feedback for robot behavior</title>
		<author>
			<persName><forename type="first">Hadas</forename><surname>Kress-Gazit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morteza</forename><surname>Lahijanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasumathi</forename><surname>Raman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics, and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="211" to="236" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>Annual Review of Control</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Finite-Memory Near-Optimal Learning for Markov Decision Processes with Long-Run Average Reward</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>KretÃ­nskÃ½</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1149" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">PRISM 4.0: Verification of Probabilistic Real-time Systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Aided Verification (CAV)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6806</biblScope>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Formal controller synthesis for continuous-space MDPs via model-free reinforcement learning</title>
		<author>
			<persName><forename type="first">Abolfazl</forename><surname>Lavaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Somenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Majid</forename><surname>Zamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Cyber-Physical Systems (ICCPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="98" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Automated verification and synthesis of stochastic hybrid systems: A survey</title>
		<author>
			<persName><forename type="first">Abolfazl</forename><surname>Lavaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Abate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Majid</forename><surname>Zamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page">110617</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Average reward reinforcement learning: Foundations, algorithms, and empirical results</title>
		<author>
			<persName><surname>Sridhar Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="159" to="195" />
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Symbolic Qualitative Control for Stochastic Systems via Finite Parity Games</title>
		<author>
			<persName><forename type="first">Rupak</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Mallik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne-Kathrin</forename><surname>Schmuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IFAC-PapersOnLine</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="127" to="132" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Symbolic Controller Synthesis for BÃ¼Chi Specifications on Stochastic Systems</title>
		<author>
			<persName><forename type="first">Rupak</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Mallik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hybrid Systems: Computation and Control (HSCC) (Sydney, New South Wales, Australia)</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Article 14, 11 pages</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Regret-Free Reinforcement Learning for LTL Specifications</title>
		<author>
			<persName><forename type="first">Rupak</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Salamati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2025">2025. 2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Necessary and Sufficient Certificates for Almost Sure Reachability</title>
		<author>
			<persName><forename type="first">Rupak</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Vr Sathiyanarayana</surname></persName>
		</author>
		<author>
			<persName><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Systems Letters</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A hierarchy of temporal properties</title>
		<author>
			<persName><forename type="first">Zohar</forename><surname>Manna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Pnueli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth annual ACM symposium on Principles of distributed computing</title>
				<meeting>the ninth annual ACM symposium on Principles of distributed computing</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="377" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Resource management with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Hongzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishai</forename><surname>Menache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikanth</forename><surname>Kandula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM workshop on hot topics in networks</title>
				<meeting>the 15th ACM workshop on hot topics in networks</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="50" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Azade Nova, et al. 2021. A graph placement methodology for fast chip design</title>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Yazgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><forename type="middle">Wenjie</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ebrahim</forename><surname>Songhori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young-Joon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Pathak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">594</biblScope>
			<biblScope unit="page" from="207" to="212" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Human-level control through reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015-02">2015. Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Lyapunov-Based Policy Synthesis for Multi-Objective Interval MDPs</title>
		<author>
			<persName><forename type="first">Negar</forename><surname>Monir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>SchÃ¶n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IFAC-PapersOnLine</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Discounted Reinforcement Learning is Not an Optimization Problem</title>
		<author>
			<persName><forename type="first">R</forename><surname>Abhishek Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niko</forename><surname>Shariff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yasui</surname></persName>
		</author>
		<author>
			<persName><surname>Sutton</surname></persName>
		</author>
		<idno>abs/1910.02140</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Sadegh Soudjani, and Alessandro Abate. 2025. Data-Driven Yet Formal Policy Synthesis for Stochastic Nonlinear Dynamical Systems. Learning for Decision and Control</title>
		<author>
			<persName><forename type="first">Mahdi</forename><surname>Nazeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thom</forename><surname>Badings</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Advice-Guided Reinforcement Learning in a non-Markovian Environment</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Neider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Raphael</forename><surname>Gaglione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Gavran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ufuk</forename><surname>Topcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Policy invariance under reward transformations: Theory and application to reward shaping</title>
		<author>
			<persName><forename type="first">Daishi</forename><surname>Andrew Y Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="278" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Reinforcement learning of control policy for linear temporal logic specifications using limit-deterministic BÃ¼chi automata</title>
		<author>
			<persName><forename type="first">Ryohei</forename><surname>Oura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ami</forename><surname>Sakakibara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshimitsu</forename><surname>Ushio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Systems Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="761" to="766" />
			<date type="published" when="2020-07">2020. July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Markov Decision Processes: Discrete Stochastic Dynamic Programming</title>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1st ed.</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A tour of reinforcement learning: The view from continuous control</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics, and Autonomous Systems</title>
		<imprint>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>Annual Review of Control</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A learning based approach to control synthesis of Markov decision processes for linear temporal logic specifications</title>
		<author>
			<persName><forename type="first">Dorsa</forename><surname>Sadigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Coogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shankar Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjit</forename><forename type="middle">A</forename><surname>Seshia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Decision and Control</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1091" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Temporal logic resilience for dynamical systems</title>
		<author>
			<persName><forename type="first">Adnane</forename><surname>Saoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushpak</forename><surname>Jagtap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Atuomatic Control (under review</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Data-Driven Abstractions via Binary-Tree Gaussian Processes for Formal Verification</title>
		<author>
			<persName><forename type="first">Oliver</forename><surname>SchÃ¶n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shammakh</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wooding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IFAC-PapersOnLine</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="115" to="122" />
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Correct-by-Design Control of Parametric Stochastic Systems</title>
		<author>
			<persName><forename type="first">Oliver</forename><surname>SchÃ¶n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Birgit</forename><surname>Van Huijgevoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sofie</forename><surname>Haesaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">61st IEEE Conference on Decision and Control (CDC)</title>
				<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Data-Driven Distributionally Robust Safety Verification Using Barrier Certificates and Conditional Mean Embeddings</title>
		<author>
			<persName><forename type="first">Oliver</forename><surname>SchÃ¶n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengang</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Soudjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2024 American Control Conference (ACC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="3417" to="3423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Mastering atari, Go, chess and shogi by planning with a learned model</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">588</biblScope>
			<biblScope unit="page" from="604" to="609" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Limit-deterministic BÃ¼chi automata for linear temporal logic</title>
		<author>
			<persName><forename type="first">Salomon</forename><surname>Sickert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Esparza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Jaax</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Aided Verification (CAV)</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">Jan KÅ™etÃ­nská»³. 2016</date>
			<biblScope unit="page" from="312" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016-01">2016. Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="page" from="1140" to="1144" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Mastering the game of Go without human knowledge</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="page">354</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Learning in Markov decision processes under constraints</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Ness</surname></persName>
		</author>
		<author>
			<persName><surname>Shroff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12435</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Safety, liveness and fairness in temporal logic</title>
		<author>
			<persName><forename type="first">Sistla</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Formal Aspects of Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="495" to="511" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<author>
			<persName><forename type="first">Joar</forename><surname>Skalse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Abate</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.13769</idno>
		<title level="m">Lexicographic multi-objective reinforcement learning</title>
				<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">PAC model-free reinforcement learning</title>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Alexander L Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wiewiora</surname></persName>
		</author>
		<author>
			<persName><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><surname>Michael L Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
				<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="881" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Model-based average reward reinforcement learning</title>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dokyeong</forename><surname>Ok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="177" to="224" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for robotics: A survey of real-world successes</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Abbatematteo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>MartÃ­n-MartÃ­n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2025">2025</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="28694" to="28698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Learning reward machines for partially observable reinforcement learning</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Toro Icarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Waldie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toryn</forename><surname>Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick</forename><surname>Valenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margarita</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheila</forename><surname>Mcilraith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="15523" to="15534" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Automatic verification of probabilistic concurrent finite state programs</title>
		<author>
			<persName><forename type="first">Moshe</forename><forename type="middle">Y</forename><surname>Vardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th Annual Symposium on Foundations of Computer Science (SFCS 1985)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="327" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Dynamic Automaton-Guided Reward Shaping for Monte Carlo Tree Search</title>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Velasquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brett</forename><surname>Bissey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Beckus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ismail</forename><surname>Alkhouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Melcer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Atia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="12015" to="12023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">On ğœ”-Regular Sets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="177" />
			<date type="published" when="1979-11">1979. Nov. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16318</idno>
		<title level="m">Learning and Planning in Average-Reward Markov Decision Processes</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Learning and planning in average-reward Markov decision processes</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10653" to="10662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">On Convergence of Average-Reward Off-Policy Control Algorithms in Weakly Communicating MDPs</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OPT 2022: Optimization for Machine Learning</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>NeurIPS 2022 Workshop</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Model-free reinforcement learning in infinite-horizon average-reward Markov decision processes</title>
		<author>
			<persName><forename type="first">Chen-Yu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><forename type="middle">Jafarnia</forename><surname>Jahromi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haipeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiteshi</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10170" to="10180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Joint inference of reward machines and policies for reinforcement learning</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Gavran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yousef</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rupak</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Neider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ufuk</forename><surname>Topcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Automated Planning and Scheduling</title>
				<meeting>the International Conference on Automated Planning and Scheduling</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="590" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Active finite reward automaton inference and reinforcement learning using queries and counterexamples</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ojha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Neider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ufuk</forename><surname>Topcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Cross-Domain Conference for Machine Learning and Knowledge Extraction</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="115" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Predictive runtime monitoring for linear stochastic systems and applications to geofence enforcement for UAVs</title>
		<author>
			<persName><forename type="first">Hansol</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Frew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Sankaranarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Runtime Verification</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="349" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Sim-to-real transfer in deep reinforcement learning for robotics: a survey</title>
		<author>
			<persName><forename type="first">Wenshuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><forename type="middle">PeÃ±a</forename><surname>Queralta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomi</forename><surname>Westerlund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE symposium series on computational intelligence (SSCI)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Complex formalism, such as definitions or proofs, is motivated and explained clearly. [yes] (5) The use of mathematical notation and formalism serves the purpose of enhancing clarity and precision; gratuitous use of mathematical formalism</title>
		<imprint/>
	</monogr>
	<note>i.e., use that does not enhance clarity or precision) is avoided</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
