<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Are machine learning interpretations reliable? A stability study on global interpretations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2025-05-21">21 May 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Luqin</forename><surname>Gan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Rice University</orgName>
								<address>
									<settlement>Houston</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tarek</forename><forename type="middle">M</forename><surname>Zikry</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Departments of Statistics</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Theoretical Neuroscience</orgName>
								<orgName type="institution" key="instit1">Zuckerman Mind Brain Behavior Institute</orgName>
								<orgName type="institution" key="instit2">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Genevera</forename><forename type="middle">I</forename><surname>Allen</surname></persName>
							<email>genevera.allen@columbia.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Departments of Statistics</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Theoretical Neuroscience</orgName>
								<orgName type="institution" key="instit1">Zuckerman Mind Brain Behavior Institute</orgName>
								<orgName type="institution" key="instit2">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Are machine learning interpretations reliable? A stability study on global interpretations</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-05-21">21 May 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">914B81CB43A2589EB666CF46C2F81A6D</idno>
					<idno type="arXiv">arXiv:2505.15728v1[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2025-05-26T20:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As machine learning systems are increasingly used in high-stakes domains, there is a growing emphasis placed on making them interpretable to improve trust in these systems. In response, a range of interpretable machine learning (IML) methods have been developed to generate human-understandable insights into otherwise black box models. With these methods, a fundamental question arises: Are these interpretations reliable? Unlike with prediction accuracy or other evaluation metrics for supervised models, the proximity to the true interpretation is difficult to define. Instead, we ask a closely related question that we argue is a prerequisite for reliability: Are these interpretations stable? We define stability as findings that are consistent or reliable under small random perturbations to the data or algorithms. In this study, we conduct the first systematic, large-scale empirical stability study on popular machine learning global interpretations for both supervised and unsupervised tasks on tabular data. Our findings reveal that popular interpretation methods are frequently unstable, notably less stable than the predictions themselves, and that there is no association between the accuracy of machine learning predictions and the stability of their associated interpretations. Moreover, we show that no single method consistently provides the most stable interpretations across a range of benchmark datasets. Overall, these results suggest that interpretability alone does not warrant trust, and underscores the need for rigorous evaluation of interpretation stability in future work. To support these principles, we have developed and released an open source IML dashboard and Python package to enable researchers to assess the stability and reliability of their own data-driven interpretations and discoveries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years the field of Interpretable Machine Learning (IML) has arisen, a class of ML methods focused "the use of machine learning techniques to generate human understandable insights into data, the learned model, or the model output" <ref type="bibr" target="#b10">[10]</ref>. These methods are employed to generate data-driven discoveries in both unsupervised and supervised settings, with a focus on opening up the black box of machine learning to users and practitioners across domains, particularly critical in domains such as health, security, and even finance, where ML methods have been applied to approve or deny mortgages <ref type="bibr" target="#b127">[126]</ref>. Existing review papers have defined concepts of IML methods, provided systematic taxonomy and principles <ref type="bibr" target="#b144">[143]</ref>, and discussed its importance in various aspects including model validation <ref type="bibr" target="#b41">[40,</ref><ref type="bibr" target="#b88">87,</ref><ref type="bibr" target="#b17">17]</ref>, model debugging <ref type="bibr" target="#b83">[82,</ref><ref type="bibr" target="#b41">40]</ref>, model transparency &amp; accountability <ref type="bibr" target="#b41">[40,</ref><ref type="bibr" target="#b69">68,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b123">122,</ref><ref type="bibr" target="#b29">29]</ref>, trust <ref type="bibr" target="#b58">[57,</ref><ref type="bibr" target="#b99">98,</ref><ref type="bibr" target="#b114">113,</ref><ref type="bibr" target="#b124">123,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b105">104,</ref><ref type="bibr" target="#b99">98]</ref>, ethics <ref type="bibr" target="#b83">[82,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b69">68,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b133">132,</ref><ref type="bibr" target="#b29">29]</ref>, and scientific discoveries <ref type="bibr" target="#b103">[102,</ref><ref type="bibr" target="#b83">82,</ref><ref type="bibr" target="#b41">40,</ref><ref type="bibr" target="#b69">68,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b133">132,</ref><ref type="bibr" target="#b149">148]</ref>. Though these terms have been defined before, we redefine them here for clarity:</p><p>• Reproducibility: Do you observe the same results when the pipeline is rerun on the same data?</p><p>• Replicability: Is your code and pipeline easily usable to run on unseen data?</p><p>• Reliability: Do you observe similar results to the ground truth?</p><p>• Generalizability/Predictability: Can you predict your results or observe similar results on unseen data?</p><p>• Stability: How sensitive are your results to random data or algorithm perturbations?</p><p>Here, we believe a broad prerequisite criterion to trust interpretations is reliability. For example, from the perspective of knowledge discovery, a new finding cannnot be accepted if the discovery is unreliable after many repeated experiments. Therefore, we consider the reliability of the interpretations to be significant in building trustworthy AI systems. As machine learning methods become ever present in a wide range of critical domains, including biomedicine and drug discovery <ref type="bibr" target="#b50">[49]</ref>, physics <ref type="bibr" target="#b27">[27]</ref>, national security <ref type="bibr" target="#b55">[54]</ref>, finance <ref type="bibr" target="#b37">[36]</ref>, climate science <ref type="bibr" target="#b85">[84]</ref>, transportation <ref type="bibr" target="#b145">[144]</ref>, and more <ref type="bibr" target="#b129">[128,</ref><ref type="bibr" target="#b6">6]</ref>, significant attention has been drawn towards the quality of interpretations generated by ML models. In supervised learning tasks such as regression and classification, interpretations include feature importance ranking for a predictive model, whereas in an unsupervised task such as clustering and dimension reduction, interpretations are in the groupings generated, either in the high-dimensional space or a low-dimensional embedding. These interpretations are widely used across domains, either as standalone findings such as ranking features important in aircraft engine longevity <ref type="bibr" target="#b11">[11]</ref>, or as part of step in a downstream analysis, such as identifying groups of drug-resistant cancer cells <ref type="bibr" target="#b150">[149]</ref>.</p><p>Though vast bodies of work have been devoted to ML methods, and recently, IML methods, the reliability of interpretations themselves have not been deeply explored. As compared to traditional statistical tasks such as prediction, in which we can easily measure its accuracy with the true response, the quality of interpretations from a machine learning model are more difficult to evaluate, as ground truth interpretations are rarely observed, making measures of reliability challenging. In lieu of being able to directly compare to ground truth interpretations, many have turned to studying stability as a practical proxy; if an interpretation changes dramatically under small perturbations to the data or model, it is unlikely to be reliable. Since the inception of the idea to study feature selection in regularized models <ref type="bibr" target="#b80">[79]</ref>, further work has arisen for feature interactions <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b70">69]</ref>, graphical models <ref type="bibr" target="#b72">[71,</ref><ref type="bibr" target="#b87">86]</ref>, and dimension reduction <ref type="bibr" target="#b140">[139,</ref><ref type="bibr" target="#b26">26]</ref>. In clustering, it is commonly referred to as consensus clustering <ref type="bibr" target="#b86">[85]</ref>, where it has gained popularity for identifying optimal hyperparameters in unsupervised class discovery <ref type="bibr" target="#b138">[137,</ref><ref type="bibr" target="#b139">138,</ref><ref type="bibr" target="#b73">72]</ref>. Though widely advocated for validating reproducible discovery <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b144">143]</ref>, stability analyses can quickly become computationally burdensome, and the choice of perturbation (noise addition, different train/test splits, random reinitializations) and evaluation metrics are not always immediately clear, and can require specific methodological knowledge <ref type="bibr" target="#b142">[141,</ref><ref type="bibr" target="#b10">10]</ref>.</p><p>Despite the large volume of IML-related research on supervised learning methods <ref type="bibr" target="#b41">[40,</ref><ref type="bibr" target="#b106">105,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b123">122]</ref>, there is no systematic framework to evaluate the reliability of the interpretations on unsupervised scientific discovery. <ref type="bibr" target="#b71">[70]</ref> explored the impact of prediction accuracy on the quality interpretability in terms of accuracy and stability of feature importance ranking. However, this work focuses only on feature importance and intrinsic global IML methods. Many past efforts focus on the reliability of model-specific methods, such as <ref type="bibr" target="#b148">[147]</ref> who investigated the uncertainties in the surrogate interpretability model LIME <ref type="bibr" target="#b99">[98]</ref> due to sampling procedure, and sampling proximity and explained model credibility across different observations. Similarly, <ref type="bibr" target="#b8">[8]</ref> provides a framework to evaluate the adequacy of explanations obtained from saliency map-related methods, which measures the sensitivity of explanations to label permutation or random parameters. As a useful framework for model validation, however, it is limited to saliency maps and provides no insights into the reliability of the explanations. Additional literature explored the sensitivity of explanations by parameter randomization or feature perturbation, primarily in deep learning methods <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b12">12]</ref>. Past taxonomies classified the evaluation of interpretability into application-grounded, human-grounded, and functionally-grounded evaluation <ref type="bibr" target="#b38">[37]</ref>.</p><p>Contribution In this study, we carry out a wide-ranging systematic empirical assessment of the stability of commonly used global interpretable machine learning (IML) methods on tabular data. We study the stability of interpretations via data perturbations including train/test splits and noise addition. Specifically, we will focus on issues of the stability of interpretations across four main tasks: 1) classification, 2), regression, 3) clustering, and 4) dimensionality reduction. To facilitate transparency and reproducibility, we introduce an interactive, open-source software framework that enables practitioners to explore and visualize the reliability of IML outputs across diverse datasets and modeling scenarios. By combining these empirical investigations with user-friendly tooling, we aim to deliver critical insights and practical guidance on how much we can trust machine-learning interpretations in high-stakes domains such as human health and society.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Study Description</head><p>As detailed in Figure <ref type="figure">1</ref>, we seek to create a wide-ranging empirical framework to answer three key questions on the reliability of interpretations in machine learning methods across four tasks 1) classification, 2) regression, 3) clustering, and 4) dimension reduction. Through publicly available benchmarking datasets, we apply commonly used methods in each of these four tasks. We evaluate a diverse set of IML methods spannining multiple paradigms including comparisons between instrinsic and post-hoc approaches, modelspecific and model-agnostic approaches, and global and local methods of interpretability. We refer readers to <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b41">40,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b52">51]</ref> for further clarifications on these categorizations.</p><p>Scope For data types, we focus on tabular data only, hence the literature around IML for image, textual, or sequential data-related methods are not included. We focus on evaluating the reliability of interpretations obtained from three major types of machine learning tasks, including feature importance in supervised learning (regression and classification), clustering, and dimension reduction in unsupervised learning. We evaluate a wide range of popular model-specific and model-agnostic, intrinsic and post-hoc methods. However, we only consider global interpretation methods or local methods which can be aggregated to global representation such as Layer-wise Relevance Propagation (LRP). For the type of explanations, we consider quantiative explanations only, excluding rule-based or visual interpretations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consistency between different methods' interpretations on each split</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compute Consistency &amp;Accuracy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ex: Jaccard similarity between top features selected by Ridge in different splits</head><p>Ex: Jaccard similarity between top features selected by Ridge and XGB in one split across splits</p><p>Figure <ref type="figure">1</ref>: Overview of study design with data splitting. After conducting multiple random data splitting, interpretations, and test predictions are generated using different IML methods on each training set. Then the within-method stability and between-method stability, and average prediction accuracy on the test set are computed.</p><p>We focus on the most popular machine learning and deep learning methods in these areas, and compute metrics to measure the reliability of the interpretations through empirical analysis to a wide range of publicly-available tabular data sets. Since we also aim to test the interpretations derived from generic global methods against each other, we do not apply posthoc methods which are specifically applied to certain machine learning models for convolutional neural network, or methods that provide local feature importance, such as LIME <ref type="bibr" target="#b99">[98]</ref> or anchor <ref type="bibr" target="#b100">[99]</ref>. Specifically, we include general model-specific machine learning models, deep learning-related methods, and model-agnostic methods. Further details on methods and implementation can be found in Appendix A. Through this analysis, we seek to answer three fundamental questions in IML:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Questions</head><p>Q1: If we sample a different training set, are the interpretations similar? Within-method stability reflects how consistently an interpretation method performs when the input data is slightly altered. If small perturbations, such as resampling via different train/test splits, adding noise, or permuting features, lead to substantially different interpretations, the IML method cannot be considered stable or trustworthy for scientific discovery. Interpretations that are highly sensitive to these minor changes, or conditional on a particular train/test split, risk producing conclusions that are not robust or reproducible, and may mislead downstream analyses or decision-making.</p><p>Q2: Do two IML methods generate similar interpretations on the same data? This question seeks to compare between-method stability. On the same data, different practitioners often turn to different methods to answer the same questions in IML. However, if these methods lead to different interpretations, how do we know which interpretation is trustworthy? If these methods yield different interpretations, it raises the question of which interpretation, if any, can be trusted, emphasizing the need for a systematic evaluation of their consistency and validity to ensure reliable conclusions.</p><p>Q3: Does higher accuracy lead to more consistent interpretations? Past work has suggested that predictive accuracy is highly relevant to interpretation stability <ref type="bibr" target="#b89">[88,</ref><ref type="bibr" target="#b71">70]</ref>. To address this, we also explore the relationship between interpretation stability and prediction accuracy in greater depth to determine if accuracy can be used as a determining factor for selecting an IML method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Machine Learning Tasks</head><p>Most of the existing literature on interpretable machine learning techniques is focused on the interpretations of feature importance from supervised learning methods, or on model-specific metrics of reliability. The interpretability, particularly stability, of unsupervised learning methods, remains understudied in the literature <ref type="bibr" target="#b129">[128]</ref>. Therefore, we focus on evaluating the reliability of interpretations obtained from three major types of machine learning tasks, including feature importance in supervised learning, clustering, and dimension reduction in unsupervised learning. We focus on global methods, or when using local methods, we sum up over all points in the training set to obtain global interpretations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Classification &amp; Regression</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Local/Global Model Specific/ Most IML review papers focus on the discussion of techniques offering interpretations of feature importance <ref type="bibr" target="#b41">[40,</ref><ref type="bibr" target="#b88">87,</ref><ref type="bibr" target="#b17">17]</ref>. Such methods aim to provide information on the level of each feature's contribution to predictions by assigning an importance score or ranking to individual features by their level of relevance.</p><p>Feature importance techniques present a global relationship between each feature and the outcome. Intrinsic and model-specific feature importance methods include commonly used statistical methods such as generalized linear model, naive Bayes classifier, SVM, and tree-based gradient boosting and random forest. For example, coefficients in generalized linear models and information gain from tree-based methods <ref type="bibr" target="#b93">[92]</ref> are model-specific interpretations. Secondly, numerous deep learning-specific IML techniques have been proposed to untangle the interpretability challenges of the black box of neural networks. Other lines of work measures features importance using the changes of prediction outcome after intentional feature occlusion <ref type="bibr" target="#b68">[67]</ref>, perturbation <ref type="bibr" target="#b45">[44]</ref> or adversarial change <ref type="bibr" target="#b92">[91,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b88">87]</ref>. For model-agnostic methods, some use counterfactual explanations <ref type="bibr" target="#b128">[127]</ref> of a prediction to evaluate feature importance, including partial dependent plot (global ), accumulated local effects plot (global ), individual conditional expectation plot (local ). Another line of work measures features importance using the changes of prediction outcome after intentional feature occlusion (local ) <ref type="bibr" target="#b68">[67]</ref>, perturbation (global ) <ref type="bibr" target="#b45">[44]</ref>, Shapley value (local ) <ref type="bibr" target="#b75">[74]</ref>, LIME (local ) <ref type="bibr" target="#b99">[98]</ref> or adversarial change <ref type="bibr" target="#b92">[91,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b88">87]</ref>. For this study, we choose only global methods, or local methods that can be aggregated into global metrics, summarized in Table <ref type="table" target="#tab_0">1</ref>. For local methods, we sum the metrics over all points in the training set to obtain global interpretations. Dimension reduction (DR) methods aim to project the original data into lower dimensions that reflect latent trends. In the high dimensional setting, the original data might be redundant with noisy features or highly correlated features <ref type="bibr" target="#b28">[28]</ref>, which would significantly hamper the prediction performance of machine learning methods. Dimension reduction methods overcome the curse of dimensionality by transforming such complex data with a large number of covariates to lower dimensions while preserving the information contained. The main usage of dimension reduction techniques include <ref type="bibr" target="#b0">(1)</ref> to draw inferences on the data structure by visualization <ref type="bibr" target="#b61">[60,</ref><ref type="bibr" target="#b107">106]</ref>, which implies relative distances among the observations in the reduced space, (2) to be applied as pre-processing step for further downstream analysis clustering on the reduced embedding <ref type="bibr" target="#b28">[28]</ref>. The interpretation task of DR methods is to (1) retain the neighbor structure of the original data in lower dimension space; (2) recover correct cluster labels by applying clustering on reduced dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Dimensionality Reduction</head><p>Our choices of dimension reduction methods and their categorizations can be seen in Table <ref type="table" target="#tab_1">2</ref>. We select the most widely used linear global technique: principal component analysis (PCA), and include random projections as a baseline comparison. For non-linear manifold learning methods, we select methods that preserve global properties such as metric/non-metric multidimensional scaling (MDS) <ref type="bibr" target="#b34">[34]</ref>and Isomap <ref type="bibr" target="#b16">[16]</ref>, and methods that preserve local properties including T-distributed Stochastic Neighbor Embedding (tSNE) <ref type="bibr" target="#b126">[125]</ref>, Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP) <ref type="bibr" target="#b78">[77]</ref>, spectral embedding with different affinity <ref type="bibr" target="#b111">[110]</ref>. In addition, we apply a deep auto-encoder framework <ref type="bibr" target="#b64">[63]</ref> with a three-layered encoder and a three-layered decoder, and the low-dimensional code produced by the encoder is used as the reduced dimensions.  Clustering methods identify partitions among the data, subsets of observations sharing some key characteristics. Clustering models are intrinsic and model-specific, which provide the discovery of groups by generating a set of homogeneous subgroups with respect to the hidden data structure. Traditional clustering techniques, such as K-Means and hierarchical clustering, aim partition observations into disjoint groups <ref type="bibr" target="#b56">[55]</ref>, with broad applications in bioinformatics, computational biology, and character recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Clustering</head><p>Recently, neural network-based clustering techniques are increasingly developed <ref type="bibr" target="#b82">[81]</ref>. The deep learning methods are particularly powerful for large data sets, utilizing architectures such as autoencoders or restricted Boltzmann machines <ref type="bibr" target="#b82">[81,</ref><ref type="bibr" target="#b57">56]</ref>. On the other hand, overlapping clusters such as fuzzy clustering can assign observations into multiple groups, which are more helpful in the application to social network detection <ref type="bibr" target="#b101">[100]</ref>. There are well-established theoretic foundations for the stability of K-means clustering <ref type="bibr" target="#b94">[93]</ref>, spectral clustering <ref type="bibr" target="#b131">[130,</ref><ref type="bibr" target="#b125">124]</ref>. However, none have studied the empirical stability of partitions obtained from clustering methods. Table!3 summarizes our clustering method and hyperaparameters of choice by dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Reliability Test and Metrics</head><p>Reliability test The first step of our framework is to design sensitivity tests to obtain interpretations from machine learning models under different circumstances. We propose two types of perturbation techniques to conduct sensitivity tests: random splits and adding random noise. In supervised learning, researchers conduct a train/test split before fitting a predictive model so as to avoid overfitting. The interpretations are reliable if the resulting feature importance scores are consistent with different random train/test splits. We also apply data splitting for unsupervised tasks and fit IML methods to the training sets. We add random noise for unsupervised learning tasks including clustering and dimension reduction methods. The reliability of an IML method can be measured by the consistency after performing multiple data perturbations. Specifically, we first perform random perturbation on the data set of interest, by either conducting train/test splits, or adding random noise. To carry out this empirical study, we construct a custom web app to explore the reliability of ML interpretations, available at https://iml-reliability.herokuapp.com/home.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric Task(s) Equation Citation</head><p>Jaccard Similarity C1, R </p><formula xml:id="formula_0">J(A, B)@k = |A k ∩B k | |A k ∪B k | [96] AO (Adjusted Overlap) C1, R AO@k = 1 k k d=1 |A d ∩B d | d [133] Top K Kendall's Tau C1, R K(A, B) (p) @k = i,j∈P(A k ,B k ) K(p) i,j (A k , B k ) [62] ARI (Adjusted Rand Index) C2, D ARI = ij ( n ij 2 )−[ i ( a i 2 ) j ( a j 2 )]/( n 2 ) 1 2 [ i ( a i 2 )+ j ( a j 2 )]−[ i ( a i 2 ) j ( a j 2 )]/( n 2 ) [95] Fowlkes-Mallows Index C2, D F M = √ P P V • T P R [45] Mutual Information C2, D M I(A, B) = i∈A j∈B p(i, j) log p(i,j) p(i)p(j) [64] V-measure C2, D V = (1+β)•homogeneity•completeness β•completeness+homogeneity [103] NN-Jaccard-AUC D NN-J-AUC(A, B)@k = 1 N N i=1 AUC(J(A i , B i )@k)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stability metrics</head><p>To assess interpretation stability, we implement several robust metrics for each IML task. For feature importance rankings, we measure the stability of top K features by (1) Jaccard similarity <ref type="bibr" target="#b97">[96]</ref>, (2) average overlap (AO) <ref type="bibr" target="#b134">[133]</ref> and (3) Kendall Tau distance <ref type="bibr" target="#b63">[62]</ref>. The stability of clustering labels can be measured by widely used cluster validity indices including adjusted rand index (ARI) <ref type="bibr" target="#b96">[95]</ref>, mutual information (MI) <ref type="bibr" target="#b65">[64]</ref>, V measure <ref type="bibr" target="#b104">[103]</ref> and Fowlkes mallows index <ref type="bibr" target="#b46">[45]</ref>. For dimension reduction interpretations, we measure their : (1) clustering stability performed on the reduced dimension, measured by the same metrics as clustering, and (2) visualization reliability measured by local neighbor stability.</p><p>Here, we propose to examine whether every sample has a consistent set of nearest neighbors from different random perturbations, under the same setting. Specifically, the similarity of two sets of nearest neighbors can be calculated by the Jaccard score <ref type="bibr" target="#b97">[96]</ref>. With the number of nearest neighbors K ranging from 1 to N , where N is the total number of samples, we can draw a receiver operating characteristic curve of Jaccard scores against K, and obtain the area under the curve (AU C) of the Jaccard scores curve. We use the resulting AUC score as a local neighbor stability metric, denoted as NN-Jaccard-AUC score. Table <ref type="table" target="#tab_4">4</ref> shows all stability metrics, and further details are available in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Datasets</head><formula xml:id="formula_1">Data N p # classes Type Task(s)</formula><p>Online News Popularity <ref type="bibr" target="#b44">[43]</ref> 39644 59 -predict # of shares in social networks R BlogFeedback <ref type="bibr" target="#b25">[25]</ref> 52397 280 -predict how many comments the post will receive R Satellite image <ref type="bibr" target="#b102">[101]</ref> 6435 36 --R STAR <ref type="bibr" target="#b5">[5]</ref> 2161 39 -Tennessee Student Teacher Achievement Ratio (STAR) project R Communities and crime <ref type="bibr" target="#b21">[21]</ref> 1993 99 -predict # of violent crimes R Bike <ref type="bibr" target="#b43">[42]</ref> 731 13 -hourly and daily count of rental bikes R CPU <ref type="bibr" target="#b102">[101]</ref> 209 7 --R Wine <ref type="bibr" target="#b33">[33]</ref> 178 13 -red wine quality R Music <ref type="bibr" target="#b102">[101]</ref> 1059 117 -geographical origin of music R Residential <ref type="bibr" target="#b95">[94]</ref> 372 103 -predict house price R Tecator <ref type="bibr" target="#b102">[101]</ref> 240 124 --R Word <ref type="bibr" target="#b21">[21]</ref> 523 526 -word occurrence data to predict the length of a newsgroup record R Riboflavin <ref type="bibr">[</ref>  The study is conducted on 25 popular benchmarking datasets across domains for the three tasks of regression, classification, and dimensionality reduction, characterized in Table <ref type="table" target="#tab_6">5</ref>. In order to assess the stability of interpretations over a wide range of accuracy scopes, we also ensure to have a variety of ranges of prediction/clustering accuracy for each task.</p><p>Specifically, for the classification task, we include three high-dimension data sets (P &gt; N ): Asian Religions <ref type="bibr" target="#b108">[107]</ref>, PANCAN tumor cell <ref type="bibr" target="#b135">[134]</ref>, and DNase sequencing <ref type="bibr" target="#b31">[31]</ref>; six mid-size data sets: Statlog image segment data <ref type="bibr" target="#b21">[21]</ref>, spam base data set <ref type="bibr" target="#b21">[21]</ref>, author word counts data set <ref type="bibr" target="#b21">[21]</ref>, Amphibians species prediction data <ref type="bibr" target="#b54">[53]</ref>, Madelon artificial data from NIPS 2003 Feature Selection Challenge <ref type="bibr" target="#b53">[52]</ref>, and breast cancer TCGA data <ref type="bibr" target="#b2">[3]</ref>; and four data sets with large N (N &gt; 5000): bean image attributes data <ref type="bibr" target="#b21">[21]</ref>, anuran call data <ref type="bibr" target="#b21">[21]</ref>, theorem prediction data <ref type="bibr" target="#b23">[23]</ref> and Digit MNIST data <ref type="bibr" target="#b35">[35]</ref>. In the case of regression, we have two high dimensional riboflavin genomics data set <ref type="bibr" target="#b24">[24]</ref> and word occurrence data to predict the length of a newsgroup record <ref type="bibr" target="#b21">[21]</ref>; eight mid-size data Tennessee Student Teacher Achievement Ratio (STAR) data <ref type="bibr" target="#b5">[5]</ref>, communities and crime data <ref type="bibr" target="#b21">[21]</ref>, Bike rental count data <ref type="bibr" target="#b43">[42]</ref>, CPU data <ref type="bibr" target="#b102">[101]</ref>, red wine quality data <ref type="bibr" target="#b33">[33]</ref>, music original data <ref type="bibr" target="#b102">[101]</ref>, residential house price data <ref type="bibr" target="#b95">[94]</ref> and Tecator data <ref type="bibr" target="#b102">[101]</ref>; and three data sets with large N &gt; 5000: online news popularity <ref type="bibr" target="#b44">[43]</ref>, blog feedback data set <ref type="bibr" target="#b25">[25]</ref> and Satellite image data <ref type="bibr" target="#b102">[101]</ref>. We select classification data sets whose ARIs are all above 0.2, which include data MNIST, Madelon, DNase, Theorem, and Amphibians. Besides, we add six additional publicly available benchmark data sets for the clustering task: Iris species data <ref type="bibr" target="#b21">[21]</ref>, WDBC Breast Cancer Wisconsin <ref type="bibr" target="#b21">[21]</ref>, Tetragonula bee species data <ref type="bibr" target="#b47">[46]</ref>, Chemical composition of ceramic samples data <ref type="bibr" target="#b21">[21]</ref> and AFLP data of Veronica plants <ref type="bibr" target="#b77">[76]</ref>. For the dimension reduction task, we exclude difficult data sets whose clustering accuracy on the first two reduced dimensions is always below the accuracy ARI cutoff 0.2. As we tend to reduce the dimensionality of given data sets, the original data is better to have a larger number of features, for which we set the cutoff at P &gt; 15. Due to computational expense, we also remove data sets that are too large, using a threshold of N &gt; 2000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of the Results</head><p>We address key IML questions Q1-Q3 in two sections, one for supervised tasks (classification and regression, and one for unsupervised tasks (clustering and dimension reduction). For supervised tasks, we consider feature importance rankings as our primary interpretation, whereas for unsupervised tasks we consider clustering results, both in the full dimensional space and reduced dimension embeddings. In each category, we select and apply robust metrics to measure the reliability of the resulting interpretations. For each task, summary figures generated via the dashboard are used to assess each question. For example, in Figure <ref type="figure" target="#fig_1">2</ref>, subplots (A) and (B) we aim to address Q1 by measuring whether interpretations are consistent among repeats, within each method. The heatmap (A) demonstrates the average within-method stability scores of interpretations of an IML method aggregated over 100 repeats, ranging in [0,1], with IML methods on the x-axis and data set on the y-axis. The bump plot (B) further addresses Q1 by ranking IML methods by their stability in each data set. The data sets are ordered by the # observation/# feature ratio, with the left y-axis showing the rank of each method based on stability. We add a column of average stability over all methods at the right, and the methods of the y-axis on the right are ordered by the average stability, from the most consistent to the least consistent.</p><p>The heatmaps in (C) answer Q2 by evaluating whether different methods would result in similar interpretations on the same perturbed data via between-method average stability of interpretations obtained from each pair of IML methods. The heatmaps in panel (D) present each method's task-specific accuracy on the test set of each data, all averaging over 100 repeats. Heatmaps in (E) illustrate the average stability of prediction on the test set.</p><p>To address Q3, we investigate the relationships between interpretation stability and prediction accuracy with scatterplots (F) and (G). Different colors represent different data sets in (F), and different colors represent different IML methods in (G), all averaging over 100 repeats. In each scatter plot, we also visualize the relationships by fitting regression lines, either aggregated over data or IML methods. Note that for the unsupervised tasks of clustering and dimension reduction, we cannot generate panels (E)-(G), due to the nature of the unsupervised tasks. Using these tools, we directly address Q1-Q3 for all supervised and unsupervised tasks. In Appendix C.2, we assess the performance of specific methods for each question and task in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Supervised Learning: Feature Importance (Classification &amp; Regression)</head><p>Q1: Overall, the feature importance measures generated by IML methods are have low reliability with small stability scores, as shown in part (A) in Figures <ref type="figure" target="#fig_2">2 and 3</ref>. For an individual data set, different IML methods also result in different stability levels in feature importance tasks. For example, in the DNase data, interpretations from permutation with RF are consistent with over AO of over 0.9, but the AO scores of other IML methods are all below 0.3. Comparing different tasks, the interpretations in regression are more consistent than those in classification with higher scores in part (A), using the same IML models. In panel (D) of Figures <ref type="figure" target="#fig_2">2, 3</ref>, the accuracy performance is consistent across methods. Specifically on feature importance tasks, by pairing stability and predictability analyses in Figure <ref type="figure" target="#fig_1">2</ref> and Figure <ref type="figure" target="#fig_2">3</ref>, methods have significantly different stability (panel (A)) even though they have similar predictive performance (panel (D)), such as in the PANCAN, Author, Statlog, Bean, and Call data sets in the classification task. Therefore, supervised models with similar accuracy may not have similar interpretation reliability.</p><p>Q2: Feature importance interpretations from different IML methods are not consistent with each other, with small stability scores for the between-method stability heatmaps in panel (C) of Figure <ref type="figure" target="#fig_1">2</ref> and Figure <ref type="figure" target="#fig_2">3</ref>. Tree based methods and linear models have the best between-method results, and permutation methods are more consistent with their base models. Combining with results panels (E), we can infer IML methods generate different feature importance scores on the same data, even with strong prediction strength.</p><p>Q3: To explore whether predictive accuracy can be used as an indicator for interpretation stability, we investigate their relationship by scatterplots in (F) and (G) of Figure <ref type="figure" target="#fig_1">2</ref> and Figure <ref type="figure" target="#fig_2">3</ref>. In panel (F) in both figures, most of the fitted lines are flat, and only two data sets have significant coefficients. Hence on a given data set, the higher predictive accuracy of an IML method does not imply higher interpretation stability. Note that in Riboflavin data in regression, MLP-related methods perform poorly in terms of both prediction accuracy and interpretation stability with almost 0 values, which causes a significant p-value between predictive accuracy and interpretation stability. From the perspective of IML methods, as shown in part (G), for a given method, higher accuracy on one data set does not imply higher interpretation stability. Though there are several methods that have positive coefficients, their coefficients are not significant, with p-values equal to 1 after Bonferroni correction due to the high variance of the results from different data sets. Therefore, none of the IML methods show significant associations between interpretation stability and predictive accuracy.</p><p>The heatmap in (D) shows the average predictive accuracy on test sets of each ML method on each data, and the accuracy scores are similar across the ML methods. None of the methods have high accuracy scores in Madelon, Amphibians, and Theorem, which are more difficult to classify. And all of the methods have over 0.9 accuracies in Author, Call, and Bean data sets. Linear models and tree-based models result in high accuracy score of over 0.9 in most of the data sets, except the decision tree is less accurate in most of the high dimensional data. MLP is also relatively accurate in most of the data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unsupervised Learning: (Dimension Reduction &amp; Clustering)</head><p>Q1:</p><p>Clustering In Figure <ref type="figure" target="#fig_3">4</ref>, we can evaluate the IML tasks on the clustering results. In the bump plots of ranked feature importance in part (B), we can observe that methods can have varying levels of stability in different data sets, regardless of the data sizes or prediction accuracy. Similar to the feature importance results in supervised tasks, we still find that on a single data set, different IML methods result in different stability levels. However, compared to classification and regression results which favored tree-methods and linear models, the overall stability scores in clustering are much higher. In the clustering task, on average, the spectral (RBF) and K-Means++ generate the most consistent clustering over all data sets, and HC (single) generates the least consistent clustering labels in data splitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dimension Reduction</head><p>We evaluate the stability from two perspectives in the dimension reduction tasks: 1) stability on clustering performed in the reduced dimension and 2) the stability of an individual observations nearest neighbors. In Figure <ref type="figure" target="#fig_4">5</ref>(A) and (B), we can see the within-method clustering label stability. The stability is measured by the ARI metric. Plot (A) and (B) in Figure <ref type="figure" target="#fig_7">8</ref> analyze nearest neighbor-based stability with noise addition perturbation using random Gaussian noise with a standard deviation of 0.15, measured by the NN-Jaccard-AUC score, as explained in Section2.3. From both perspectives, we reach similar answers to Q1, that the interpretations are not reliable within-method.</p><p>In Figure <ref type="figure" target="#fig_4">5</ref>(D), we carry out clustering on the reduced dimension embedding as a notion of DR accuracy, with results showing similar inconsistency across methods and datasets. We utilize random projections here as a baseline model, which logiaclly yields the lowest accuracy and stability overall. The local methods t-SNE and UMAP work well for the bulk cell RNA-seq PANCAN data, but have poor performance in the single cell RNA-seq Darmanis data, possibly due to the sparsity of scRNA-seq data. In addition, inaccurate methods/data may have consistent clustering labels. For example, TCGA data has 0 clustering accuracy for all methods, but its labeling stability is moderate. The global methods PCA and spectral clustering can result in consistent clustering labels on average, as shown in plot (B) in Figure <ref type="figure" target="#fig_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B A D C</head><p>The nearest neighbor stability is relatively low for most methods and data sets, as shown in plot (A) Figure <ref type="figure" target="#fig_7">8</ref>. Comparing the within-method stability heatmaps (A) in Figure <ref type="figure" target="#fig_4">5</ref> and Figure <ref type="figure" target="#fig_7">8</ref> and bump plots (B) in Figure <ref type="figure" target="#fig_4">5</ref> and Figure <ref type="figure" target="#fig_7">8</ref>, on the same reduced dimensions, the stability ranks of nearest neighbors are quite different from that of clustering. For example, even though the clustering labels based on DAE are unstable, the nearest neighbors are consistently maintained for larger values, especially for the Statlog and Tetragonula data sets. The bump plot Figure <ref type="figure" target="#fig_7">8(B)</ref> shows that DAE generates the most consistent nearest neighbors, while it is the least consistent in clustering.</p><p>Q2: Between-method stability performance for clustering interpretations is moderate, with most of the ARI values around 0.5. K-Means and K-Means++ logically highly consistent with each other with an ARI of 0.812. All other methods are similarly stable, except HC (single), which has poor performance overall. We further consider the clustering label stability across different methods on the reduced dimension. The overall stability scores across different methods are moderate with most of the ARI around 0.5, as shown in the heatmap (C) in Figure <ref type="figure" target="#fig_4">5</ref>, similar to the clustering results. Clustering labels based on t-SNE are relatively consistent with PCA, Spectral (NN), and UMAP with ARI greater than 0.7, while PCA, Spectral (NN), and UMAP are not very consistent with each other.</p><p>Q3: Note that under the task of clustering and dimension reduction, we treat the clustering labels our interpretation of the underlying structure of the data set. Therefore, Q3 does not apply here as the model accuracy is also calculated by the resulting labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dashboard and Software</head><p>Since our empirical study covers a wide range of IML tasks, methods, benchmark data sets, perturbation techniques, and stability metrics, we are only able to show a small part of the analyses in the manuscript, and refer to Appendix C.2 for further results. Moreover, in order to demonstrate the full results with not only summary figures but also detailed figures, we created a user-friendly and interactive visualization dashboard * based on the Python Dash developed by Plotly <ref type="bibr" target="#b59">[58]</ref>, with all data, methods, and metrics shown in this study. On this dashboard, researchers can also assess the reliability of their own data, IML method, or metric under this same framework. We have further developed an open-source Python package † to conduct the same framework of reliability tests, with functions that allow users to easily upload, evaluate, and compare their own reliability results. Further detailed description of software usage can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Through our extensive empirical study, our findings demonstrate that interpretations generated by commonly used interpretable machine learning (IML) methods are often unstable and sensitive to minor changes in the data. Across multiple tasks and datasets, we observe that small perturbations, such as train/test split variation or additive noise, can lead to substantial shifts in feature rankings, cluster assignments, or low-dimensional embeddings. This suggests that even when prediction accuracy remains unchanged, the interpretations provided by the model may not be trustworthy. Furthermore, different IML methods often produce divergent interpretations of the same data, and the consistency of a given method can vary significantly across datasets. Together, these results underscore that interpretation reliability is not a guaranteed byproduct of good predictive performance and must be assessed independently.</p><p>A central takeaway from our study is that no single IML method consistently yields the most stable or reliable interpretations across all datasets or tasks. The bump plots shown in panels (B) of Figures <ref type="figure" target="#fig_3">2, 3, 4</ref>, and 5 illustrate this clearly: different methods rank highly on different datasets, and their relative stability fluctuates depending on the task and data characteristics. For instance, in clustering tasks, the most stable method varies from dataset to dataset without a clear relationship to factors such as the number of features or the sample size. This aligns with the "no free lunch" principle in machine learning-there is no universally optimal method for interpretation reliability. Instead, method choice should be guided by empirical testing on the specific dataset at hand.</p><p>Although prior literature often advocates for simple, interpretable models like linear regression or shallow trees <ref type="bibr" target="#b71">[70]</ref>, our findings challenge the assumption that such models are inherently more reliable in their interpretations. While these methods may be easier to understand, they do not always produce the most stable feature rankings or clustering outputs. In several instances, more complex or model-agnostic methods, such as permutation-based importance with random forests, yielded more consistent interpretation scores than their simpler counterparts. This introduces an important trade-off: simpler models may be more transparent, but that transparency does not always translate into stability. Critically, we also find that predictive accuracy is not a reliable proxy for interpretation stability. A highly accurate model may still yield volatile or misleading explanations. These results suggest that without explicit testing for stability, it is difficult to justify trusting the outputs of IML methods in sensitive decision-making settings. In practice, we recommend researchers to weigh the computational cost, interpretability, and stability of the methods they use.</p><p>While our study spans a diverse range of datasets and methods, it is limited to tabular data. Future work should extend the reliability framework to other data modalities such as images, text, and time series, where interpretation methods tend to differ and may be differently affected by data or model perturbations. Besides numerical interpretations, numerous formats of explanations can be considered in future studies, including rules, textual, or visual interpretations. Additionally, future work may be devoted specifically to inference methods to statistically quantify uncertainty in interpretation metrics. Recent advances in selective inference, post-selection inference, and model-agnostic feature attribution provide rigorous tools for estimating the variability and significance of interpretations <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b67">66,</ref><ref type="bibr" target="#b121">120,</ref><ref type="bibr" target="#b81">80,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b136">135]</ref>. These methods can complement empirical stability analyses by offering confidence intervals or p-values for interpretations, enabling users to distinguish signal from noise in a statistically principled way.</p><p>Given our findings, we recommend for a new standard of practice in IML research and application: interpretation reliability should be validated and reported alongside predictive performance. Whenever possible, researchers and practitioners should evaluate their interpretations under stability measures defined here -perturbations such as resampling, noise addition, and random re-initializations, to assess robustness. This practice is especially important in contexts involving knowledge discovery from new datasets, development of novel IML algorithms, or proposals of new metrics for interpretability. Without such assessments, conclusions drawn from model interpretations may be overstated or misleading, leading to negative downstream impacts. Our dashboard and available Python package are valuable open-source, easy-to-use tools towards these aims. Ultimately, the reliability of interpretations affects not just academic analyses but also real-world decisions in healthcare, finance, and public policy. By advancing methods that ensure greater reliability in interpretation, we not only strengthen the foundation for scientific discovery, but also build the underlyign societal trust necessary for the responsible deployment of machine learning systems in these critical domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Study Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Data Randomization</head><p>We define an interpretation to be reliable if the same or similar interpretations can be derived from new data of the same distribution. Therefore, the reliability of a machine learning model can be measured by the stability of its derived interpretations, via sensitivity tests <ref type="bibr" target="#b103">[102]</ref>, or randomly permuting data. In deep learning these permutations are exemplified through methods such as occlusion sensitivity tests <ref type="bibr" target="#b146">[145]</ref> to analyze important parts of images by systematically occluding different portions of images, and saliency maps <ref type="bibr" target="#b8">[8]</ref> for random parameters. Interpretations for surrogate models such as LIME <ref type="bibr" target="#b148">[147]</ref> investigate the uncertainties by its variance in explaining a single point and under different parameter choices, but such tests are highly model specific. For unsupervised learning methods, <ref type="bibr" target="#b137">[136]</ref> apply noise addition to measure the robustness of clustering results, and other literature such as <ref type="bibr" target="#b152">[151]</ref> study the stability of their proposed clustering algorithm by randomly changing the initial data labels, or through bootstrapping for embeddings <ref type="bibr" target="#b91">[90]</ref> or feature importance scores <ref type="bibr" target="#b71">[70]</ref>.</p><p>A reliable machine learning model should not be overly sensitive to small changes in the data or parameters of the model. To this end, the first step of our framework is to design sensitivity tests to obtain interpretations from machine learning models under different perturbations. We propose two types of perturbation techniques to conduct sensitivity tests: 1) random splits and 2) noise addition. In supervised learning, researchers conduct a train/test split before fitting a predictive model so as to avoid overfitting. The interpretations are reliable if the resulting feature importance scores can remain unchanged and consistent across both 1) and 2). Therefore, in supervised models, we measure the stability of the top K important features (K ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">30]</ref>) obtained from 100 repeats of 70%/30% train/test splits. For unsupervised learning tasks including clustering and dimension reduction, we also implement sensitivity tests by subsampling 70% of the data sets. Secondly, we add random noise during the unsupervised learning tasks of clustering and dimension reduction. The stability of interpretations and their resistance to additional noise can be obtained by adding random noise to the data of interest, and measuring the changes of results. With increasing levels of noise added, we are able to illustrate how the stability would change with more difficult data. Noise generated by either Normal (N (0, σ 2 ))or Laplace (Laplace(0, σ 2 )) distribution with mean zero is added to the original data set. The level of noise is controlled by variance σ 2 ∈ [0, 5], with higher variance indicating more noisy and difficult data. For dimension reduction methods, we additionally investigate the stability against the number of reduced dimensions(rank = 2, 5, 10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Stability Metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Interpretation Stability Metrics</head><p>We include three categories of interpretability tasks: 1) feature importance/ranking derived from supervised learning, 2) clustering result, and 3) interpretations, namely clustering results, in reduced dimensions. Below we discuss metrics used to measure reliability of these interpretations in more detail.</p><p>Feature importance ranking Kendall's Tau distance <ref type="bibr" target="#b63">[62]</ref>, which measures the total number of pairwise inversion, and Spearman footrule distance <ref type="bibr" target="#b132">[131]</ref>, which is the L1 distance between ranks, are the two most popular metrics to compute rank similarities in the area of information retrieval. However, these metrics are indifferent to the top ranks and bottom ranks, while researchers are more interested in the stability of top rankings. <ref type="bibr" target="#b112">[111]</ref> propose weighted Kendall's Tau statistics that can impose higher weights on items with top rankings. <ref type="bibr" target="#b66">[65]</ref> propose weighted generalized versions of the two metrics by considering element weights, elements similarity, and position weights. <ref type="bibr" target="#b143">[142]</ref> propose a new rank coefficient: AP correlation, which aims to put more weight on errors in top ranks. In terms of measurement of ranking on feature importance, <ref type="bibr" target="#b71">[70]</ref> explore the impact of prediction accuracy on the quality interpretability in terms of accuracy and stability of feature importance ranking. <ref type="bibr" target="#b71">[70]</ref> construct a set of features rankings through data bootstrapping utilize a weighted Kendall's Tau distance as the stability metric for feature ranking interpretation, by adding a 1 k penalty on bubble swap involving the k th rank. A smaller average pairwise distance indicates higher stability. <ref type="bibr" target="#b71">[70]</ref> also propose to measure the interpretation accuracy by bounding the probability of true ranking that a given ranking is the ground truth by the probability that the ranking is equal to the mode.</p><p>As researchers are generally more interested in the most important features, we focus on top-K rank stability in the case of feature importance. One commonplace metric is the Jaccard similarity <ref type="bibr" target="#b97">[96]</ref> of top-K features. With ranks A and B, the Jaccard similarity is given by</p><formula xml:id="formula_2">J(A, B)@k = |A k ∩ B k | |A k ∪ B k | . (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>where A k and B k contain only the top k features.</p><p>In order to measure the top-k stability, <ref type="bibr" target="#b42">[41]</ref> provide K (p) , the Kendall distance with penalty parameter p, which is determined by the co-occurrence of items within the top K ranks.</p><formula xml:id="formula_4">K(A, B) (p) @k = i,j∈P(A k ,B k ) K(p) i,j (A k , B k ).<label>(2)</label></formula><p>where P(A, B) is the set of all unordered pairs of elements in A k ∪B k and K(p) i,j (A k , B k ) varies depending on whether i and j's co-occurrence in A k and B k :</p><p>Furthermore, <ref type="bibr" target="#b42">[41]</ref> propose unbounded set-intersection overlap, which is also called average overlap (AO) <ref type="bibr" target="#b134">[133]</ref>. Specifically, AO is calculated as the average agreement of A and B of each depth and is given by</p><formula xml:id="formula_5">AO@k = 1 k k d=1 |A d ∩ B d | d<label>(3)</label></formula><p>In this study, we implement these three widely used top K rank metrics including Jaccard similarity, top K Kendall's Tau, and AO to measure the top K feature rankings. The Jaccard similarity and AO range in [0, 1], and the top K Kendall's Tau range in [−1, 1], with a higher value indicating higher stability. The difference is that the Jaccard similarity and top K Kendall's Tau consider whether two sets contain the same elements, while the AO can further measure the stability of ranking.</p><p>Clustering The stability of a clustering method can be measured by the average pairwise similarity of clustering results under the same setting in the sensitivity test. To quantify the similarity between two clustering results: A and B, a widely used metrics Adjusted Rand Index (ARI) <ref type="bibr" target="#b96">[95]</ref> represents the frequency of occurrence of agreements over the total pairs, with correction for chance, which is given by</p><formula xml:id="formula_6">ARI = ij n ij 2 − [ i a i 2 j a j 2 ]/ n 2 1 2 [ i a i 2 + j a j 2 ] − [ i a i 2 j a j 2 ]/ n 2<label>(4)</label></formula><p>where n ij denotes the number of observations in common between cluster i in A and cluster j in B, and</p><formula xml:id="formula_7">a i = j∈B n ij , b i = i∈A n ij .</formula><p>We also utilize Fowlkes mallows index <ref type="bibr" target="#b46">[45]</ref>, which is the geometric mean between the precision and recall:</p><formula xml:id="formula_8">F M = √ P P V • T P R<label>(5)</label></formula><p>where P P V is the positive predictive rate and T P R is the true positive rate. Another common clustering similarity metric is mutual information (MI) <ref type="bibr" target="#b65">[64]</ref>:</p><formula xml:id="formula_9">M I(A, B) = i∈A j∈B p(i, j) log p(i, j) p(i)p(j)<label>(6)</label></formula><p>where i and j represent clusters in A and B, respectively. The V measure <ref type="bibr" target="#b104">[103]</ref> calculates the harmonic mean between homogeneity and completeness, given by</p><formula xml:id="formula_10">V = (1 + β) * homogeneity * completeness βcompleteness + homogeneity<label>(7)</label></formula><p>In this study, we use default β = 1. Note that the V measure is equivalent to normalized mutual information.</p><p>All of the metrics are ranged in [0, 1], with a higher value indicating higher stability. As evaluated in <ref type="bibr" target="#b79">[78]</ref>, the adjusted rand index(ARI) and Fowlkes mallows index measure clustering similarity based on counting pairs of points in which two clustering results agree, while V measure and mutual information measure the variation of information. We implement these four metrics to have create a multifaceted perspective on stability. We further apply the average pairwise stability to measure the stability of the clustering techniques and obtain clustering accuracy by applying the same metrics to the predicted clustering labels and true labels.</p><p>We perform sensitivity tests on clustering methods using noise addition and data splitting with 70% of the data. However, in the case of data splitting, since each subset contains different samples, we only compare the samples belonging to the intersection of every pair of splits and utilize the same clustering similarity metrics as noise addition. For example, if split 1 contains samples [a, b, c, d, e], and split 2 contains [a, c, d, e, f ], we measure the similarity of clustering results of only samples [a, c, d, e] using ARI, Fowlkes mallows, V measure and mutual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dimension reduction</head><p>We measure the reliability of dimension reduction from two aspects: 1) visualization reliability measured by local neighbor stability and 2) clustering stability performed on the reduced dimension. Therefore, we develop a metric, denoted as NN-Jaccard-AUC score, to measure the local neighbor stability in the reduced dimensions, for each sample, we can examine its nearest neighbors and check if this sample has a consistent set of nearest neighbors within different random replicates, under the same setting. Specifically, the similarity of two sets of nearest neighbors can be calculated by the Jaccard score <ref type="bibr" target="#b97">[96]</ref>. The number of nearest neighbors to measure can be ranged from 1 to N , where N is the total number of samples. Specifically, for K = N , the stability measured by the Jaccard score is always 1 as the N-nearest neighbor contains the whole set of observations. We obtain the overall Jaccard scores by averaging the score for every sample. We then use the curve of Jaccard scores computed from K = 1 to K = N against K as a receiver operating characteristic curve, and we can further obtain the area under the curve (AU C) of the Jaccard scores curve. Hence, the AUC score of the Jaccard curve provides a quantitative stability metric on local similarity. A higher value of AU C indicates higher similarities between two reduced dimensions, where AU C = 1 indicates that the dimension reduction method is perfectly consistent in terms of local similarity under any K range. Empirically, we construct the average Jaccard score with 50 values of K, ranging from 1 to N , and we average the Jaccard score of 500 randomly sampled samples under each K to save computational cost when the number of samples is too large (&gt; 500). The ROC curve of the jaccard score for dimension reduction is shown in an example dataset PANCAN in Figure <ref type="figure" target="#fig_5">6</ref>. One may ask why not use the AO or Kendall's Tau, which are used as metrics in the feature ranking stability. However, as we increase the number of neighbors K, the values would be dominated by the top neighbors so that the results would be less informative with large K. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Prediction Stability in Supervised Models</head><p>If the training models are similar under different training sets, would they also generate similar interpretations? This question is specific to supervised learning methods, where we wish to explore the relationship between the stability of predicted estimates and the stability of feature importance interpretations. Similar to interpretation stability, we measure the prediction stability within each method as well as across different methods.</p><p>Within-Method Prediction Stability Specifically, for each split, we build a prediction model using the training set, record the feature importance scores, and make predictions on the test set. Then we measure the prediction stability of each sample by leveraging the dissimilarity of its predicted values. However, since each train/test split contains different samples in the test sets, we are not able to directly compare test predictions from two splits. Instead, we evaluate the stability for each sample over 100 repeats of data splitting. In the classification task, the dissimilarity of prediction on a specific sample is evaluated by the entropy of its predicted classification groups, where the entropy of sample i is defined as</p><formula xml:id="formula_11">entropy i = − K k=1 p k log(p k )</formula><p>where K is the number of classes and p k is the proportion of class k. In the regression task, the standard deviation of predicted values is used as the stability metric. In both scenarios, a smaller value indicates higher purity of the predicted responses, and we construct the stability scores by exponential negative dissimilarity values.</p><p>Between-method prediction stability On each train/test split, the similarity between test predictions generated from two IML methods can be evaluated as they are based on the same set of samples. In classification, we evaluate pairwise prediction stability by the proportion of the same predicted labels. And in regression, we compute the MSE between two test predictions and obtain stability scores by 1 minus the average of the min-max normalized MSE over all repeats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Interpretable Machine Learning Methods</head><p>We focus on the most popular machine learning and deep learning methods in these areas, developed reliable metrics to measure the reliability of the interpretability, and then conduct an empirical analysis to a wide range of real tabular data sets. Since we also aim to test the interpretations derived from generic global methods against each other, we do not apply posthoc methods which are specifically applied to certain machine learning models for convolutional neural network, or methods that provide local feature importance, such as LIME <ref type="bibr" target="#b99">[98]</ref> or anchor <ref type="bibr" target="#b100">[99]</ref>. Specifically, we include general model-specific machine learning models, deep learning related methods, and model-agnostic methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Feature importance</head><p>We apply the most popular general supervised models including linear models LASSO/Logistic LASSO <ref type="bibr" target="#b120">[119]</ref>, Ridge/Logistic Ridge <ref type="bibr" target="#b122">[121]</ref>, tree-based models random forest <ref type="bibr" target="#b22">[22]</ref>, XGradient Boosting (XGB) <ref type="bibr" target="#b30">[30]</ref>, and linear SVM <ref type="bibr" target="#b32">[32]</ref> in the case of classification. The optimal hyper-parameters are chosen by crossvalidation. In addition, even though neural networks are black box models with no inherent interpretation abilities, there are plenty of post-hoc methods to generate feature importance by gradient-based attribution <ref type="bibr" target="#b110">[109,</ref><ref type="bibr" target="#b113">112,</ref><ref type="bibr" target="#b115">114,</ref><ref type="bibr" target="#b146">145,</ref><ref type="bibr" target="#b119">118,</ref><ref type="bibr" target="#b62">61,</ref><ref type="bibr" target="#b110">109]</ref>, layer-wise relevance propagation <ref type="bibr" target="#b84">[83,</ref><ref type="bibr" target="#b60">59,</ref><ref type="bibr" target="#b51">50]</ref>, proxy model <ref type="bibr" target="#b151">[150]</ref>, or knockoffs <ref type="bibr" target="#b74">[73]</ref>. Therefore, we measure the stability of interpretations from neural network-based methods by fitting a multi-layer perception (MLP) model with two hidden layers, and internal ReLu activations with a softmax activation in the final layer. We extract feature importance scores from the MLP model by implementing a number of these post-hoc methods. We note that that we stopped any method that runs over 12 hours to finish a single repeat, such as permutation and Shapley value methods in high-dimensional data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear models</head><p>• LASSO &amp; Ridge Linear models are favored in applications with their straightforward interpretations.</p><p>We evaluate the feature importance rankings by the magnitude of the regression coefficients, which reflects the change of response with respect to the unit change of the feature. The formula for LASSO regression is defined as</p><formula xml:id="formula_12">Y = Xβ + λ|β|,</formula><p>and the formula for ridge regression is defined as</p><formula xml:id="formula_13">Y = Xβ + λ|β| 2 2 ,</formula><p>where λ is the level of penalty. In our implementation, we select a penalty via 5-fold cross-validation.</p><p>• Linear SVM Support vector machine make predictions by creating a hyperplane that can maximize the distance between support vectors. Similar to least square models, we use the magnitude of the coefficients for feature importance ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details:</head><p>• LASSO: sklearn.linear_model.LassoCV function with L1 penalty is used to build the logistic lasso model. The penalty parameter is selected by cross-validation with 5 folds, saga solver, max number of iteration =100, and all other parameters are set as default.</p><p>• Ridge: sklearn.linear_model.RidgeCV function with L2 penalty is used to build the logistic lasso model. The penalty parameter is selected by cross-validation with 5 folds, saga solver, and max number of iterations =100, and all other parameters are set as default.</p><p>• SVM (Regression): sklearn.svm.LinearSVR with L2 penalty is used used to build the SVM model. All parameters are set as default.</p><p>• Logistic LASSO: sklearn.linear_model.LogisticRegressionCV function with L1 penalty is used to build the logistic lasso model. The penalty parameter is selected by cross-validation with 5 folds, saga solver, max number of iteration =100, and all other parameters are set as default.</p><p>• Logistic Ridge: sklearn.linear_model.LogisticRegressionCV function with L2 penalty is used to build the logistic lasso model. The penalty parameter is selected by cross-validation with 5 folds, saga solver, and max number of iterations =100, and all other parameters are set as default.</p><p>• SVM (Classification): sklearn.svm.LinearSVC with L2 penalty is used used to build the SVM model. All parameters are set as default.</p><p>Tree-based methods Tree-based methods obtain feature importance by the accumulated impurity decrease within each tree. The feature importance of feature j can be calculated as</p><formula xml:id="formula_14">I j = 1 B B b=1 s∈S b I(v(s) = j)(L s−1 − Ls)</formula><p>where B is the number of decision trees, S b includes all nodes in tree b, v(s) denotes the feature used to split node s, and L s denotes loss after splitting node s. Features with higher importance scores are able to generate more homogeneous sub-nodes. We implemented widely used tree-based models, including decision trees, random forests, and eXtreme Gradient Boosting (XGB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details:</head><p>• Decision tree (Classification): sklearn.tree.DecisionTreeClassifier function is used to build the decision tree model. All parameters are set as default.</p><p>• Random forest (Classification): sklearn.ensemble.RandomForestClassifier function is used to build the random forest model. All parameters are set as default.</p><p>• XGB (Classification): xgboost.XGBClassifier is used used to build the XGB model. The "objective" paramter is set to be "multi:softmax" if the number of classes is greater than 2. And all other parameters being defaulted.</p><p>• Decision tree (Regression): sklearn.tree.DecisionTreeRegressor function is used to build the decision tree model. All parameters are set as default.</p><p>• Random forest (Regression): sklearn.ensemble.RandomForestRegressor function is used to build the random forest model. All parameters are set as default.</p><p>• XGB (Regression): xgboost.XGBRegressor is used to build the XGB model. All parameters being default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-hoc deep learning methods.</head><p>Despite the lack of interpretability in the deep neural networks models, post-hoc methods have been proposed to measure feature importance by assigning attribution value to the input features. Gradient-based methods compute attributions of features in a forward and backward path of the network <ref type="bibr" target="#b8">[8]</ref>, by applying chain rule of gradients. Popular methods include integrated gradients <ref type="bibr" target="#b119">[118]</ref>, guided backpropagation <ref type="bibr" target="#b116">[115]</ref>, saliency map <ref type="bibr" target="#b115">[114]</ref>, with further work establishing DeepLIFT and Episilon-LRP as modified gradient backpropagation methods <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b113">112,</ref><ref type="bibr" target="#b14">14]</ref>. Another set of methods is perturbation-based methods, which measure the marginal effect of the input features on the output neurons by replacing features with zero baselines <ref type="bibr" target="#b146">[145]</ref>.</p><p>• Saliency map Saliency map <ref type="bibr" target="#b115">[114]</ref> constructs feature importance by partial gradients of input feature on the output target.</p><p>• Guided backpropagation Guided backpropagation <ref type="bibr" target="#b116">[115]</ref> conduct backpropagation through ReLu units, by setting negative gradients to zeros.</p><p>• Integrated Gradients Integrated gradients <ref type="bibr" target="#b119">[118]</ref> computes the average gradients by summing over various inputs from zero baselines to actual values sample.</p><p>• Epsilon-LRP Epsilon-LRP <ref type="bibr" target="#b14">[14]</ref> computes a layer's relevance to the target neuron, and recursively redistribution a layer's relevance until the input layer.</p><p>• deepLIFT Similar to Epsilon-LRP, DeepLIFT <ref type="bibr" target="#b113">[112]</ref> assigns input features' attributions by their relative effect on the output compared to some reference input.</p><p>• Occlusion <ref type="bibr" target="#b146">[145]</ref> measures the marginal effect of features on the target output neurons by replacing input array with zeros on a rolling base.</p><p>Implementation:</p><p>• Regression: We build a multi-layer perception model with 2 hidden layers, internal ReLu activations, mean squared error loss, and adam optimizer. The MLP model is fitted with 10 epochs and a batch size of 50.</p><p>-deepLIFT: the DeepLift package <ref type="bibr" target="#b0">[1]</ref> with nonlinear_mxts_mode=NonlinearMxtsMode.RevealCancel (DeepLIFT-RevealCancel at all layers) to construct feature importance scores. We set find_scores_layer_idx=0, target_layer_idx=-1. The test data set is passed to the scoring function with batch_size=100 and task_idx = 0.</p><p>-Guided backpropagation masked: the DeepLift package with nonlinear_mxts_mode=NonlinearMxtsMode.GuidedBackprop to construct feature importance scores, and we mask out positions that are zero. We set find_scores_layer_idx=0, target_layer_idx=-1. The test data set is passed to the scoring function with batch_size=100 and task_idx = 0.</p><p>-Epsilon-LRP: the DeepExplain package <ref type="bibr" target="#b1">[2]</ref> with "elrp" as method_name is used to construct Epsilon-LRP results. We set the first layer of MLP as the input layer and the last layer as the output.</p><p>-Integrated Gradients: the DeepExplain package with "intgrad" as method_name is used to construct integrated gradient results. We set the first layer of MLP as the input layer and the last layer as the output.</p><p>-Saliency map: the DeepExplain package with "saliency" as method_name is used to construct saliency map results. We set the first layer of MLP as the input layer and the last layer as the output.</p><p>-Occlusion: the DeepExplain package with "occlusion" as method_name is used to construct occlusion feature importance. We set the first layer of MLP as the input layer and the last layer as the output.</p><p>• Classification: We first build a multi-layer perception model with 2 hidden layers, and internal ReLu activations with a softmax activation in the final layer, categorical cross-entropy loss, adam optimizer, and accuracy as the metric. The MLP model is fitted with 2 epochs and a batch size of 10. We use the pre-softmax layer to construct feature importance measures.</p><p>-deepLIFT: the DeepLift package with nonlinear_mxts_mode=NonlinearMxtsMode.RevealCancel (DeepLIFT-RevealCancel at all layers) to construct feature importance scores. We set find_scores_layer_idx=0, target_layer_idx=-2. The test data set is passed to the scoring function with batch_size=100 and task_idx = 0.</p><p>-Guided backpropagation masked: the DeepLift package with nonlinear_mxts_mode=NonlinearMxtsMode.GuidedBackprop to construct feature importance scores, and we mask out positions that are zero. We set find_scores_layer_idx=0, target_layer_idx=-2. The test data set is passed to the scoring function with batch_size=100 and task_idx = 0.</p><p>-Epsilon-LRP: the DeepExplain package with "elrp" as method_name is used to construct Epsilon-LRP results. We set first layer of MLP as the input layer and the pre-softmax layer as the output. For data with the number of classes greater than 2, we use the average scores across different classes as the global feature importance score.</p><p>-Integrated Gradients: the DeepExplain package with "intgrad" as method_name is used to construct integrated gradient results. We set the first layer of MLP as the input layer and the pre-softmax layer as the output. For data with the number of classes greater than 2, we use the average scores across different classes as the global feature importance score.</p><p>-Saliency map: the DeepExplain package with "saliency" as method_name is used to construct saliency map results. We set the first layer of MLP as the input layer and the pre-softmax layer as the output. For data with the number of classes greater than 2, we use the average scores across different classes as the global feature importance score.</p><p>-Occlusion: the DeepExplain package with "occlusion" as method_name is used to construct occlusion feature importance.We set the first layer of MLP as the input layer and the pre-softmax layer as the output. For data with the number of classes greater than 2, we use the average scores across different classes as the global feature importance score.</p><p>Model agnostic methods (applied to Logistic ridge, random forest, XGB and MLP) Model agnostic methods are post-hoc interpretable methods that can be applied to a wide range of prediction models. For the local model agnostic methods, we approximate the global feature importance by averaging the local feature importance score over all instances.</p><p>• Permutation feature importance Permutation feature importance <ref type="bibr" target="#b117">[116,</ref><ref type="bibr" target="#b118">117]</ref> is obtained by comparing the prediction errors after randomly permuting the feature of interest. It provides global insights into the effect of a feature on the prediction results.</p><p>• SHAP Shapley values <ref type="bibr" target="#b75">[74]</ref> provides local feature importance by computing the average marginal effect of a feature by considering all possible coalitions. The computation cost increases exponentially with the number of features. SHAP (Shapley Additive Explanations) provides additive explanations of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementations:</head><p>• Permutation -Permutation Importance (Ridge/Logistic Ridge): the sklearn.inspection.permutation_importance function is used to calculate permutation feature importance for the constructed ridge/logistic ridge model. The number of repeats is 10 and all other parameters are set as default.</p><p>-Permutation Importance (RF): the sklearn.inspection.permutation_importance function is used to calculate permutation feature importance for the constructed random forest model. The number of repeats is 10 and all other parameters are set as default.</p><p>-Permutation Importance (XGB): the sklearn.inspection.permutation_importance function is used to calculate permutation feature importance for the constructed XGB model. The number of repeats is 10 and all other parameters are set as default.</p><p>-Permutation Importance (MLP): the eli5.sklearn.PermutationImportance function is used to calculate permutation feature importance for the MLP model. And all parameters are set as default.</p><p>• Shapley value <ref type="bibr" target="#b75">[74]</ref> -Shapley value (Ridge/Logistic Ridge): the shap.PermutationExplainer function is used to calculate shapley value for the constructed ridge/logistic ridge model. All parameters are set as default.</p><p>-Shapley value (RF): the shap.TreeExplainer function is used to calculate Shapley value for the constructed random forest model. All parameters are set as default.</p><p>-Shapley value (XGB): the shap.TreeExplainer function is used to calculate Shapley value for the constructed XGB model. All parameters are set as default.</p><p>-Shapley value (MLP): the DeepExplain package with "shapley_sampling" as method_name is used to construct Shapley value for the MLP model. In classification, we set the first layer of MLP as the input layer and the pre-softmax layer as the output. For data with the number of classes greater than 2, we use the average scores across different classes as the global feature importance score. In regression, we set the first layer of MLP as the input layer and the last layer as the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Clustering</head><p>To measure the stability of clustering interpretations, we apply the existing popular clustering techniques including K-Means based methods <ref type="bibr" target="#b76">[75,</ref><ref type="bibr" target="#b109">108,</ref><ref type="bibr" target="#b13">13]</ref>, hierarchical clustering with different linkage and euclidean distance <ref type="bibr" target="#b90">[89]</ref>, spectral clustering with different distance affinity <ref type="bibr" target="#b111">[110]</ref>, Gaussian mixture model <ref type="bibr" target="#b20">[20]</ref> and BIRCH <ref type="bibr" target="#b147">[146]</ref>, which is more efficient with large data sets. For all methods, the number of clusters is set to be the oracle number.</p><p>• Hierarchical clustering Hierarchical clustering seeks to build a nested dendrogram of the objects. Hierarchical clustering is quite flexible such that objects can be merged with different distance function and linkages, based on the nature of data set. Agglomerative Hierarchical Clustering <ref type="bibr" target="#b90">[89]</ref> constructs the dendrogram using a bottom-up algorithm, where each object forms its own cluster, and similar clusters are merged together. Each variation of hierarchical clustering uses different linkages:</p><p>-Average linkage: The average linkage tend to merge two clusters with the minimum average distance between all observations in the two clusters.</p><p>-Ward.D linkage: The Ward.D linkage tend to merge two clusters with the minimum sum of squared differences between observations in the two clusters.</p><p>-Complete linkage: The Complete linkage tend to merge two clusters with the minimum distance between the farthest observations in the two clusters.</p><p>-Single linkage: The Single linkage tend to merge two clusters with the minimum distance between the closest observations in the two clusters.</p><p>For each data set, we select distance function with the highest clustering accuracy for the single, complete and average linkages, respectively, as specified in Table <ref type="table" target="#tab_3">3</ref>. We apply euclidean distance for Ward.D linkage. Additionally, the BIRCH algorithm (balanced iterative reducing and clustering) <ref type="bibr" target="#b147">[146]</ref> stores summary information using a dynamic clustering feature tree structure. And it is advantageous for large data sets and data with outliers <ref type="bibr" target="#b141">[140]</ref>.</p><p>• BIRCH: The BIRCH algorithm (Balanced Iterative Reducing and Clustering) stores summary information about candidate clusters in a dynamic tree data structure. This tree hierarchically organizes the clusters represented at the leaf nodes. The tree can be rebuilt when a threshold specifying cluster size is updated manually, or when memory constraints force a change in this threshold. This algorithm has a time complexity linear in the number of instances.</p><p>• K-Means &amp; extensions K-means <ref type="bibr" target="#b76">[75]</ref> partitions observations into clusters by minimizing the withincluster euclidean distance. Starting with initial random centroids, the K-means algorithm iteratively clusters objects and constructs new centroids until convergence, with the goal of minimizing the within-cluster sum of squares. Numerous extensions to the K-means algorithm have been proposed, such as the Mini Batch K-Means <ref type="bibr" target="#b109">[108]</ref> randomly selects a subset of observations at each iteration, so as to provide faster computation but lower accuracy than K-means. On the other hand, instead of starting with random initialization of centroids, K-means++ <ref type="bibr" target="#b13">[13]</ref> initialize distant centroids, which may lead to higher accuracy than regular K-means.</p><p>• Spectral clustering Spectral clustering <ref type="bibr" target="#b130">[129]</ref> first projects the affinity matrix of data into lowerdimensional embedding, then performs clustering methods on the resulting space. The affinity matrix can be constructed using a kernel function, such as radial basis function (RBF) kernel, or by computing a graph of nearest neighbors.</p><p>• Gaussian mixture model The Gaussian mixture model <ref type="bibr" target="#b98">[97]</ref> assumes the observations are generated from a mixture of Gaussian distribution. It can be regarded as a generalized K-means algorithm but it takes the covariance structure of data into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementations</head><p>• Hierarchical clustering + Average linkage: The sklearn.cluster.AgglomerativeClustering function with linkage "average" is used to construct hierarchical clustering. All parameters are set as default.</p><p>• Hierarchical clustering + Ward.D linkage: The sklearn.cluster.AgglomerativeClustering function with linkage "ward" is used to construct hierarchical clustering. All parameters are set as default.</p><p>• Hierarchical clustering + Complete linkage: The sklearn.cluster.AgglomerativeClustering function with linkage "complete" is used to construct hierarchical clustering. All parameters are set as default.</p><p>• Hierarchical clustering + Single linkage: The sklearn.cluster.AgglomerativeClustering function with linkage "single" is used to construct hierarchical clustering. All parameters are set as default.</p><p>• BIRCH: The sklearn.cluster.Birch is used to construct Birch clustering. All parameters are set as default.</p><p>• K-Means: The sklearn.cluster.KMeans is used to construct KMeans clustering. All parameters are set as default.</p><p>• K-means++: The sklearn.cluster.KMeans is used to construct KMeans clustering. "init" is set as "k-means++" and all other parameters are set as default.</p><p>• Mini Batch K-Means: The sklearn.cluster.MiniBatchKMeans is used to construct MiniBatch K-Means clustering. All parameters are set as default, where the subsample size is set as 3 * n_clusters.</p><p>• Spectral clustering + nearest neighbors as affinity: The sklearn.cluster.SpectralClustering is used to construct Spectral clustering. "affinity" is set as "nearest_neighbors" and all other parameters are set as default, where K-means clustering is used.</p><p>• Spectral clustering + radial basis function (RBF) kernel as affinity: The sklearn.cluster.SpectralClustering is used to construct KMeans clustering. "affinity" is set as "rbf" and all other parameters are set as default, where K-means clustering is used.</p><p>• Gaussian mixture model: sklearn.cluster.GaussianMixture is used to construct Gaussian mixture clustering. All parameters are set as default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.3 Dimension reduction</head><p>For dimension reduction methods, we select the most widely used linear global technique: principal component analysis (PCA) <ref type="bibr" target="#b4">[4]</ref>, and include random projection as a base line. For non-linear manifold learning methods, we select methods that preserve global properties such as metric/non-metric multidimensional scaling (MDS) <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b9">9]</ref> and Isomap <ref type="bibr" target="#b16">[16]</ref>, and methods that preserve local properties including t-distributed Stochastic Neighbor Embedding (tSNE) <ref type="bibr" target="#b126">[125]</ref>, Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP) <ref type="bibr" target="#b78">[77]</ref>, spectral embedding with different affinity <ref type="bibr" target="#b111">[110]</ref>. In addition, we apply a deep auto-encoder framework <ref type="bibr" target="#b64">[63]</ref> with a three-layered encoder and a three-layered decoder, and the low-dimensional code produced by the encoder is used as the reduced dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear models</head><p>• PCA Principal component analysis linear transforms data of high dimensions to lower coordinates which can explain most of the variance of the original data.</p><p>• Gaussian random projection Random projection is a linear dimension reduction technique by applying multiplication to a random Gaussian matrix. The reduced dimensions can preserve the distances among points via Johnson-Lindenstrauss Lemma, and the algorithm is fast and robust to outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-linear manifold learning</head><p>• Global methods -Metric MDS(euclidean) Multidimensional scaling generates lower dimension from given e pairwise distance between data points.</p><p>-Non-metric MDS Non-metric MDS is a variant of MDS method which aims to preserve the rank of similarities.</p><p>-Isomap Isomap is a nonlinear manifold learning method that conducts isometric mapping, using the geodesic distance from embedded neighborhood graph.</p><p>• Local methods -tSNE t-distributed stochastic neighbor embedding <ref type="bibr" target="#b126">[125]</ref> maps high dimensional data into 2 or 3 dimension. It first constructs a probability distribution that reflects similarities of points and then aims to minimize its KL divergence to another similar probability distribution in lower dimensions. t-SNE is popular for visualization purposes.</p><p>-UMAP Uniform Manifold Approximation and Projection (UMAP) is another nonlinear dimension reduction technique that is popular for visualization. It aims to find lower dimensional data with the closest fuzzy topological structure as the original data.</p><p>-Spectral embedding Spectral embedding utilizes the spectral of an affinity matrix of data and projects into lower-dimensional embedding.</p><p>• Deep Learning -Deep Autoencoder Autoencoder is a type of neural network that encodes the information of data into a small number of neurons and then reconstruct the message by decoding. The code layer of the autoencoder can be regarded as the reduced dimensions of the input data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation</head><p>• PCA: The sklearn.decomposition.PCA is used to construct principal component analysis. All parameters are set as default.</p><p>• Random projection: The sklearn.decomposition.GaussianRandomProjection is used to construct principal component analysis. n_components is set as the given rank, and all other parameters are set as default.</p><p>• Metric MDS(euclidean): The sklearn.decomposition.MDS is used to construct MDS. n_components is set as the given rank, metric as True, and all other parameters are set as default.</p><p>• non-Metric MDS: The sklearn.decomposition.MDS is used to construct non-metric MDS. n_components is set as the given rank, metric as False, and all other parameters are set as default.</p><p>• Isomap: The sklearn.decomposition.Isomap is used to construct Isomap. n_components is set as the given rank, and all other parameters are set as default.</p><p>• tSNE: The sklearn.decomposition.TSNE is used to construct tSNE. n_components is set as the given rank, learning_rate as 100, and all other parameters are set as default.</p><p>• UMAP: We set the learning rate as 1, number of maximum iteration as 200, number of neighbors as 15 and minimum distance as 0.1</p><p>• Spectral embedding + nearest neighbors affinity: The sklearn.decomposition.SpectralEmbedding is used to construct Spectral embedding. n_components is set as the given rank, "affinity" as "nearest_neighbors, and all other parameters are set as default.</p><p>• Spectral embedding + radial basis function (RBF) kernel: The sklearn.decomposition.SpectralEmbedding is used to construct Spectral embedding. n_components is set as the given rank, "affinity" as "rbf", and all other parameters are set as default.</p><p>• Deep Autoencoder: We build an auto-encoder using 3 hidden layers in the encoder and 3 hidden layers in the decoder. The numbers of nodes in the hidden layers of the encoder are M/6, M/12, and M/24, respectively; the numbers of nodes in the hidden layers of the decoder are M/24, M/12 and M/6, respectively; and size of code is 2. We use internal ReLu activations with a sigmoid activation in the final layer, categorical cross-entropy loss, and adam optimizer. The Autoencoder model is fitted using 90% of the data with 20 epochs and validated with the rest 10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Results</head><p>This appendix provides detailed empirical results that support the main findings presented in Section 3.</p><p>While the main text focuses on summarizing high-level insights, here we include extended figures and dataset-level observations across all tasks (classification, regression, clustering, and dimension reduction) to answer the reliability questions Q1-Q3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Q1: Within-method stability</head><p>Figures 2, 3 summarize empirical results of feature importance methods in classification and regression tasks, measured with 10 top features and AO metrics. Note that the model-agnostic methods permutation and Shapley value are computationally inefficient and did not finish running one repeat within 12 hours in some of the large N or large P data sets. Figure <ref type="figure" target="#fig_3">4</ref> shows the summarized empirical results in the clustering task, measured with ARI metrics with data splitting, and Figure <ref type="figure" target="#fig_4">5</ref> summarized results for dimension reduction.</p><p>Across tasks, the heatmaps of within-method stability (A) demonstrates overall, (1) the interpretations generated by IML methods are still not always consistent; (2) an IML method can have quite different levels of stability in different data sets, regardless of the data sizes; and (3) for a single data set, different IML methods result in different stability levels. The bump plots in (B) convey the message of no free lunch: there is no IML method that can work universally well in all data. The permutation (RF) and RF generate the most consistent interpretations over all data sets in both regression and classification.</p><p>Another takeaway from these figures is that simple methods are not necessarily the most consistent ones. Permutation (RF), which is a more complicated model, is quite consistent overall. But the trade-off is that it would take a longer time to compute. Shapely values and MLP-related methods are the least consistent ones on average. Combining with results in (A) and (C), we can infer that levels of interpretation stability of IML methods are quite different, even though they are similarly accurate, such as in the PANCAN, Author, Statlog, Bean, and Call data sets in classification. Therefore, models with similar accuracy may not have similar interpretations.</p><p>Classification In Figure <ref type="figure" target="#fig_1">2</ref>, we can see that the linear and tree-based IML models are more consistent in large N data sets, such as the strong performance in the high dimensional PANCAN data. MLP-based methods also work better in larger N data sets than high dimensional data, but they in general produce less consistent interpretation stability scores. The model-agnostic method permutation has good performance with linear or tree as base models. But another model-agnostic method Shapley value is not reliable in most of the data sets.</p><p>We observe that a single IML method can have different levels of stability in different data sets, regardless of the data sizes. For example, in classification, logistic ridge regression has an AO of 0.806 in PANCAN data, but its AO score is only 0.224 in the DNase data, where both of the data are high dimensional. For a single data set, different IML methods result in different stability levels. In the DNase data, interpretations from permutation with RF are quite consistent with over AO of over 0.9, but the AO scores of other IML methods are all below 0.3. We also notice that most of the methods have less reliable interpretations in large P data sets. And in general, RF is a better choice for data with a moderate # observation/# feature ratio. Tree-based and linear models usually work better than MLP methods in terms of interpretation stability. Comparing model-agnostic methods, permutation with RF and logistic ridge as base models better performance on large N data. On average, the permutation method is more consistent than Shapley values.</p><p>The heatmap in Figure <ref type="figure" target="#fig_1">2</ref>(D) shows the average predictive accuracy on test sets of each ML method on each data, and the accuracy scores are similar across the ML methods. None of the methods have high accuracy scores in Madelon, Amphibians, and Theorem, which are more difficult to classify. All of the methods have over 0.9 accuracies in Author, Call, and Bean data sets. Linear models and tree-based models result in high accuracy score of over 0.9 in most of the data sets, except the decision tree is less accurate in most of the high dimensional data. MLP is also relatively accurate in most of the data sets.</p><p>Regression From Figure <ref type="figure" target="#fig_2">3</ref>, we see that as compared to classification results in Figure <ref type="figure" target="#fig_1">2(A)</ref>, the stability scores of linear and tree-based models are higher for the mid-size data sets including Music, Wine, CPU, and Bike. Also, the MLP-based methods and Shapley value-related methods also have better stability than in classification. The heatmap in Figure <ref type="figure" target="#fig_2">3(D)</ref> shows the average predictive accuracy on test sets of each ML method on each data. To be consistent with other tasks, in regression, we transform the prediction loss measurement to an accuracy metric by taking the exponential of negative MSE between the predictions and true response in the test sets. We validate our choice of accuracy transformation in regression through Figure <ref type="figure" target="#fig_6">7</ref>, which shows that the transformation is almost linear for smaller MSE values and the transformed accuracy scores are within the proper range when the MSE is too large. All ML methods make accurate predictions on Tecator and Residential data sets, and less accurate predictions for the Star and News data. And they have mixed levels of prediction accuracy in Riboflavin, Word, and Blog. Combining the results in Figure <ref type="figure" target="#fig_2">3</ref>(A) and (D), we can still infer that models with similar accuracy may not have similar interpretations. And note that though the prediction accuracy for Wine is moderate (around 0.5), the interpretation stability is relatively high for most of the IML methods (around 0.9). Clustering We find that a clustering algorithm can have vastly different levels of stability in different data sets. Even for a single data set, different IML methods result in different stability levels. For example, in Figure <ref type="figure" target="#fig_3">4</ref>, interpretations in Religion data are quite inconsistent, except Birch has over 0.9 stability score. The heatmap in Figure <ref type="figure" target="#fig_3">4</ref>(C) shows the average clustering accuracy using the true label of each clustering method on each data. It matches our expectation that a clustering algorithm can have different levels of accuracy in different data sets, due to different assumptions about the data structure. When combining results in Figure <ref type="figure" target="#fig_3">4</ref>(A) and (D), we can infer that higher levels of clustering accuracy actually do lead to higher interpretation stability, as they are both based on clustering labels. However, the overall stability scores are higher than accuracy levels, as the interpretations can be consistently incorrect. For example, the clustering accuracy of HC (single) is low in most of the data sets, but it has moderate interpretation stability. With a clustering ARI of 0.472 in the Tetragonula data, HC (single) reaches an interpretation stability of 0.993. The interpretations yielded from K-Means and HC-related clustering algorithms in Iris and Tetragonla data sets are quite consistent (around 0.8), even though their clustering accuracy scores are around 0.5. The bump plot in (B) carries similar information as in the feature importance. For different data sets, the most consistent clustering algorithm is different, and there is no pattern relating to the data's # observation/# feature ratio. On average, the spectral (RBF) and K-Means++ generate the most consistent clustering over all data sets, and HC (single) generates the least consistent clustering labels in data splitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dimension Reduction</head><p>We evaluate the interpretation stability from two perspectives in the dimension reduction tasks: clustering stability on reduced dimension and nearest neighbor stability. Figure <ref type="figure" target="#fig_4">5</ref>(A) and (B) show the within-method clustering label stability, using hierarchical clustering with ward linkage and Euclidean distance on the reduced dimension with rank 2. The stability is measured by the ARI metric. Plot (A) and (B) in Figure <ref type="figure" target="#fig_7">8</ref> show the within-method nearest neighbors stability with noise addition perturbation using normal noise with a standard deviation of 0.15, measured by the NN-Jaccard-AUC score, as explained in Section 2.3 and Section A.2. From both figures, we still have similar answers to Q1 that the interpretations are not reliable within-method. Similar to the clustering task, the accuracy and stability of clustering on the reduced dimensions demonstrate similar trends. Veronica has both relatively high clustering accuracy and stability. As Darmanis data is a single-cell RNA-seq data set, it contains a large number of zeros. Its overall performance would be better with feature selection or imputation.</p><p>The method of random projection can be used as a baseline model, which yields the lowest accuracy and stability overall. The local methods t-SNE and UMAP work well for the bulk cell RNA-seq PANCAN data, but have poor performance in the single cell RNA-seq Darmanis data, possibly due to the sparsity of scRNA-seq data. Also, inaccurate methods/data may have consistent clustering labels. For example, TCGA data has 0 clustering accuracy for all methods, but its labeling stability is moderate. The global methods PCA and spectral clustering can result in consistent clustering labels on average, as shown in plot (B) in Figure <ref type="figure" target="#fig_4">5</ref>.</p><p>The nearest neighbor stability is relatively low for most methods and data sets, as shown in plot (A) Figure <ref type="figure" target="#fig_7">8</ref>. Comparing the within-method stability heatmaps (A) in Figure <ref type="figure" target="#fig_4">5</ref> and Figure <ref type="figure" target="#fig_7">8</ref> and bump plots (B) in Figure <ref type="figure" target="#fig_4">5</ref> and Figure <ref type="figure" target="#fig_7">8</ref>, on the same reduced dimensions, the stability ranks of nearest neighbors are quite different from that of clustering. For example. even though the clustering labels based on DAE are not very consistent, the nearest neighbors are quite consistent with larger values, especially for the Statlog and Tetragonula data sets. And the bump plot (B) Figure <ref type="figure" target="#fig_7">8</ref> shows that DAE generates the most consistent nearest neighbors, while it is the least consistent in clustering. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Q2: Between-method stability</head><p>The between-method stability heatmaps in Figures <ref type="figure" target="#fig_2">2, 3</ref>(C) show that in classification and regression, on the same training set, the interpretations from different IML methods are not consistent with each other, while the predictions in the test sets (E) across different IML methods are quite consistent. Combining with results in (C) and (E), we can infer IML methods can generate quite different interpretations of the same data, even though their predictions on the test set are very consistent. The tree-based models and their related model-agnostic methods are relatively consistent with each other, and the same is true for linear models.</p><p>Classification Most of the AO values in the between-method stability heatmap in (C) ranging from 0.2-0.3 in classification. The tree-based methods decision tree, RF, and XGB are relatively more consistent with each other with AO around 0.4, and permutation methods are more consistent with their base models, as the ML models are the same. Also, we notice that Epsilon-LRP is relatively consistent with Shapley value (MLP), with an AO of 0.6. The predictions in the test sets (E) across different IML methods have pairwise accuracy all greater than 0.7. The linear models SVM and logistic ridge have similar predictions with a pairwise prediction stability score of over 0.9, and the same is true for RF and XGB, which are both tree ensemble methods. In addition, though SVM and logistic ridge have pairwise prediction stability of 0.917, their interpretation stability AO score is only 0.34. Therefore, models with similar predictions may not have similar interpretations. Detailed figures of each data tell a similar story, as shown in Figure <ref type="figure" target="#fig_8">9</ref>.</p><p>Regression The overall stability scores in the regression task are higher than in classification. Epsilon-LRP is quite consistent with Shapley value (MLP) as well. The prediction stability in the middle scatterplots is measured by the exponential negative entropy, where entropy is measured by the average standard deviation of all predicted values for every single observation, which is detailed in Section 2.3 and Section A.2. Most of the ML predictions are fairly consistent with each other with over 0.75 stability scores, except that the predictions of decision trees are less consistent with linear or MLP methods. Clustering The between-method stability heatmap in Figures <ref type="figure" target="#fig_3">4(C</ref>) shows that on average, the interpretations from different clustering algorithms are moderately consistent with each other with most of the ARI values around 0.5. The K-Means and K-Means++ are quite consistent with each other with an ARI of 0.812. All other methods are similarly consistent with each other, except HC(single), which is not accurate overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dimension Reduction</head><p>We consider the clustering label stability across different methods on the reduced dimension. The overall stability scores across different methods are moderate with most of the ARI around 0.5, as shown in the heatmap (C) in Figure <ref type="figure" target="#fig_4">5</ref>, similar to the clustering results. Clustering labels based on t-SNE are relatively consistent with PCA, Spectral (NN), and UMAP with ARI greater than 0.7, while PCA, Spectral (NN), and UMAP are not very consistent with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Q3: Interpretation stability vs. accuracy</head><p>Note that under the task of clustering, we treat the clustering labels are a kind of interpretation of the underlying structure of the data set. Therefore, Q3 does not apply here as the model accuracy is also calculated by the resulting labels.</p><p>To explore whether predictive accuracy can be used as an indicator for interpretation stability, besides the scatterplots shown in Figure <ref type="figure" target="#fig_1">2</ref>, we further investigate the relationship between prediction stability and interpretation stability, as well as the relationship between prediction stability and prediction accuracy. In both Figure <ref type="figure" target="#fig_9">10</ref> and Figure <ref type="figure" target="#fig_10">11</ref>, scatterplots (A) are aggregated over each data set, where colors represent different data sets, and scatterplots (C) are aggregated over each IML method, where colors represent different methods, all averaged over 100 repeats. In each scatter plot, we construct fitted regression lines over one data or one method. In scatterplots (A) and (C), the left scatterplots are the same as Figure <ref type="figure" target="#fig_1">2</ref>, which reveals the relationship between prediction accuracy and interpretation stability; the middle scatterplots aim to illustrate the relationship between prediction stability on the test set and the interpretation stability; and the right scatterplots explore the relationship between prediction stability and the prediction accuracy. We also visualize the relationships by fitting regression lines in each plot, either aggregated over data or IML methods. Lines with significant slope coefficients indicate there might be a stronger relationship between the values on the y-axis and the values on the x-axis, and flat lines indicate fewer correlations. To quantify the relationships among the interpretation stability, prediction stability, and accuracy scatterplots with respect to each method as well as each data, we also report the table of corresponding p-values of the significance of the fitted coefficients. The tables (B) and (D) in Figure <ref type="figure" target="#fig_9">10</ref> and Figure <ref type="figure" target="#fig_10">11</ref> quantify the relationships among the interpretation stability, prediction stability, and accuracy scatterplots with respect to each method as well as each data. Similar to the classification results, on a given data set, higher predictive accuracy from an IML method does imply higher interpretation stability, as most of the fitted lines are quite flat in the left plot of (A) in Figure <ref type="figure" target="#fig_10">11</ref>. Only the fitted lines of two data sets have significant p-values (&lt;0.05) after the Bonferroni correction. From the perspective of IML methods, although all of the fitted lines seem to have positive coefficients, none of them are significant with p-values equal to 1, due to high variance. Therefore, for a given method, higher accuracy on one data set does not imply higher interpretation stability. None of the classification IML methods result in significant connections between interpretation stability and predictive accuracy.</p><p>The prediction stability in the middle scatterplots is measured by the purity of the predicted label for every single observation, over the multiple train/test splits, which is detailed in Section 2.3. As shown in the second columns, on a given data set, consistently predicted estimates do not imply higher interpretation stability. And for a given method, higher prediction stability on one data set does not imply higher interpretation stability as there is no significant correlation for each method as well, though all of the MLP-related methods have noticeably positive coefficients. Therefore, stability in prediction does not imply stability in feature importance interpretations.</p><p>Classification From the left scatterplots for classification, only the fitted line of data Statlog and Amphibians have significant p-values (&lt;0.05) after Bonferroni correction, as shown in the p-value table (C) of Figure <ref type="figure" target="#fig_9">10</ref>. From the perspective of IML methods, though there are several methods that have positive coefficients (logistic ridge, tree, XGB, and Epsilon-LRP (MLP), the coefficients are not significant with p-values equal to 1 after Bonferroni correction, because of the high variance of the results from different data sets.</p><p>The prediction stability in the middle scatterplots is measured by the purity of the predicted label for every single observation, over the multiple train/test splits, which is detailed in Section 2.3. The fitted lines in both of the middle scatterplots are similar to the left ones. As shown in the second columns in table (B) and (D) of Figure <ref type="figure" target="#fig_9">10</ref>, on a given data set, consistently predicted estimates do not imply higher interpretation stability. Only the predictive stability and interpretation stability are significantly correlated in Statlog and Amphibians data sets. And for a given method, higher prediction stability on one data set does not imply higher interpretation stability as there is no significant correlation for each method as well, though SVM, logistic ridge, tree, XGB, and Epsilon-LRP (MLP), and guided backpropagation (MLP) have noticeably positive coefficients. Therefore, stability in prediction does not imply stability in feature importance interpretations as well in classification. The right figures plot the prediction stability against predictive accuracy in test sets. We would expect the coefficients in the right scatterplot to be significant as more accurate predictions should be more consistent with each other. As shown in the third columns in table (B) and (D) of Figure <ref type="figure" target="#fig_9">10</ref>, all of the classification data sets except Call and TCGA have significant coefficients, and the same is true for IML methods except SVM and some of the MLP-based methods. The results indicate in most cases, prediction stability is significantly correlated with predictive accuracy, which matches our expectations as accurate predictions are similar. Regression Similar to the classification results, on a given data set, higher predictive accuracy from an IML method does imply higher interpretation stability, as most of the fitted lines are quite flat in the left plot of plot (A) in Figure <ref type="figure" target="#fig_10">11</ref>. Only the fitted line of data Bike and Riboflavin have significant p-values (&lt;0.05) after Bonferroni correction, as shown in the p-value table (B) of Figure <ref type="figure" target="#fig_10">11</ref>. Note that in Riboflavin data, MLP-related methods perform poorly in terms of both prediction accuracy and interpretation stability with almost 0 values, which causes a significant p-value between predictive accuracy and interpretation stability. From the perspective of IML methods, although all of the fitted lines seem to have positive coefficients, none of them are significant with p-values equal to 1, due to high variance. Therefore, for a given method, higher accuracy on one data set does not imply higher interpretation stability. None of the classification IML methods result in significant connections between interpretation stability and predictive accuracy.</p><p>The prediction stability in the middle scatterplots is measured by the purity of the predicted label for every single observation, over the multiple train/test splits, which is detailed in Section 2.3. As shown in the second columns in table (B) and (D) in Figure <ref type="figure" target="#fig_10">11</ref>, on a given data set, consistently predicted estimates do not imply higher interpretation stability. Satellite and Bike data sets have significant coefficients between predictive stability and interpretation stability. And for a given method, higher prediction stability on one data set does not imply higher interpretation stability as there is no significant correlation for each method as well, though all of the MLP-related methods have noticeably positive coefficients. Therefore, stability in prediction does not imply stability in feature importance interpretations in regression. Though we believe more accurate predictions should be more similar to each other, in the case of regression, the prediction stability and accuracy are not significantly correlated in some of the data sets (right plot of (A) in Figure <ref type="figure" target="#fig_10">11</ref>) and for most of the IML methods (right plot of (C) in Figure <ref type="figure" target="#fig_10">11</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Dashboard</head><p>We have provided summary figures that demonstrate only a small part of the empirical results in this supplement material. In total, our empirical study includes over 50 IML methodologies, 33 data sets, over 15 perturbation variations with different noise types and levels, 8 stability metrics, the number of top features from 1 to 30 in feature importance, and the number of ranks 2, 5 and 10 in dimension reduction. We have run over of 250,000 repeats throughout this empirical study in total, with 100 repeats for each feature importance method and 50 repeats for each unsupervised method. More summary figures and detailed figures of more IML methods, benchmark data sets, stability metrics, and different perturbation settings can be found in our well-developed interactive dashboard https://iml-reliability.herokuapp.com/home. However, researchers may ask the questions that, what if they develop an IML methodology, have a new data set to explore, or are interested in some other stability metrics other than the ones we provide? Can they evaluate and compare such reliability results under the same framework? Therefore, in addition to providing the full results for researchers and machine learning practitioners to explore, we also build a powerful function that allows users to easily upload, evaluate, and compare their own reliability results. Such results can be generated with the reliability Python package we develop, which can be found at https://github.com/DataSlingers/IML_reliability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 App interface and interactive options</head><p>The dashboard consists of seven pages (Figure <ref type="figure" target="#fig_11">12</ref>) including a home page ("Home"), an instruction page ("How to use"), and 5 results pages: "Feature Importance (Classification)", "Feature Importance (Regression)", "Clustering", "Dimension Reduction (Clustering)" and "Dimension Reduction (KNN)". All of the results are computed prior to dashboard visualization. We construct results figures based on which question they aim to answer and the users can explore different summary and detailed figures corresponding to different questions by selecting the options in (B).</p><p>In classification and regression tasks, we conduct reliability tests on 13 data sets with 20 IML methods prior to dashboard visualization. The default figures show the results of 13 IML methods of all data sets, including model-specific methods SVM, (Logistic) Ridge, decision tree, RF, XGB, epsilon-LRP (MLP), guided backpropagation (MLP), and model agnostic methods permutation and Shapley values, applied with base models (Logistic) Ridge, RF, and MLP, respectively. We set the default number of top K features to be evaluated as 10 and the reliability metric as AO because it puts higher weights on the top-ranked features. The details of parameters can be found in Section A.2. Some may argue that the feature ranks can be similar if K is too large and includes noise features. In the dashboard, users can explore how the reliability changes with different numbers of top features by varying K (1-30) using the dropdown components. And results with reliability metrics Jaccard and Kendall's Tau are available by changing the "stability Metric" choices, which will trigger interactive actions to update the figures.</p><p>On the clustering task results page, we present reliability results on 10 popular clustering methods with 14 benchmark data sets. We calculate the results with two kinds of perturbation methods: data splitting and noise addition. The default figures show the results with data splitting with ARI metric for both interpretation stability and clustering accuracy. Users can explore the IML reliability with noise addition by changing the "Perturbation Method" to "Noise Addition", and two more parameter selections will show up when "Noise addition" is selected: type of added noise (normal or Laplace) and the noise level measured by the variance of added noise (sigma). The dashboard will update the figures based on data with N (0, 0.5) noise addition. Since some IML methods may be more sensitive to noise than others, users can explore how the stability as well as accuracy change when the noise level increases. And some may be curious about whether the types of noise would make a difference to the reliability results, so we also provide results with noise generated by Laplace distribution.</p><p>We have two pages for the reliability results of dimension reduction methods, examining the clustering stability and the stability of local neighbors on the reduced dimensions, respectively. The "Dimension Reduction (Clustering)" page is similar to the "Clustering" page in terms of options and layouts. We have one additional parameter to choose from in dimension reduction tasks: the rank of reduced dimension. As researchers often use rank 2 in data analysis, we start with the clustering stability results with data splitting perturbation using hierarchical clustering applied on the top two reduced dimensions. As the optimal ranks that can explain most of the variance can be different in data sets, users can change the "Dimension Rank" option to visualize the differences. Also, the performance might also depend on the clustering algorithm, so we also provide results using K-Means clustering to explore. Still, we provide reliability results with noise addition, either generated from the normal distribution or Laplace distribution, and the users are free to explore how the stability changes with different levels of noise added by varying "sigma" values.</p><p>The "Dimension Reduction (KNN)" page shows the results of local neighbor stability in reduced dimensions. We develop a metric to measure the neighborhood stability by NN-Jaccard-AUC score, as explained in Sections 2.3 and A.2. In this task, we only demonstrate results to answer Q1: If we sample a different training set, are the interpretations similar, as our metric mainly focuses on within-method stability and there is no ground truth to calculate accuracy. Still, users are free to change noise addition-related parameters and the number of reduced dimensions to evaluate the reliability of IML methods.</p><p>In all three tasks, we include the results of all the data sets and all IML methods we used in this empirical study and at the same time provide drop-down options for users to select the specific data and IML methods of their interests. In addition, researchers may have their own data or IML methods of interest using our Python package that enables users to generate reliability results that can be integrated into the dashboard. After generating reliability results from the imlreliability python package, users can upload the new results to visualize and compare to existing results via the drag-and-drop dashboard builder in (A). In addition, if users are interested in any other methods to measure interpretation stability, our package also allows them to try out their metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Interactive Figures</head><p>The interactive figures in our dashboard are organized by the three types of IML questions they aim to address, and users can visualize the results by selecting options in "Select IML questions".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.1 Q1</head><p>To address Q1 of the within-method stability, we design three types of summary figures to illustrate our empirical results, which are aggregated over data sets: (1) within-method stability heatmap, (2) Within-method stability bump plot, and (3) within-method stability line plot. And we have one detailed figure which shows the stability line plot with different levels of top K features/noise level/number of neighbors of all data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Summary Figures</head><p>Within-stability heatmap We aim to address question 1 by measuring whether interpretations are consistent among repeats, within each method. As shown in Figure <ref type="figure" target="#fig_14">15</ref>, the left summary heatmap demonstrates the average pairwise stability scores of interpretations of an IML method aggregated over 100 repeats, ranging in [0,1], with IML methods on the x-axis and data set on the y-axis, and the heatmap on the right side presents the each method's prediction accuracy on the test set, averaged over 100 repeats. Figure <ref type="figure" target="#fig_14">15</ref> is an example of a within-method stability heatmap for classification feature importance methods. The left heatmap shows the within-method stability scores of each IML method on each data set, with higher values and darker colors indicating higher stability. The blank cells are missing results because those IML methods' running times are over 12 hours for one repeat on those data sets. The right heatmap shows the prediction accuracy on test sets of each data, with higher values and darker colors indicating higher test prediction accuracy, including Logistic Ridge, SVM, Tree, RF, XGB, and MLP. From these two heatmaps, we can evaluate how each IML method performs cross different data sets, from large N to high dimensional data. Also, within one data set, we can compare the interpretation stability of different IML methods. Comparing the accuracy heatmap with the stability heatmap, we can see that even though some IML methods can generate accurate test predictions, their interpretation of important features can be inconsistent and unreliable. Users can also get more specific information by hovering over the cell of interest.  Within-stability bump plot The bump plot addresses Q1 by ranking IML methods by their stability in each data set. As shown in the example Figure <ref type="figure" target="#fig_16">17</ref>, the data sets are ordered by their # observation/ # ratio, with the left y-axis showing the rank of each method based on stability. We add a column of average stability over all methods at the right, and the methods of the y-axis on the right are ordered by the average stability, from the most consistent to the least consistent. There are some missing values in high dimensional data sets due to the large running time. And in the task of clustering, we aim to measure the relationship between interpretation stability and the noise level, measured by the variance of noise added to the data. In the stability of local neighbors in dimension reduction, we plot the Jaccard scores with the number of neighbors. Figure <ref type="figure" target="#fig_17">18</ref> illustrates the AO score against the number of top featuers K in feature importance of classification methods for each data set, where colors represent different clustering methods. We are able to explore how the stability scores change with different numbers of top features and compare the trends of different IML methods in each data set. The stability scores increase with a larger number of K, which makes sense as more noise features are included. We design a summary heatmap to address Q2 of the interpretation stability between methods, which is aggregated over datasets. We also present detailed between-method stability heatmaps of all datasets separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Summary Figures</head><p>Cross-stability heatmap Among different methods, we aim to evaluate whether different methods would result in similar interpretations on the same perturbed data. As shown in Figure <ref type="figure" target="#fig_18">19</ref>, an example of a between-method stability heatmap for classification feature importance methods, the left heatmap shows the between-method average stability of interpretations obtain from each pair of IML methods, and the right heatmap shows the average stability of prediction on the test set. For instance, the cell of method LogisticRidge and method SVM represents the stability between their top K feature rankings, averaging over 100 repeats and over all data sets. The stability metric can be chosen AO, Jaccard, and Kendall's Tau. Higher values and darker colors indicate more similar interpretations between the two IML interpretations. The right heatmap shows the pairwise between-method stability of predictions on the test sets, where higher values and darker colors indicate the two methods provide more similar predictions on the same test set. Detailed between-method stability heatmap The detailed between-method stability heatmap is similar to the summary heatmap figure, but it shows the between-method average stability of interpretations in each data set. Figure <ref type="figure" target="#fig_8">9</ref> is a subset of detailed between-method heatmap in classification. Since the IML performance can vary with different data sizes or the different underlying data structures, we can explore how similar the interpretations generated by different IML methods are for each data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.3 Q3</head><p>We address Q3 "Does higher accuracy leads to more consistent interpretations?" by plotting two sets of scatterplots to explore the relationships among interpretation stability, prediction accuracy, and prediction stability. These figures demonstrate the detailed scatter plots of interpretation stability and prediction accuracy of each data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Summary Figures</head><p>Interpretation stability, prediction stability &amp; accuracy scatterplots We address Q3 with six scatterplots, which explore the relationship among model prediction accuracy, prediction stability, and interpretation stability, with each dot representing one IML method on one data set. As shown in the scatterplots Figure <ref type="figure" target="#fig_9">10</ref>(A),(C), which are aggregated over each data set, or each IML method, respectively. The figures demonstrate the relationship between prediction accuracy on the test set and the interpretation stability (left), the relationship between prediction stability on the test set and the interpretation stability (middle), and the relationship between prediction stability and the prediction accuracy (right). In each scatter plot, we construct fitted regression lines over one data. The significance of the fitted coefficient implies whether there exists a strong relationship between values on the y-axis and the values on the x-axis. The relationships are also quantified by the p-values for testing the fitted coefficients significance, which is reported in the tables Figure <ref type="figure" target="#fig_9">10</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: IML Performance on Classification Tasks. A: Heatmap of within-method interpretation stability. B: Bump plot of IML methods ranked by the level of interpretation stability. C: Heatmap of between-method interpretation stability. D: Heatmap of between-method prediction accuracy on test sets. E: Heatmap of prediction stability on test sets. F: Scatterplot of accuracy and interpretation stability, colored by data sets, with fitted OLS lines aggregated over data. G: Scatterplot of accuracy and interpretation stability, colored by data sets, with fitted OLS lines aggregated over IML methods.</figDesc><graphic coords="11,63.93,209.02,88.63,115.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: IML Performance on Regression Tasks. A: Heatmap of within-method interpretation stability. B: Bump plot of IML methods ranked by the level of interpretation stability. C: Heatmap of between-method interpretation stability. D: Heatmap of between-method prediction accuracy on test sets. E: Heatmap of prediction stability on test sets. F: Scatterplot of accuracy and interpretation stability, colored by data sets, with fitted OLS lines aggregated over data. G: Scatterplot of accuracy and interpretation stability, colored by data sets, with fitted OLS lines aggregated over IML methods.</figDesc><graphic coords="13,398.67,218.32,152.12,106.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: IML Performance on Clustering Methods. A: Heatmap of within-method interpretation stability. B: Bump plot of IML methods ranked by the level of interpretation stability. C: Heatmap of between-method interpretation stability. D: Heatmap of between-method prediction accuracy.</figDesc><graphic coords="14,67.20,278.67,235.63,177.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: IML Performance on Dimension Reduction Methods. A: Heatmap of within-method interpretation stability. B: Bump plot of IML methods ranked by the level of interpretation stability. C: Heatmap of between-method interpretation stability. D: Heatmap of between-method prediction accuracy.</figDesc><graphic coords="15,315.04,368.07,229.55,177.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: ROC curve of Jaccard score with the number of nearest neighbors K</figDesc><graphic coords="21,155.15,245.22,298.09,164.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Scatterplot of the exponential of negative MSE and MSE, colored by machine learning models of each regression data set. The transformation is almost linear for smaller MSE values and the transformed accuracy scores are within the proper range when the MSE is too large.</figDesc><graphic coords="33,57.60,132.01,496.82,457.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Interpretation stability of nearest neighbors in dimension reduction IML methods with rank 2 normal noise addition with sigma = 0.15. A: Heatmap of within-method interpretation stability. B: Bump plot of IML methods ranked by the level of interpretation stability.</figDesc><graphic coords="35,313.30,71.82,157.99,124.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Detailed Results: Between-method stability Heatmap of Each Data Set.</figDesc><graphic coords="36,57.60,57.60,496.80,151.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: A: Summary scatterplots for each data in classification. Left: Interpretations stability against prediction accuracy; Middle: Interpretations stability against prediction stability; Right: prediction stability against prediction accuracy. B: P-values of fitted coefficients in classification data sets. C: Summary scatterplots for each IML method in classification. Left: Interpretations stability against prediction accuracy; Middle: Interpretations stability against prediction stability; Right: prediction stability against prediction accuracy. D: P-values of fitted coefficients in classification IML methods.</figDesc><graphic coords="38,62.13,192.57,268.84,65.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Answering Q3 From the Dashboard. A: Summary scatterplots for each dataset used in regression. Left: Interpretations stability against prediction accuracy; Middle: Interpretations stability against prediction stability; Right: prediction stability against prediction accuracy. Figure 4 P-values of fitted coefficients in regression data sets. C: Summary scatterplots for each IML method in regression. Left: Interpretations stability against prediction accuracy; Middle: Interpretations stability against prediction stability; Right: prediction stability against prediction accuracy. D: P-values of fitted coefficients in regression IML methods.</figDesc><graphic coords="39,61.15,228.87,270.60,64.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Dashboard navigation bar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Screenshot of a list components of in feature importance (classification) page of the dashboard. (A): The user can upload their own reliability results for visualization using the drag-and-drop dashboard builder; (B): interactive choices to display figures, according to the specific IML questions; (C): parameters filters and selections of IML methods and data; (D): summary figures of IML results over all data sets; (E): detailed figures of IML results of each data set.</figDesc><graphic coords="41,57.60,57.60,496.80,284.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Example of Preloaded Datasets Within the Dashboard.</figDesc><graphic coords="42,229.67,57.60,149.04,95.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Summary Figure: Stability Heatmap of IML methods in Classification</figDesc><graphic coords="43,57.60,342.78,496.81,187.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Summary Figure: Stability Line Plot Aggregated over Datasets)</figDesc><graphic coords="44,57.60,81.01,496.80,177.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Summary Figure: Stability Bump Plot of Classification</figDesc><graphic coords="44,155.15,436.69,298.08,190.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Detailed Results: Relationship between Interpretation stability and the Number of Top Features K of Each Data Set</figDesc><graphic coords="45,57.60,210.54,496.80,192.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Summary Figure: Between-method stability Heatmap of Classification</figDesc><graphic coords="46,57.60,115.15,496.79,264.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>(B),(D). Bonferroni-corrected p-values less than 0.05 indicate significant correlations. Users can check more information by hovering over the scatterplots on the dashboard.• Detailed FiguresDetailed stability &amp; accuracy scatterplotsWe aim to explore the relationship between model prediction accuracy and interpretation stability of each data set. The detailed Figure20) is similar to the left figure of scatterplot (A) in Figure10, but separates the scatters by data sets. With scatters representing different IML methods, we can explore how the interpretation stability related to model accuracy in each data. For example, Figure20) shows the results in classification feature importance methods, and we can find that methods with higher accuracy do not necessarily have higher stability, such as the high dimensional PANCAN, DNase, and TCGA data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Detailed Results: Relationship between Interpretation stability and Predictive Accuracy of each data set</figDesc><graphic coords="47,57.60,436.53,496.78,189.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Feature Importance in Classification/Regression Methods Summary.</figDesc><table><row><cell>Model Agnostic Intrinsic/Post-hoc</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Dimension Reduction Methods Summary.</figDesc><table><row><cell cols="3">Dimension Reduction Method Global/Local Linear/Non-Linear</cell></row><row><cell>PCA</cell><cell>Global</cell><cell>Linear</cell></row><row><cell>Random projection</cell><cell>Global</cell><cell>Linear</cell></row><row><cell>Metric MDS (euclidean)</cell><cell>Global</cell><cell>Non-linear</cell></row><row><cell>Non-metric MDS</cell><cell>Local</cell><cell>Non-linear</cell></row><row><cell>Isomap</cell><cell>Global</cell><cell>Non-linear</cell></row><row><cell>Deep Autoencoder</cell><cell>-</cell><cell>Non-linear</cell></row><row><cell>tSNE</cell><cell>Local</cell><cell>Non-linear</cell></row><row><cell>UMAP</cell><cell>Local</cell><cell>Non-linear</cell></row><row><cell>Spectral Embedding</cell><cell>Global</cell><cell>Non-linear</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Summary of best-performing distance metrics for hierarchical clustering, and hyperparameter variants used for other clustering methods.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Stability Metrics Summary. C1 indicates metrics used for classification, C2 indicates clustering,</cell></row><row><cell>R indicates regression, and D indicates dimension reduction.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Datasets for classification, regression, and dimensionality reduction tasks. C1 indicates data used</cell></row><row><cell>for classification, C2 indicates clustering.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments G.I.A acknowledges funding from NSF DMS-1554821.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<idno>20-05-2025</idno>
		<ptr target="https://github.com/kundajelab/deeplift" />
		<title level="m">GitHub -kundajelab/deeplift: Public facing deeplift repo -github</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A unified framework of perturbation and gradient-based attribution methods for Deep Neural Networks interpretability. DeepExplain also includes support for Shapley Values sampling</title>
		<author>
			<persName><surname>Github -Marcoancona/Deepexplain</surname></persName>
		</author>
		<idno>ICLR 2018) -github.com</idno>
		<ptr target="https://github.com/marcoancona/DeepExplain" />
		<imprint/>
	</monogr>
	<note>Accessed 20-05-2025</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">13, G. data analysis: Baylor College of Medicine Creighton Chad J. 22 23 Donehower Lawrence A. 22 23 24 25, I. for Systems Biology Reynolds Sheila 31 Kreisberg Richard B</title>
		<imprint/>
	</monogr>
	<note>31 Bernard Brady 31 Bressler Ryan 31 Erkkila Timo 32 Lin Jake 31</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Thorsson Vesteinn 31 Zhang Wei 33 Shmulevich Ilya 31, et al. Comprehensive molecular portraits of human breast tumours</title>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">490</biblScope>
			<biblScope unit="issue">7418</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Principal component analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wiley interdisciplinary reviews: computational statistics</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="433" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Tennessee&apos;s Student Teacher Achievement Ratio (STAR) project</title>
		<author>
			<persName><forename type="first">C</forename><surname>Achilles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bellott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boyd-Zaharias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Folger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Word</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Peeking inside the black-box: a survey on explainable artificial intelligence (xai)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="52138" to="52160" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Local explanation methods for deep neural networks lack sensitivity to parameter values</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03307</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03292</idno>
		<title level="m">Sanity checks for saliency maps</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generalized non-metric multidimensional scaling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cayton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interpretable machine learning for discovery: Statistical challenges and opportunities</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and Its Application</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shap-based insights for aerospace phm: Temporal feature importance, dependencies, robustness, and interaction analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Alomari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Results in Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">101834</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Towards better understanding of gradient-based attribution methods for deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ancona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ceolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Öztireli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06104</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">k-means++: The advantages of careful seeding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">e0130140</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">How to explain individual classification decisions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Baehrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schroeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1803" to="1831" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The isomap algorithm and topological stability</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">295</biblScope>
			<biblScope unit="page" from="7" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bastani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09773</idno>
		<title level="m">Interpretability via model extraction</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Iterative random forests to discover predictive and stable high-order interactions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumbier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1943" to="1948" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Valid post-selection inference. The Annals of Statistics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Berk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="802" to="837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pattern recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Uci repository of machine learning databases</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blake</surname></persName>
		</author>
		<ptr target="http://www.ics.uci.edu/mlearn/MLRepository.html" />
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Machine learning for first-order theorem proving</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Bridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Paulson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of automated reasoning</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="141" to="172" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High-dimensional statistics with a view toward applications in biology</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Meier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and Its Application</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="255" to="278" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feedback prediction for blogs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Buza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data analysis, machine learning and knowledge discovery</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and principal curves for high resolution pseudotemporal ordering of single-cell rna-seq profiles</title>
		<author>
			<persName><forename type="first">K</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Ponting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">27219</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Machine learning and the physical sciences</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carleo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cirac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Daudet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vogt-Maranto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zdeborová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews of Modern Physics</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">45002</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A review of dimension reduction techniques</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Carreira-Perpinán</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="69" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science. University of Sheffield</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. CS-96-09</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Cardoso</surname></persName>
		</author>
		<title level="m">Machine learning interpretability: A survey on methods and metrics. Electronics</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">832</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Xgboost: extreme gradient boosting</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Khotilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
	<note>R package version 0.4-2</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An integrated encyclopedia of dna elements in the human genome</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">489</biblScope>
			<biblScope unit="issue">7414</biblScope>
			<biblScope unit="page" from="57" to="74" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling wine preferences by data mining from physicochemical properties</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cerdeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision support systems</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="547" to="553" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multidimensional scaling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Sireci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of applied multivariate statistics and mathematical modeling</title>
				<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="323" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digit images for machine learning research</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>best of the web</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="141" to="142" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Machine learning in finance</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Halperin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bilokon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">1170</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08608</idno>
		<title level="m">Towards a rigorous science of interpretable machine learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Considerations for evaluation and generalization in interpretable machine learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Explainable and interpretable models in computer vision and machine learning</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Trust in automl: exploring information needs for establishing trust in automated machine learning systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Drozdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weisz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Intelligent User Interfaces</title>
				<meeting>the 25th International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="297" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Techniques for interpretable machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="77" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Comparing top k lists</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fagin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on discrete mathematics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="134" to="160" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Event labeling combining ensemble detectors and background knowledge</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fanaee-T</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="127" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A proactive intelligent decision support system for predicting the popularity of online news</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vinagre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cortez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Portuguese Conference on Artificial Intelligence</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="535" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Interpretable explanations of black boxes by meaningful perturbation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3429" to="3437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A method for comparing two hierarchical clusterings</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Mallows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical association</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">383</biblScope>
			<biblScope unit="page" from="553" to="569" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Nest architecture and genetic differentiation in a species complex of australian stingless bees</title>
		<author>
			<persName><forename type="first">P</forename><surname>Franck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Good</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Rasplus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Oldroyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Ecology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2317" to="2331" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Inference for interpretable machine learning: Fast, model-agnostic confidence intervals for feature importance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Allen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.02088</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Explaining explanations: An overview of interpretability of machine learning</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Gilpin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Specter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kagal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 5th International Conference on data science and advanced analytics (DSAA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="80" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">How machine learning will transform biomedicine</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goecks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Heiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="92" to="101" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Understanding individual decisions of cnns via contrastive backpropagation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="119" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A survey of methods for explaining black box models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Monreale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Giannotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Result analysis of the nips 2003 feature selection challenge</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Presence of amphibian species prediction using features obtained from gis and satellite images</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Habib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K A</forename><surname>Maghasib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Al-Ghazali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Abu-Nasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Abu-Naser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Academic and Applied Research (IJAAR)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Applied artificial intelligence in modern warfare and national security policy</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Haney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hastings Sci. &amp; Tech. LJ</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Handbook of cluster analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meilă</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Murtagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rocci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning by neighbourhood discovery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Towards trustable explainable ai</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ignatiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5154" to="5158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">P. T. Inc. Collaborative data science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Explaining convolutional neural networks using softmax gradient layer-wise relevance propagation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Iwana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kuroki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4176" to="4185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Pca consistency in high dimension, low sample size context. The Annals of Statistics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Marron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="4104" to="4130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Xrai: Better attributions through regions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kapishnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4948" to="4957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A new measure of rank correlation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="1938">1938</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Nonlinear principal component analysis using autoassociative neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIChE journal</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="243" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Estimating mutual information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kraskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stögbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">66138</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Generalized distances between rankings</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
				<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="571" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Exact post-selection inference, with application to the lasso</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="907" to="927" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep feature selection: theory and application to identify enhancers and promoters</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Biology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="322" to="336" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="31" to="57" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Allen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2502.06661</idno>
		<title level="m">iloco: Distribution-free inference for feature interactions</title>
				<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Udell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09903</idno>
		<title level="m">Impact of accuracy on model interpretations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Stability approach to regularization selection (stars) for high dimensional graphical models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Bayesian consensus clustering</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Lock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="2610" to="2616" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Noble</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01185</idno>
		<title level="m">Deeppink: reproducible feature selection in deep neural networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07874</idno>
		<title level="m">A unified approach to interpreting model predictions</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
				<meeting>the fifth Berkeley symposium on mathematical statistics and probability<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Species boundaries and phylogeographic patterns in cryptic taxa inferred from aflp markers: Veronica subgen. pentasepalae (scrophulariaceae) in the western mediterranean</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Martínez-Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Albach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Elena-Rosselló</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systematic Botany</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="965" to="986" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Comparing clusterings-an information based distance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meilă</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of multivariate analysis</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="873" to="895" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Stability selection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B: Statistical Methodology</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="417" to="473" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Quantifying uncertainty in random forests via confidence intervals and hypothesis tests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mentch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hooker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="841" to="881" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A survey of clustering with deep learning: From the perspective of network architecture</title>
		<author>
			<persName><forename type="first">E</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="39501" to="39514" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Molnar</surname></persName>
		</author>
		<title level="m">Interpretable Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Layer-wise relevance propagation: an overview. Explainable AI: interpreting, explaining and visualizing deep learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="193" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Climate informatics: accelerating discovering in climate science with machine learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Monteleoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcquade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="32" to="40" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Consensus clustering: a resampling-based method for class discovery and visualization of gene expression microarray data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tamayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mesirov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Golub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="91" to="118" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Generalized stability approach for regularized graphical models</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kurtz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07072</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Definitions, methods, and applications in interpretable machine learning</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Murdoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumbier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Abbasi-Asl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Proceedings of the National Academy of Sciences</publisher>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="22071" to="22080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Murdoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumbier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Abbasi-Asl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04592</idno>
		<title level="m">Interpretable machine learning: definitions, methods, and applications</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A survey of recent advances in hierarchical clustering algorithms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Murtagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The computer journal</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="354" to="359" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Ten quick tips for effective dimensionality reduction</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">e1006907</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.04765</idno>
		<title level="m">Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Strong consistency of k-means clustering. The Annals of Statistics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pollard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="135" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">A novel machine learning model for estimation of sale prices of real estate units</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Rafiei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Construction Engineering and Management</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4015066</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Objective criteria for the evaluation of clustering methods</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Rand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical association</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">336</biblScope>
			<biblScope unit="page" from="846" to="850" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">The probabilistic basis of jaccard&apos;s index of similarity</title>
		<author>
			<persName><forename type="first">R</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Vargas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systematic biology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="380" to="385" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Gaussian mixture models. Encyclopedia of biometrics</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">741</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">why should i trust you?&quot; explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Anchors: High-precision model-agnostic explanations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">A survey of clustering algorithms. Data mining and knowledge discovery handbook</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="269" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>La Cava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Gregg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Himmelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Moore</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00058v2</idno>
		<title level="m">Pmlb v1.0: an open source dataset collection for benchmarking machine learning methods</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Explainable machine learning for scientific insights and discoveries</title>
		<author>
			<persName><forename type="first">R</forename><surname>Roscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="42200" to="42216" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">V-measure: A conditional entropy-based external cluster evaluation measure</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL)</title>
				<meeting>the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="410" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Building trust in artificial intelligence</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of international affairs</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="134" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Interpretable machine learning: Fundamental principles and 10 grand challenges</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Semenova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics Surveys</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="85" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">What do asian religions have in common? an unsupervised text analytics exploration</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fokoué</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10847</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Web-scale k-means clustering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
				<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1177" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Grad-cam: Why did you say that?</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">A weighted kendall&apos;s tau statistic</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Shieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics &amp; probability letters</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="24" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Learning important features through propagating activation differences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kundaje</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3145" to="3153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Building trust in artificial intelligence, machine learning, and robotics</title>
		<author>
			<persName><forename type="first">K</forename><surname>Siau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cutter business technology journal</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="47" to="53" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Conditional variable importance for random forests</title>
		<author>
			<persName><forename type="first">C</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Boulesteix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kneib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Augustin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeileis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Bias in random forest variable importance measures: Illustrations, sources and a solution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Boulesteix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeileis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hothorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3319" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Exact post-selection inference for sequential regression procedures</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">514</biblScope>
			<biblScope unit="page" from="600" to="620" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Logistic regression: relating patient characteristics to outcomes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Meurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jama</title>
		<imprint>
			<biblScope unit="volume">316</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="533" to="534" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Interpretable to whom? a rolebased model for analyzing interpretable machine learning systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tomsett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Braines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Preece</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07552</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">The relationship between trust in ai and trustworthy machine learning technologies</title>
		<author>
			<persName><forename type="first">E</forename><surname>Toreini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Coopamootoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Zelaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Moorsel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 conference on fairness, accountability, and transparency</title>
				<meeting>the 2020 conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="272" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">A variational approach to the consistency of spectral clustering</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Trillos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Slepčev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="281" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Increasing trust and fairness in machine learning applications within the mortgage industry</title>
		<author>
			<persName><forename type="first">W</forename><surname>Van Zetten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramackers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning with Applications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">100406</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">Counterfactual explanations and algorithmic recourses for machine learning: A review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Boonsanong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Hines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10596</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Vilone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Longo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00093</idno>
		<title level="m">Explainable artificial intelligence: a systematic review</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">U</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Consistency of spectral clustering. The Annals of Statistics</title>
		<author>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="555" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Mathematical statistics with applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wackerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mendenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Scheaffer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Cengage Learning</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">The explanation game: a formal framework for interpretable machine learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Floridi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ethics, Governance, and Policies in Artificial Intelligence</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="185" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">A similarity measure for indefinite rankings</title>
		<author>
			<persName><forename type="first">W</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">The cancer genome atlas pan-cancer analysis project</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Collisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R M</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Ozenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ellrott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shmulevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Stuart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature genetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1113" to="1120" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">A general framework for inference on algorithm-agnostic variable importance</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>justaccepted</note>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Comparing the performance of biomedical clustering methods</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wiwie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baumbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Röttger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1033" to="1038" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">K-means-based consensus clustering: A unified view</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="169" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">A review on consensus clustering methods. Optimization in Science and Engineering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xanthopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Honor of the 60th Birthday of Panos M. Pardalos</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="553" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Statistical method scdeed for detecting dubious 2d single-cell embeddings and optimizing t-sne and umap hyperparameters</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1753</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Survey of clustering algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wunsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="645" to="678" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">On splitting training and validation set: a comparative study of crossvalidation, bootstrap and systematic sampling for estimating the generalization performance of supervised learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goodacre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of analysis and testing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="262" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">A new rank correlation coefficient for information retrieval</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval</title>
				<meeting>the 31st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="587" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Veridical data science</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on web search and data mining</title>
				<meeting>the 13th international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">A review of machine learning and iot in smart transportation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zantalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Koulouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karabetsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kandris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Internet</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">94</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Birch: an efficient data clustering method for very large databases</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Livny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM sigmod record</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="103" to="114" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title level="m" type="main">why should you trust my explanation?&quot; understanding uncertainty in lime explanations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Udell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12991</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Limeade: Local interpretable manifold explanations for dimension evaluations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Zikry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2025 Workshop on Machine Learning for Genomics Explorations</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Cell cycle plasticity underlies fractional resistance to palbociclib in er+/her2-breast tumor cells</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Zikry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Wolff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Ranek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Naugle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Luthra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Whitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Kedziora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Stallaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Kosorok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">e2309261121</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Deepred-rule extraction from deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Zilke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Mencía</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Janssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Discovery Science</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="457" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Consistency clustering: a robust algorithm for group-wise registration, segmentation and automatic atlas construction in diffusion mri</title>
		<author>
			<persName><forename type="first">U</forename><surname>Ziyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E L</forename><surname>Grimson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-F</forename><surname>Westin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="290" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
