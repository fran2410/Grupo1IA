<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">REAL-TIME SPATIAL RETRIEVAL AUGMENTED GENERATION FOR URBAN ENVIRONMENTS A PREPRINT</title>
				<funder ref="#_zeJgCFs">
					<orgName type="full">European Commission</orgName>
				</funder>
				<funder ref="#_6s68suQ">
					<orgName type="full">Spanish Agencia Estatal de Investigación</orgName>
				</funder>
				<funder ref="#_mhgMrZc">
					<orgName type="full">CM</orgName>
				</funder>
				<funder ref="#_vZq6FRq #_ZP3GF4D #_tB7hPNn">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2025-04-30">April 30, 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">Nazareno</forename><surname>Campo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ETSI de Telecomunicación</orgName>
								<orgName type="institution" key="instit2">Universidad Politécnica de Madrid</orgName>
								<address>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">FIWARE Foundation</orgName>
								<address>
									<postCode>10587</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Javier</forename><surname>Conde</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ETSI de Telecomunicación</orgName>
								<orgName type="institution" key="instit2">Universidad Politécnica de Madrid</orgName>
								<address>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Álvaro</forename><surname>Alonso</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ETSI de Telecomunicación</orgName>
								<orgName type="institution" key="instit2">Universidad Politécnica de Madrid</orgName>
								<address>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gabriel</forename><surname>Huecas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ETSI de Telecomunicación</orgName>
								<orgName type="institution" key="instit2">Universidad Politécnica de Madrid</orgName>
								<address>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joaquín</forename><surname>Salvachúa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ETSI de Telecomunicación</orgName>
								<orgName type="institution" key="instit2">Universidad Politécnica de Madrid</orgName>
								<address>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pedro</forename><surname>Reviriego</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ETSI de Telecomunicación</orgName>
								<orgName type="institution" key="instit2">Universidad Politécnica de Madrid</orgName>
								<address>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">REAL-TIME SPATIAL RETRIEVAL AUGMENTED GENERATION FOR URBAN ENVIRONMENTS A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-04-30">April 30, 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">268B14883B04AEEC191DA6A19A4B182D</idno>
					<idno type="arXiv">arXiv:2505.02271v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-19T11:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The proliferation of Generative Artificial Ingelligence (AI), especially Large Language Models, presents transformative opportunities for urban applications through Urban Foundation Models. However, base models face limitations, as they only contain the knowledge available at the time of training, and updating them is both time-consuming and costly. Retrieval Augmented Generation (RAG) has emerged in the literature as the preferred approach for injecting contextual information into Foundation Models. It prevails over techniques such as fine-tuning, which are less effective in dynamic, real-time scenarios like those found in urban environments. However, traditional RAG architectures, based on semantic databases, knowledge graphs, structured data, or AI-powered web searches, do not fully meet the demands of urban contexts. Urban environments are complex systems characterized by large volumes of interconnected data, frequent updates, real-time processing requirements, security needs, and strong links to the physical world. This work proposes a realtime spatial RAG architecture that defines the necessary components for the effective integration of generative AI into cities, leveraging temporal and spatial filtering capabilities through linked data. The proposed architecture is implemented using FIWARE, an ecosystem of software components to develop smart city solutions and digital twins. The design and implementation are demonstrated through the use case of a tourism assistant in the city of Madrid. The use case serves to validate the correct integration of Foundation Models through the proposed RAG architecture. It also enables the analysis of current model limitations, such as their inability to handle large volumes of information, even when it fits within their context window, and the high latency of Large Language Models caused by transformer-based architectures, which generate output token by token.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The rise of Generative Artificial Intelligence (AI) in general, and foundation models specifically, has enabled the proliferation of new tools in which Large Language Models (LLMs) become just one component within the software architecture. LLMs are neural networks composed of billions of parameters that have been trained on massive datasets with the task of predicting the next token. The transformer-based architecture, along with Big Data capabilities, has significantly increased LLM performance in recent years <ref type="bibr">Naveed et al. [2024]</ref>.</p><p>A limitation of foundation models is that their knowledge is restricted to the dataset on which they were trained and the specific task for which they were designed. Additionally, training a model is extremely costly, both computationally and economically, making it unfeasible to train a new model from scratch just to expand its knowledge base. Alternatives like fine-tuning allow us to modify the behavior of the LLMs. Fine-tuning is less expensive than training a model from scratch, and it works well to adapt the behavior of the model by providing a lot of examples of the desired task, but it is less effective in injecting new knowledge <ref type="bibr" target="#b1">Ovadia et al. [2024]</ref>. In addition, it would be necessary to retrain the LLM every time new information needs to be added, making it incompatible with dynamic systems such as cities. Another alternative is Retrieval Augmented Generation (RAG) <ref type="bibr" target="#b2">Lewis et al. [2020,?]</ref>. RAG is a technique applied to LLMs to provide and restrict the LLM to new information not contained in the training dataset <ref type="bibr" target="#b1">Ovadia et al. [2024]</ref>. RAG is one of the most widely used techniques for integrating LLMs in specific use cases where contextual information, unknown to the base model, is required <ref type="bibr" target="#b3">Fan et al. [2024]</ref>. Unlike fine-tuning, which modifies the model's parameters through a light retraining, RAG allows external information to be injected without altering the base model. This makes RAG more suitable for dynamic systems with frequent information updates.</p><p>One of the environments where base foundation models are not sufficient is cities. Urban environments are characterized by being complex ecosystems with multiple data sources that are updated in real time, which were not in the training dataset of the model and with many interconnected systems. The digitalization of cities through the development of smart cities has been one of the main goals of governments worldwide in recent years <ref type="bibr" target="#b4">Kirimtat et al. [2020]</ref>, with efforts focused on the creation of urban digital twins (DTs) <ref type="bibr" target="#b5">Weil et al. [2023]</ref>. These urban DTs collect real-time data from the physical city through Internet of Things (IoT) sensors, process them within the virtual entity, and, based on the results, modify the state of the city through IoT actuators. Traditionally, DTs have processed information mainly through physical models or machine learning models aimed at predicting the future state of city elements such as traffic congestion Talkhestani et al. <ref type="bibr">[2019]</ref>. However, these systems have been limited to handling structured data, excluding natural language. The proliferation of Generative AI paves the way for the integration of components into urban environments that are capable of processing natural language. This enables the implementation of more advanced and complex actions designed to improve the lives of citizens, moving toward what is known as Urban General Intelligence <ref type="bibr" target="#b7">Zhang et al. [2025]</ref>.</p><p>Cities are complex systems that require the integration of multiple data sources, the ability to process large volumes of data in a distributed manner, ensuring information security, interconnecting the physical world through IoT devices, modeling of physical information, processing data in real-time, and the executing complex models <ref type="bibr" target="#b4">Kirimtat et al. [2020]</ref>, <ref type="bibr" target="#b8">Haque et al. [2022]</ref>, Javed et al. <ref type="bibr">[2022]</ref>. The need to adapt foundation models to urban environments has led to the emergence of Urban Foundation Models (UFMs), i.e., foundation models that are pre-trained on urban data to be applied in urban applications <ref type="bibr" target="#b10">Zhang et al. [2024]</ref>. These models are designed with capabilities to process natural language, time series, and multimodal information, as well as to perform vision tasks, management, and prediction of human and vehicle trajectories.</p><p>The adoption of UFMs requires RAG capabilities in order to process real-time information from cities, with the need of designing new architectures to support all the previously mentioned requirements (connection to the physical world, scalability, security, and real-time processing). In this work, we explore current RAG system solutions for urban environments, propose an architecture that meets the specific requirements of urban environments, and extend a pre-existing digital twin architecture for cities with foundation models. Our proposal is based on interoperable, extensible, open source, and scalable components, enabling the integration of an LLM capable of processing city information. In addition, we include a use case based on the city of Madrid to evaluate the feasibility of the proposed architecture.</p><p>The manuscript is structured as follows. In the next section, we discuss existing RAG solutions, their application to urban areas, and we present the FIWARE technology that we will use to implement the urban RAG architecture. In Section 3, we propose the requirements a RAG system must meet to be integrated into cities, as well as a reference architecture to develop urban RAGs. Next, we implement the proposed architecture that uses the component-based technology of FIWARE for cities. In Section 5, we validate our proposal through a use case in the city of Madrid. Finally, in Section 6, we present the conclusions and limitations of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Retrieval Augmented Generation</head><p>RAGs are one of the best options for injecting new knowledge into foundation models without the need to retrain it <ref type="bibr" target="#b1">Ovadia et al. [2024]</ref>. The drawback of RAGs is that the new information is not persistent in the model, since it does not modify any of its parameters. Therefore, the maximum amount of information that can be injected is limited by the context window of the model <ref type="bibr" target="#b11">Dong et al. [2024]</ref>. Additionally, it is necessary to provide the information in each iteration with the model since each prediction starts from the base model. This apparent disadvantage can actually be an advantage for real-time systems, where the value of properties changes over time. If the information was persistent in the LLM, it would need to be able to discern which version of the data is the most recent.</p><p>RAGs are used for different purposes, such as answering questions, summarizing texts, text analysis or decision making <ref type="bibr" target="#b12">Arslan et al. [2024]</ref>. If an LLM has been trained up to a certain date, it does not know anything that has happened after that date. For example, LLMs cannot correctly respond to what the weather is like in Paris right now. Another use case of RAGs is to narrow down the context information to the UFM and limit the model's responses to the desired data. For example, with RAG techniques, you can instruct the model on the specifications of a standard or even tell it to respond that the capital of Italy is Naples. Research has shown how RAG helps reduce possible hallucinations by grounding the model response in retrieved facts, as long as the provided information is accurate <ref type="bibr" target="#b13">Perković et al. [2024]</ref>.</p><p>The processing of a naive RAG is divided into different phases <ref type="bibr">Zhao et al. [2024a]</ref>. First, there is the indexing phase, where the data are organized into chunks and indexed in an external data source, typically a vector database. This phase ensures that data are stored in small pieces of data, which allows fast search and retrieval based on the user's query. It is important to design the chunking strategy controlling the way fragments are divided and the size of each chunk <ref type="bibr" target="#b15">Sarthi et al. [2024]</ref>. Once the chunks are generated, an embedding is created for each one. At this stage, it is important to choose an embedding model that fits the RAG, observing that higher-dimensional embeddings provide greater search granularity but also slow down retrieval. When a user submits a prompt, it is compared with the stored embeddings. As a first step, the user's prompt can be preprocessed and transformed to improve the search. Next, in the retrieval phase, relevant data is obtained from the indexed source based on semantic similarity metrics. The system performs a search query against the indexed database to retrieve content that matches the user's prompt or context. This phase is crucial to narrow down the most relevant information. There are different retrieval techniques divided into two types of search. In flat searches, all system embeddings are compared. Flat methods offer higher accuracy by exploring all vectors, but are not scalable solutions <ref type="bibr" target="#b16">Wang et al. [2024]</ref>. On the other hand, Approximate Nearest Neighbors (ANN) methods, such as Locality Sensitive Hashing (LSH), Hierarchical Navigable Small World (HNSW), and Inverted File Index (IVF), speed up retrieval by avoiding the need to compare all possible embeddings. Once the data has been received, a reordering of the top-k most relevant documents can be carried out using reranking techniques <ref type="bibr" target="#b17">Nogueira and Cho [2020]</ref>. Different data sources can also be merged and the retrieval information can be improved by including neighboring chunks or summaries.</p><p>After the retrieval it starts the content generation phase where the user's prompt and the content extracted during the retrieval and augmentation phase are passed to the LLM. The LLM is then instructed to respond to the user's prompt while limiting itself to the retrieved content. The model generates a response that integrates the information retrieved from the database. Finally, the response generated by the LLM can be refined or corrected. Post-processing can involve tasks such as improving readability, ensuring factual accuracy, or adjusting the tone of the response. The implementation of a RAG is agnostic to the LLM, as the retrieval phase occurs prior to the interaction with the LLM, and the interaction with the LLM is based on a modified prompt that includes the retrieved information and specific instructions for the LLM. For that reason the quality of an RAG depends on both the retrieval architecture but also on the LLM used to generate the response.</p><p>Different techniques have also been studied to improve RAGs, taking advantage of advances in embedding models or exploring new paradigms such as token-level embedding, which generates an embedding vector for each token instead of each chunk <ref type="bibr" target="#b18">Khattab and Zaharia [2020]</ref>. However, this type of RAG is not effective with multihop questions <ref type="bibr" target="#b19">Tang and Yang [2024]</ref> in which the solution requires exploring relationships between entities, something that is frequent in urban environments <ref type="bibr" target="#b10">Zhang et al. [2024]</ref>.</p><p>To solve this, the researchers propose to model the information as a knowledge graph and take advantage of graph structures to improve the retrieval of information, locate other relevant documents, and generate follow-up questions Pan et al. <ref type="bibr">[2024]</ref>. Solutions such as GraphRAG <ref type="bibr" target="#b21">Edge et al. [2025]</ref> use natural language processing to transform input documents into a graph that contains entities, relationships, and covariates (claims about the entities). To respond to the questions, the information from the embeddings of the chunks, entities, covariates, and relationships is combined with embeddings that represent the structure of the graph itself through graph vector representation algorithms such as Node2Vec <ref type="bibr" target="#b22">Grover and Leskovec [2016]</ref>. Graph-based RAGs can apply the same techniques as naive RAGs in the indexing, retrieval, augmentation, and generation phases. These RAGs perform very well for questions involving specific entities (e.g., "Is Eiffel Tower closed to traffic?"). However, they do not perform well for more general questions (e.g., "How many streets are currently closed to traffic?"). For global questions, researchers propose generating different levels of summarization that allow global and parallel searches by applying the map-reduce pattern <ref type="bibr" target="#b21">Edge et al. [2025]</ref>. RAGs based on knowledge graphs are better suited to urban environments because they are capable of processing relationships. However, real-time information updates can limit their performance, especially with the appearance of new elements that modify the graph structure.</p><p>Within RAGs, retrieval is not limited to similarity searches in semantic databases. Solutions that operate on structured data can also be used <ref type="bibr">Zhao et al. [2024b]</ref>. In this case, an LLM transforms the user prompt in natural language into a query in a specific syntax, for example, some models have been used to generate SQL queries <ref type="bibr">Vichev and Marchev [2024]</ref>. In this case, the complexity lies not in the search phase, but in the LLM's capabilities to generate SQL queries or extract search parameters from the user's prompt <ref type="bibr" target="#b24">Li et al. [2023]</ref>. A key component in these types of solution is the connection with external systems. Proposals such as the Model Context Protocol<ref type="foot" target="#foot_0">1</ref> facilitate the integration. This approach can be combined with semantic searches, resulting in hybrid systems based on structured and unstructured data sources.</p><p>Another popular RAG system is the AI Web searcher or deep research where the retrieval of information comes from Internet searches <ref type="bibr" target="#b25">Xiong et al. [2024]</ref>. This type of RAG allows us to answer dynamic questions such as the current weather in Paris; however, they are limited to public data on the Internet.</p><p>In addition to the core processes of retrieval and generation, several factors are crucial in optimizing the performance of RAG systems. Effective prompt engineering is essential to guide the LLM in generating accurate and contextually relevant responses <ref type="bibr" target="#b26">Marvin et al. [2024]</ref>. Balancing retrieval accuracy with computational efficiency is another critical consideration, as more complex retrieval strategies can increase latency. While RAG systems provide a flexible and dynamic way to augment the knowledge of LLMs without retraining, they also require careful management of the knowledge base to ensure the reliability and consistency of retrieved information. By addressing these considerations, RAG systems can be effectively utilized to provide real-time, contextually enriched responses in various applications, including urban environments powered by platforms like FIWARE.</p><p>Several interesting works have been done with LLM and RAG in the urban domain. Open-TI introduces a system leveraging augmented language models for open traffic intelligence <ref type="bibr" target="#b27">Da et al. [2024]</ref>. The authors demonstrate the application of advanced language models, potentially using RAG principles, in analyzing traffic data and providing insights for urban transportation management. In <ref type="bibr" target="#b28">Ieva et al. [2024]</ref>, authors introduce a digital twin framework for smart-grid energy infrastructure that leverages a RAG pipeline, combining machine learning, a knowledge graph, and an LLM-based conversational assistant; to provide enhanced decision support for asset management and predictive maintenance. Other works <ref type="bibr" target="#b29">Fu et al. [2023]</ref> explore how pre-trained LLMs can facilitate collaborative research between humans and AI in urban science. This work investigates the potential for LLMs to enhance analytical capabilities and address complex urban problems through synergistic interaction between human expertise and AI's processing power. The study in <ref type="bibr" target="#b30">Ji and Gao [2023]</ref> is the first to systematically evaluate foundation LLMs, including GPT-2 and BERT, for encoding geometries in Well-Known Text (WKT) format and preserving spatial relations, demonstrating that while their embeddings can distinguish geometry types and capture spatial relations with up to 73% accuracy, they still struggle with numeric value estimation and retrieval of spatially related objects, thereby highlighting the urgent need to integrate geospatial domain knowledge to advance GeoAI applications. The work in <ref type="bibr" target="#b31">Mei et al. [2023]</ref> is the first to leverage pre-trained models to enhance the candidate-generation phase of Point-of-Interest search in urban environments, substantially improving both retrieval efficiency and the contextual relevance of user recommendations.</p><p>Table <ref type="table" target="#tab_0">1</ref> summarizes the characteristics that a RAG must meet for urban environments, as well as the most widely used types of RAGs, indicating whether they meet the requirements. The last column shows real-time spatial RAG, our proposed solution for integrating urban foundation models into urban environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">FIWARE</head><p>FIWARE is an open source framework that can be assembled together with other third-party components to facilitate the development of smart solutions faster and more efficiently. This includes smart solutions in various domains, such as urban environments. According to the latest information, the platform is in more than 400 cities in at least 35 countries FIWARE Foundation <ref type="bibr">[2023]</ref> and counts more than 630 members in 64 countries, including large corporate, medium and small companies, as well as an ecosystem of innovation centers which are typically run by innovation hubs, RTOs, and universities<ref type="foot" target="#foot_1">2</ref> .</p><p>FIWARE components have served as a foundation for researchers to develop solutions and define architectures in fields such as agriculture <ref type="bibr" target="#b33">Kolehmainen et al. [2023]</ref>, industry <ref type="bibr">Carvajal-Flores et al. [2024]</ref>, dataspaces <ref type="bibr" target="#b35">Segou et al. [2024]</ref>, The central element of any FIWARE architecture is the context information, which consists of a set of entities with properties and relationships between them that allows modeling any use case. The core of the FIWARE platform lies in providing tools to manage this context information and can be divided into three fundamental components: 1) the Next Generation Service Interface for Linked Data (NGSI-LD) Standard European Telecommunications Standards Institute [2020] that defines the format of the entities as linked data and provides an API specification to manage them effectively. 2) A suite of components and APIs that enable the integration and management of data from diverse sources through the NGSI-LD standard, facilitating the development of intelligent applications. 3) A collection of ontologies, named Smart Data Models<ref type="foot" target="#foot_2">3</ref> , that allow model entities and ensure interoperability of data between systems.</p><p>FIWARE components are divided into four layers: a) components responsible for context information management, such as the OrionLD context broker<ref type="foot" target="#foot_3">4</ref> . This component implements the NGSI-LD standard and provides the ability to access and modify NGSI-LD entities, also allowing asynchronous access to information through a publish-subscribe pattern. c) Components that act as gateways to physical systems or external systems, such as IoT Agents<ref type="foot" target="#foot_4">5</ref> , which enable the connection to IoT sensors and actuators, or Draco<ref type="foot" target="#foot_5">6</ref> , which specializes in connecting to other data sources such as APIs or databases and acts as Extract Transform Load (ETL) system from any protocol and format to NGSI-LD. c) Processing systems, such as Cosmos<ref type="foot" target="#foot_6">7</ref> , for big data processing and machine learning. d) Auxiliary systems, such as Keyrock<ref type="foot" target="#foot_7">8</ref> for security. Figure <ref type="figure" target="#fig_0">1</ref> shows the components of FIWARE divided into layers, and the official catalog<ref type="foot" target="#foot_8">9</ref> collects is a list of all of them.</p><p>Numerous studies have validated FIWARE components for the development of applications in urban environments demonstrating that they meet requirements such as real-time notifications, interoperability between systems, scalability, information security, integration with external systems, and By combining FIWARE's data management capabilities with the language understanding and generation abilities of LLMs, we can develop intelligent agents, personalized recommendation systems, and content generation tools for urban environments. Moreover, if this can be achieved in real-time and tailored to user preferences, it opens up a new kind of experience for the end user. Our research aims to explore the potential of integrating FIWARE with UFMs for applications in the urban sector. We will extend the FIWARE architecture for digital twins <ref type="bibr">Conde et al. [2022b]</ref>, analyze the data flow required for such integration, and evaluate our proposal through a practical use case.</p><p>3 Real-time Spatial RAG for urban environments</p><p>The application of generative AI in urban environments is an idea already explored in the literature Zhang et al.</p><p>[2024], <ref type="bibr" target="#b27">Da et al. [2024]</ref>, <ref type="bibr" target="#b28">Ieva et al. [2024]</ref>, <ref type="bibr" target="#b29">Fu et al. [2023]</ref>, <ref type="bibr" target="#b30">Ji and Gao [2023]</ref>, <ref type="bibr" target="#b31">Mei et al. [2023]</ref>, which allows adding new functionalities to the city management through queries using natural language, information processing, etc. However, one of the requirements of urban environments is the ability to process data in real-time (or with soft real-time requirements). The concept of real-time in smart cities is broad and encompasses a wide range of scenarios. In systems such as autonomous driving, we deal with hard real-time requirements, where data updates and processing must occur within milliseconds and any delay may cause irreparable damage <ref type="bibr" target="#b44">Lin et al. [2018]</ref>. In contrast, other systems are less demanding, allowing soft real-time scenarios where information updates can occur in a few seconds <ref type="bibr" target="#b45">Barbieru and Pop [2016]</ref>. Current architectures based on transformers, which generate information token by token, are not capable of meeting hard real-time requirements so the use of LLMs in urban environments is limited to soft real-time applications <ref type="bibr" target="#b46">Lin et al. [2024]</ref>. Additionally, in urban environments, there are a large number of elements, so it is important to have the ability to perform preliminary filtering both in time and space to avoid introducing excessive latency in the retrieval and generation phases <ref type="bibr" target="#b10">Zhang et al. [2024]</ref>. Urban environments are characterized by the existence of multiple relationships between their elements. For example, a malfunction of a traffic light can affect the operation of other nearby traffic lights. Therefore, the RAG system must have navigation and self-discovery capabilities among urban entities. One way to achieve this is by representing information as a knowledge graph, which allows augmenting the LLM context by including new entities <ref type="bibr" target="#b21">Edge et al. [2025]</ref>.</p><p>In this work, we propose a real-time spatial RAG architecture that satisfies the requirements for urban areas. The spatial dimension refers to the use of the geospatial position as a search criterion. This approach leverages research on clustering techniques, which can keep the number of queries and response sizes controlled. The temporal dimension refers to querying data on databases capable of recording information in real-time. The relationship dimension refers to the ability to retrieve related elements, which can be addressed by modeling the information as a graph. In addition to these three basic features, the RAG architecture must satisfy the other requirements of urban environments:</p><p>• Connection to the real world. IoT devices enable the connection between the virtual world and the real world.</p><p>IoT sensors keep the virtual world up to date, while IoT actuators allow actions to be performed in the real world.</p><p>• Interoperability. IoT devices are not the only source of information in urban environments. Other systems can act as data sources, such as databases or APIs (Application Programming Interfaces). The different data sources must follow a common data format to facilitate interoperability. Ideally, these data formats should follow a standard to extend interoperability to external systems.</p><p>• Generation of historical records. Cities generate large amounts of data, so there must be mechanisms for storing historical information for future analysis or to train Machine Learning models.</p><p>• Scalability. The architectures must be scalable to be able to process hundreds of thousands of data points in real-time.</p><p>• Security. Urban environments contain sensitive information, making it necessary to protect it through authentication and authorization mechanisms.</p><p>Figure <ref type="figure">2</ref> represents the simplified architecture of a real-time spatial RAG. The information flow starts when an IoT device provides a measurement or an external data source updates the context information (update phase). Data extraction can follow an asynchronous communication flow (1-async), where the data are stored in the database whenever a change occurs at the source. This would allow the system to operate in real-time, which can be implemented through permanent sockets or pattern/subscription architectures. In the case of synchronous communication (1-sync), periodic queries are performed are performed to update the information. In this second approach, the information will not be recorded in real-time. Then a system with ETL capabilities transforms the information into the corresponding data model and creates all the relationships with other entities (2). Finally, in the load phase (3), the linked data are stored in a spatial database that will be accessible by the RAG system.</p><p>In parallel, the RAG system will connect to the data source, and it will be able to process and respond to external prompts with updated information. In this case, the flow begins at (a) when a user makes a request (req) about a region (R). Next, the request is made to the spatial database for the region (R) and any other additional filters specified by the user (b). This interaction with the spatial database can be defined in multiple ways. The number of entities retrieved can Figure <ref type="figure">2</ref>: Simplified real-time spatial RAG architecture be limited and ordered according to defined criteria, linked elements can also be recovered, and clustering techniques can be applied to enhance the scalability of the system. The response from the spatial database (c, resp-spatial) is combined with the initial user prompt (req + resp-spatial) and sent to an UFM (d). The LLM will be configured with a prompt like "answer the following question: (req) limited to the data (resp-spatial)" resulting in the UFM response (resp-llm) adjusted to the data extracted from the spatial database (e). Finally, the final response (resp) is composed and sent to the user (f). When a user makes a query again, a new interaction with the RAG will begin (a-f). Since flows (1-3) and (a-f) run in parallel, the system is able to react to changes in real-time. The capabilities of spatial databases allow the response size to be limited to the established region, the temporal filtering allows filtering data by date, and the linked data helps to improve the retrieval by including related entities.</p><p>The real-time spatial RAG architecture meets all the requirements for the effective integration of UFM in urban environments. Table <ref type="table" target="#tab_0">1</ref> presents a comparison with other popular RAG systems in the literature. This is a simplified architecture, since it does not take into account security aspects such as user and service authentication and authorization, or input and output safeguards to prevent the system from disclosing sensitive information or deviating from its intended purpose. The strategy for defining the prompt has also not been discussed as it will depend on each use case. However, it is a relevant factor that affects the quality of the RAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation of Real-Time Spatial RAG with FIWARE</head><p>Recently, the integration of LLMs with different tools like LlamaIndex<ref type="foot" target="#foot_9">10</ref> , Langchain<ref type="foot" target="#foot_10">11</ref> , GraphRag<ref type="foot" target="#foot_11">12</ref> has gained significant attention for the development of RAGs. However these tools are focused on RAGs for retrieving information from documents or graphs and do not meet all the requirements for urban environments. In this work, we propose an architecture based on FIWARE to bridge this gap.</p><p>Figure <ref type="figure" target="#fig_1">3</ref> illustrates a general architecture that integrates FIWARE technology with UFMs to provide a real-time spatial RAG. This architecture aims to deliver contextually enriched real-time information from entities in a city to be processed by UFMs. The centerpiece of architecture lies the context broker, which serves as the component responsible for managing and storing contextual information. The context broker utilizes a database system for persistent storage and can link to other data repositories, including binary data such as images. Data can be populated into the system through various methods. Users can inject raw entities directly into the context broker using NGSI-LD POST queries, such as storing Points of Interest (PoI) entities of a city. In addition, relevant contextual information can be incorporated from third-party systems. FIWARE Draco normalizes and transforms the data from these systems into NGSI-LD format before sending it to the context broker and making them compliant with the ontologies defined by the Smart Data Models. IoT agents play a role in receiving data from various IoT sensors and devices, converting it into NGSI-LD format and updating the information in the context broker. External data sources, such as weather updates or social media feeds, can also be registered to provide real-time information, enhancing the contextual data model. As illustrated in Figure <ref type="figure" target="#fig_1">3</ref>, a user can interact with the system through an application. This application displays contextual information and reflects changes resulting from user interactions or updates from other systems. When a user submits a question about the city, the RAG application retrieves relevant entities from the context broker using the NGSI-LD API, filtering by entity properties, date information, spatial location, ensuring that the response is geographically, temporally, and contextually relevant, and optionally retrieving additional entities that are linked through NGSI-LD relationships.</p><p>The application then combines the retrieved contextual data with the user's prompt and sends this enriched prompt to an LLM. The LLM processes the prompt, leveraging its knowledge base and the provided context to generate an informative and contextually appropriate response. This response is returned to the application, which presents it to the user in a user-friendly format. Thanks to the subscription system of the context broker, the RAG can be configured to receive real-time updates of modified entities, eliminating the need for periodic requests to check whether the contextual information provided to the LLM has changed, something common in urban environments.</p><p>In the initial experiments that we will present later, we observed that LLMs are capable of understanding the NGSI-LD format, which simplifies the post-processing of the retrieved entities. In cases where the LLM is unable to interpret the properties of an NGSI-LD entity, it can be programmed to consult the ontology, as NGSI-LD entities include a "context" attribute that links to the Smart Data Model used. This model provides a natural language description of the entity and explains the meaning of each of its properties and relationships. Moreover, the architecture can be extended by incorporating additional FIWARE components to enhance functionality and user experience further. Cygnus<ref type="foot" target="#foot_12">13</ref> can be integrated to store historical data, enabling the system to track changes and trends over time. Including Identity and Access Management (IAM) components, such as Keyrock<ref type="foot" target="#foot_13">14</ref> , would add security to the data by implementing user identification and authentication.</p><p>5 Use case: Real-time spatial RAG for tourism in Madrid</p><p>In this section, we present an implementation example in Madrid that will allow us to validate the proposed architecture.</p><p>To do so, we will evaluate the system's performance in different scenarios, considering two dimensions: response speed and response correctness. All code, data sets, and results are available in a public repository<ref type="foot" target="#foot_14">15</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Definition of the use case</head><p>The use case consists of developing a real-time tourist assistant for the city of Madrid. To this end, 1,088 PoIs have been loaded, including both static information (e.g., name and description) and dynamic information (e.g., visitor affluence and price). This use case allows us to validate the architecture, as it enables the delivery of real-time, contextually enriched information about the city's points of interest, ensuring that tourists receive up-to-date and relevant insights.</p><p>We have implemented a similar but simplified architecture as depicted in Figure <ref type="figure" target="#fig_1">3</ref>. The system employs the FIWARE OrionLD context broker to manage NGSI-LD data related to POIs with their detailed description, GPS coordinates, visiting hours, entry fees, relevance of the PoI, and current visitor affluence. This data management ensures that all information is up-to-date and accurately reflects real-time conditions. An example of a POI NGSI-LD entity can be consulted in Figure <ref type="figure" target="#fig_1">3</ref>.</p><p>The system includes an interactive map that allows users to explore specific areas of the city. The map dynamically updates the bounding box coordinates, which are used to filter relevant POIs from the context broker. This interactive element helps users focus on areas of interest and obtain detailed information about these locations while limiting the number of PoIs retrieved. Users submit questions through a text box that functions as a chat interface. These queries can be specific questions about a particular monument or broader inquiries about nearby attractions and general travel tips. The user's question, along with the retrieved PoI data from the context broker are sent to the LLM, which acts as an expert tourist guide. We have used GPT-4.1 (gpt-4.1-2025-04-14) as LLM, as it is one of the most powerful models currently available and offers a large context window, which facilitates the processing of large amounts of entities.</p><p>The LLM synthesizes the provided data with its internal knowledge base to generate a comprehensive and informative response, including detailed information about the queried PoIs, practical visiting tips, historical context, and other relevant insights. The model temperature has been set to 0 to ensure that the experiment is reproducible. The following prompt structure has been used:</p><p>• System prompt: "You are a tourist guide in the city from where the data is provided. Also you are expert in NGSI and semantics. You should only answer about the following points of interests that I will provide you in NGSI format. At first, you should only provide the name of the places with not extra detail, unless requested in the prompt message by the user. If you don't know about any place, or you cannot find anything matching the request you should just say that you can't find anything in a expresive and emphatic way related to the asked question. You should provide the information in plain text, with natural language understandable by tourists. Please, also consider only the following points of interest when giving advices. Otherwise, just say that you can't find anything. Answer in plain natural text please, neither markdown or HTML. Here are the PoIs:" + {{list of entities from the context broker}}. • User prompt: {{specific question}}.</p><p>The system continuously updates information based on real-time data changes, such as visitor affluence or price, ensuring that users will receive the most current and relevant information. This dynamic capability is crucial for providing accurate and timely guidance to tourists. The subscription system provided by FIWARE enables the RAG to meet the real-time requirements of smart cities. A typical workflow would involve an IoT sensor updating an entity in the context broker (e.g., the occupancy level of a landmark). The context broker would then automatically send a notification to all users subscribed to that entity. In our scenario, the tourist assistant would immediately make the updated information available to the UFM and the UFM would be able to answer the user in natural language.</p><p>Initially, NGSI-LD data about the city's monuments and PoIs are loaded into the FIWARE context broker. One limitation of urban environments is the high number of elements, which makes searches in a naive RAG that lacks geographic search capabilities, both challenging and slow. To overcome this, the tourist assistant leverages the geospatial and temporal filtering capabilities of the FIWARE context broker to reduce the number of entities. The number of retrieved entities directly affects the RAG's performance, both in terms of execution time and response accuracy. In the case of the tourist assistant, the filtering is applied based on the geographical area of interest specified by the user. In this study, rectangular polygons were used.</p><p>A tourist planning to visit a famous monument in Madrid can use the application to find optimal visit times, current prize conditions, and nearby events. By zooming into the area of interest on the map, they receive detailed information about monuments, including their history and significance, enriched by the UFM to provide an engaging narrative. This is depicted in Figure <ref type="figure" target="#fig_2">4</ref> where a screenshot of the interface shows a user asking for recommendations.</p><p>The sequence diagram of Figure <ref type="figure" target="#fig_3">5</ref> illustrates an example of the interactions among five components of the system: user, front end, context broker, IoT Agents, and LLM, to manage NGSI-LD entities and respond to user queries dynamically. The process begins with the user interacting with a map on the front end, performing actions such as zooming in or moving, which triggers the front end to request relevant POIs from the FIWARE context broker to be displayed. The context broker returns the requested POIs in NGSI-LD format. Concurrently, the context broker engages in continuous </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and discussion</head><p>We devised a two-stage experimental setup in order to evaluate both the efficiency and the quality of the responses generated by our LLM-driven RAG architecture. The first stage focuses on measuring the response times associated with the context broker retrieval and subsequent LLM processing, examining how variations in query parameters and data volume influence latency. In the second stage, we conducted a series of tests to assess the ability of the model to provide correct and contextually relevant answers, given real-time data on PoIs. We present the time and accuracy measurement experiments in detail, highlighting how both the context broker and LLM react to changes in query configuration and size. We have tested three configurations with respect to the maximum number of elements set to be returned by the context broker with 10, 100, and 650 entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Measuring latency on different contextual information</head><p>In the first set of experiments, we focused on assessing the latency associated with retrieving PoI from the Orion-LD context broker and subsequently obtaining responses from OpenAI <ref type="bibr">GPT-4.1 API (gpt-4.1-2025-04-14)</ref>. To this end, we crafted a set of seven distinct questions, each representing a different query type. For statistical robustness, each question was repeated 10 times under identical conditions. Moreover, the number of allowed PoIs in the query response was systematically varied among three limits: 10, 100, and 650 PoIs. This parameter tuning allowed us to observe how changes in the context scale impacted the overall response time.</p><p>The data set used in these tests consisted of 1,088 PoIs centered around Madrid, injected into the context broker. To ensure consistency in the results, no modifications were introduced to the bounding box defining the search area, so all queries targeted the same fixed geographical region. Due to the deterministic nature of the context broker, repeated queries with the same parameters yielded an identical set of PoIs, thus minimizing variability arising from data retrieval. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QL4</head><p>What can I visit today in Madrid that is free of charge and has a ratio of occupancy of less than 10%?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QL5</head><p>What can I visit today in Madrid that is free of charge and has a ratio of occupancy of less than 10%? You will know the occupancy percentage with the ration between the occupancy over the capacity of the PoI. QL6 Do you know if I can visit Museo del Prado? QL7 Do you know if I can visit the Eiffel Tower in Madrid? Furthermore, the instructions provided to the GPT model were kept constant throughout all trials, with only the prompt (i.e., the specific question) changed. This controlled environment ensured that observed time differences could be more directly attributed to inherent computational and retrieval overheads, rather than extraneous factors such as prompt engineering or fluctuating network conditions. The logged latency values included both the time required to fetch the relevant POIs from the context broker and the time associated with querying the LLM. After each query was repeated 10 times and the latencies recorded, the maximum PoI limit parameter was increased from 10 to 100, and eventually to 650 PoIs.</p><p>We deliberately sought to cover a broad spectrum of prompts, including general, open-ended questions such as "What can I visit in Madrid?" as well as more narrowly constrained prompts such as "What can I visit today in Madrid that costs between 10 and 20 C?". Furthermore, we incorporated queries referencing well-known entities where relevant PoI data was expected to be present such as "Do you know if I can visit Museo del Prado?", as well as deliberately misleading or contextually unsuitable inquiries "Do you know if I can visit the Eiffel Tower in Madrid?" to test scenarios where the answer was unequivocally unavailable or incorrect. By constructing a diverse query set along these dimensions, ranging from generic to highly specific, and from presumably correct to manifestly invalid, we aimed to assess how the complexity, specificity, and foreknowledge of a query's validity might influence the UFM's overall latency performance. The queries used in this phase of the experiment are labeled as latency questions (QL) and are included in Table <ref type="table">2</ref>. For our analysis in these experiments, we chose to report the median rather than mean latency because our response-time distributions are highly skewed and median is more robust to occasional network or processing outliers. Figure <ref type="figure" target="#fig_4">6</ref> shows a dual-axis bar chart with gold bars for the LLM API call median durations plotted against the left y-axis, scaled from 1000 to 2100 ms, and blue bars for the context broker median durations against the right y-axis, scaled from 0 to 500 ms. Superimposed on every bar are its observed minimum and maximum values. This annotation scheme reveals that, although the LLM medians vary from about 1109 -1128 ms on the simplest lookups (QL6/QL7 respectively) to roughly 1928 ms for the most complex constraint (QL5), the absolute worst-case latencies can be much higher (for example, a 57480 ms maximum on QL1), while minimums remain well below the median (566-918 ms). This variation in the measurements is mainly due to fluctuations in the load on OpenAI's servers, where multiple users are accessing concurrently.</p><p>In contrast, context broker call latencies cluster tightly: medians values range from 37 ms (QL6) to 52 ms (QL5), maximums from 151 (QL7) to 224 ms (QL1), and minimums from 15 (QL4) to 23 ms (QL1). This results reveal that for every prompt (QL1-QL7) the LLM's latency is more than an order of magnitude larger than the context broker's times even when comparing the minimum delay in the LLMs that would correspond to low load on OpenAI servers. It is clear that LLM-driven round trips not only exhibit substantially higher typical latencies than context broker queries but also suffer from far greater variability and pronounced long-tail delays, whereas the context broker remains highly predictable and low-variance.</p><p>Queries QL1 and QL5 exhibit the highest LLM median latencies because each imposes substantial, albeit different, processing burdens on the model. In the case of QL1, the wholly open-ended nature of the prompt forces the LLM to generate longer answers, impacting in the number of tokens generated and consequently on the delay. By contrast, QL5 is tightly constrained, multi-step requirement-identifying free points of interest whose occupancy ratio falls below ten percent-compels the LLM to parse and apply a quantitative ratio condition, effectively simulating an on-the-fly calculation in natural language. Together, these results underscore how both broad exploratory queries and complex quantitative constraints can substantially inflate end-to-end LLM response times.</p><p>In contrast, queries QL6 and QL7 yield markedly lower median latencies because they reduce the problem to a simple fact-check rather than open-ended retrieval or multi-step reasoning. Whether the model confirms the existence of the Museo del Prado (QL6) or correctly rejects the possibility of an "Eiffel Tower in Madrid" (QL7), it needs only to retrieve or verify a single entity and generate a brief affirmation or negation, minimizing both candidate filtering and token generation overhead. As a result, these single-lookup prompts consistently complete in roughly a fraction of the time required by more complex or broadly scoped queries.</p><p>Figure <ref type="figure">7</ref> disaggregates the combined median latencies of the LLM API plus context broker calls for each of our seven prompts, plotted at three PoI limits: small (10), medium (100), and large (650). Although the context broker's share is almost imperceptible in addition to the bulk processing time of the LLM, we include every query at each scale to show how feeding ever-larger context payloads of the model drives end-to-end latency. Across all three limits, the fully open-ended Q1 ("What can I visit today in Madrid?") is the slowest: broad requests force the model to scan and rank many candidate locations, and that cost grows steeply as you expand from 100 to 650 PoIs. Moreover, as will be seen in the correctness test, when many inputs that meet the conditions are passed to the LLM, it tends to hallucinate and generate very long responses, repeatedly mentioning the same entities.</p><p>In general, the total median time increases only modestly when moving from 10 to 100 PoIs, suggesting the model's internal optimizations handle moderate context expansions quite efficiently, but grows once the context exceeds a few hundred items. Meanwhile, more narrowly constrained queries, those asking for cost bounds or occupancy thresholds, consistently outperform QL1 and QL2 on every scale, since they reduce the search space before the model even begins generating text. Single-entity lookups (e.g. "Museo del Prado" or the deliberately invalid "Eiffel Tower in Madrid") likewise avoid large jumps in latency at higher PoI caps, implying that simple presence/absence checks impose far less reasoning overhead.</p><p>Overall, these results underscore two interacting drivers of LLM latency in a FIWARE pipeline: the breadth of the user's request and the volume of contextual data supplied. Highly open-ended questions suffer disproportionately as input size grows, while focused, predicate-driven prompts help contain response times even under heavier loads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Assessing correctness on different contextual information</head><p>For the second set of experiments, we maintain the same foundational setup as described in the previous subsection. Once again, we used the FIWARE OrionLD context broker and the OpenAI <ref type="bibr">GPT-4.1 (gpt-4.1-2025-04-14</ref>) model, and we followed the same parameter configurations, except the repetition of queries given that statistical significance was not needed in this case since the temperature used in the model was 0. Also, we maintained identical geographic constraints centered on Madrid. As before, we also varied the upper limit on the number of retrieved PoIs, examining Table <ref type="table">3</ref>: Questions to evaluate the correctness of the real-time spatial RAG system and the number of PoIs in the dataset that are related or satisfy the criteria of the corresponding question for the given limit scenarios with 10, 100, and 650 entities. This ensured that the experiments provided a comparable basis for analyzing the impact of increasing data volume on the model correctness.</p><p>However, unlike the initial series of tests, this round of experimentation sought to evaluate only the correctness and contextual quality of the answers provided by the UFM. To better assess the capacity of the model in this regard, we introduced a slightly modified set of questions. These new prompts were designed to test more rigorously the processing capabilities of the model. By adjusting the questions, while maintaining the underlying data source and processing pipeline unchanged, our aim was to isolate the factors that influence both the accuracy of the LLM's output and the reliability of its reasoning, ultimately gaining deeper insight into how the model handles real-time POI data when correctness is a primary objective.</p><p>The questions range from broad, open-ended requests for general recommendations to more intricate inquiries involving occupancy levels, relevance scores, and combined logical conditions. In this second batch of experiments, we designed the queries to progressively assess the ability of the LLM to interpret contextual data and apply logical constraints. For instance, the first query seeks a broad recommendation of interesting places in Madrid limited to the entities passed to the context broker, thereby testing the model's capacity to handle an open-ended request. The second query narrows down the response to the top five most relevant PoIs, utilizing a specific relevance field defined in the entity data. The third and fourth queries further examine attribute-based filtering by focusing on price, first requesting places within a specific price range and then free-of-charge entities only. This prompt validates whether the model is able to understand that a PoI with price 0 is the same as free. The fifth query introduces a categorical constraint-identifying PoIs related to sports, requiring the model to delve into the descriptions and detect which entities are associated with sports. The sixth query checks whether the model can correctly determine if a particular PoI, such as the Museo del Prado, is present in the contextual data provided by the RAG. We further expand the complexity in the eighth query by testing occupancy-awareness: the model must consider both absolute occupancy and capacity to determine whether a place is crowded. Finally, the last four queries incorporate logical clauses (AND, OR), as well as negation within these operators, to evaluate the model's capacity to interpret combined logical conditions. Through this progression, we systematically challenge the UFM to demonstrate more refined reasoning skills, from simple attribute recognition to sophisticated logical inference.</p><p>Table <ref type="table">3</ref> collects all the questions about entities (QE) and for each QE the number of entities could be positively addressed under the three different PoI retrieval limits: 10, 100, and 650. Each row corresponds to one question, and the entries in each column indicate the number of PoIs retrieved by the context broker that met the query's criteria. For instance, when the retrieval limit is set to 10, certain queries (e.g., QE1) match all the available PoIs. Conversely, other queries (e.g., QE5 under limit 10) yield no matching PoIs in that limited subset, making it impossible for the model to provide a relevant response. As the limit increases to 100 and 650, additional PoIs enter the result set, thereby expanding the number of queries that can be satisfied. In this way, the table highlights how scaling the PoI retrieval</p><p>The same behavior is observed when the number of PoIs provided to the LLM is increased to 100. Specifically, it is observed that the LLM is capable of delving into the descriptions of entities and filtering those related to sport (QE5).</p><p>In fact, it returns all three possible entities, such as the Santiago Bernabeu Stadium, which does not contain the word "sport" anywhere: "The Santiago Bernabéu Stadium is the home of Real Madrid, one of the most successful football clubs in the world. With a capacity of over 80,000 spectators, it is a must-visit for football fans." However, some errors are observed in the logical questions. In the case of QE9, it returned only one PoI out of three possible; in QE10, it returned 64 items, 5 of which did not meet the conditions; and in QE11, it returned 22, with one not meeting the conditions.</p><p>However, when scaling up to 650 PoIs, the system tends to fail. In this case, stylistic issues are observed-for example, in situations where the prompt is compatible with many PoIs, the LLM tends to return many repeated entities (QE1, QE10, QE12). In Q1, the LLM returns the entity 34 PoIs, but 14 were repeated. It also tends to fail at simple questions, such as listing the most relevant PoIs, where it does not take the relevance field into account (QE2). The model is able to detect individual entities (QE6-QE8), but in filters that should return a small number of entities, it detects some of them but not all. For example, in QE3 it returns 4 out of 7 possible entities, and in QE5 it returns 4 out of 6. As in the case of 100 PoIs, advanced logical queries involving multiple AND, OR, and NOT conditions also fail.</p><p>6 Conslusions and future work</p><p>The growing success of Foundation Models in industry, along with their potential for application across an increasing number of domains, highlights the need to standardize their integration into urban environments, taking into account both the components involved and the data flow. In this article, we propose a reference architecture for implementing RAGs in urban environments, as well as an implementation based on FIWARE technology as a comprehensive solution for deploying Foundation Model-based systems in smart cities. This architecture meets all the fundamental requirements of urban environments, including connectivity with IoT devices and third-party systems, scalability, security, and real-time updates, all enabled through geospatial and real-time searches with relationships between entities.</p><p>FIWARE, already widely validated in the literature as a robust platform for developing smart city solutions, is presented here as a technology enabler for the inclusion of RAG-based systems in urban contexts. However, although Foundation Models are rapidly evolving, with improvements in speed and larger context windows, significant limitations still remain.</p><p>Experiments conducted in three scenarios (with 10, 100, and 600 entities loaded into the model) show that the main bottleneck lies in the LLM itself, which experiences increased generation times when processing large volumes of information. In addition, a higher tendency to error has been observed as the amount of processed city data increases, even when these data fit within the model context window. However, the results obtained with a moderate number of entities are promising, demonstrating that it is currently feasible to implement RAG systems in urban environments, provided that a careful and efficient selection of relevant information is carried out.</p><p>As future lines of work, we propose validating the architecture across a wider variety of scenarios and urban domains. Additionally, the potential of providing the LLM with the ontology associated with each type of entity, as defined in the Smart Data Models, has not yet been explored. This structured and natural language information could help the model more accurately interpret the meaning of each property and the relationships between entities. Another promising line of research involves investigating the use of LLMs for the automatic generation of queries in NGSI-LD format, or alternatively, training specialized (fine-tuned) models capable of translating natural language queries into that format more efficiently and accurately. Furthermore, the LLM's ability to autonomously navigate through the entity graph has not been thoroughly evaluated. This functionality could enable dynamic context expansion, potentially improving system performance in complex reasoning, information retrieval, and task execution scenario adding agentic capabilities to the system.</p><p>To analyze in greater depth the latency observed in LLMs, it is proposed to conduct tests using a locally deployed LLM. This would allow for a fair comparison of performance between a local context broker and a local LLM, thereby eliminating the influence of potential overload on OpenAI's servers.</p><p>Regarding accuracy and scalability, a possible improvement path would be to implement a map-reduce strategy. In cases of overly general queries, the search could be broken down into subqueries by spatial areas, performing an independent request for each of them, and finally composing the global response from the partial results. This strategy could facilitate scalability in terms of the number of manageable PoIs and improve the quality of the generated responses.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: FIWARE components. The complete list of components is available at https://www.fiware.org/catalogue/</figDesc><graphic coords="5,189.00,72.00,234.00,171.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: General proposed architecture implemented with FIWARE</figDesc><graphic coords="8,72.00,72.01,467.97,251.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Screenshot depicting a user asking recommendations to the application</figDesc><graphic coords="10,130.50,72.00,351.00,228.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Sequence diagram of the real-time spatial RAG</figDesc><graphic coords="11,107.10,72.00,397.79,253.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: GPT-4.1 and context broker API median times (ms) results for prompts</figDesc><graphic coords="12,72.00,83.27,468.00,273.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification of the different types of RAG based on the requirements of urban environments.</figDesc><table><row><cell>RAG Characterstic</cell><cell cols="2">Naive RAG Graph RAG</cell><cell>AI Web Searching</cell><cell>Structured Data RAG</cell><cell>Real-time Spatial RAG</cell></row><row><cell>Contextual data</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell></row><row><cell>Private data</cell><cell>✓</cell><cell>✓</cell><cell></cell><cell>✓</cell><cell>✓</cell></row><row><cell>Real time data</cell><cell></cell><cell></cell><cell>✓</cell><cell>✓</cell><cell>✓</cell></row><row><cell>Spatial filtering</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>✓</cell></row><row><cell>Relationships between entities</cell><cell></cell><cell>✓</cell><cell></cell><cell></cell><cell>✓</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>IoT devices Alvarez-Espinar et al., Conde et al. [2022a], Araujo et al. [2019], Loss et al. [2023], Conde et al. [2022b,c], De Benedictis et al. [2024]. However, none of these works have explored the inclusion of foundation models to enhance and provide new capabilities to these systems.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Model Context Protocol: https://modelcontextprotocol.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>FIWARE Members: https://www.fiware.org/community/members/organizations-directory/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Smart Data Models: https://github.com/smart-data-models</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>OrionLD context broker: https://github.com/FIWARE/context.Orion-LD</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>IoT Agent: https://github.com/FIWARE/catalogue/tree/master/iot-agents</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>Draco: https://github.com/ging/fiware-draco</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Cosmos: https://github.com/ging/fiware-cosmos-orion-spark-connector</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>Keyrock: https://fiware-idm.readthedocs.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>FIWARE catalogue: https://www.fiware.org/catalogue/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>LLamaIndex: https://www.llamaindex.ai/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p>LangChain: https://www.langchain.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11"><p>GraphRag: https://microsoft.github.io/graphrag/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_12"><p>Cygnus: https://fiware-cygnus.readthedocs.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_13"><p>Keyrock: https://fiware-idm.readthedocs.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_14"><p>https://github.com/dncampo/real-time-spatial-rag-for-smart-cities/tree/main</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially supported by the <rs type="funder">Spanish Agencia Estatal de Investigación</rs> under Grants <rs type="grantNumber">FUN4DATE</rs> (<rs type="grantNumber">PID2022-136684OB-C22</rs>) and SMARTY (<rs type="grantNumber">PCI2024-153434</rs>), by <rs type="grantNumber">TUCAN6-CM (TEC-2024/COM460</rs>) funded by <rs type="funder">CM</rs> (<rs type="grantNumber">ORDEN 5696/2024</rs>) and SMARTY funded by the <rs type="funder">European Commission</rs> through the <rs type="grantName">Chips Act Joint Undertaking project SMARTY (Grant</rs> <rs type="grantNumber">101140087</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6s68suQ">
					<idno type="grant-number">FUN4DATE</idno>
				</org>
				<org type="funding" xml:id="_vZq6FRq">
					<idno type="grant-number">PID2022-136684OB-C22</idno>
				</org>
				<org type="funding" xml:id="_ZP3GF4D">
					<idno type="grant-number">PCI2024-153434</idno>
				</org>
				<org type="funding" xml:id="_mhgMrZc">
					<idno type="grant-number">TUCAN6-CM (TEC-2024/COM460</idno>
				</org>
				<org type="funding" xml:id="_zeJgCFs">
					<idno type="grant-number">ORDEN 5696/2024</idno>
					<orgName type="grant-name">Chips Act Joint Undertaking project SMARTY (Grant</orgName>
				</org>
				<org type="funding" xml:id="_tB7hPNn">
					<idno type="grant-number">101140087</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>limit can substantially affect the system's ability to produce correct answers, especially for queries requiring more specialized attributes or logical constraints. Table <ref type="table">4</ref>: Correctness test for questions using limits PoIs limit size in dataset Question (GPT-4.1) 10 100 650</p><p>To ensure clarity and facilitate understanding, Table <ref type="table">5</ref> summarizes the retrieved objects and their corresponding attributes for this specific experiment under the condtion of limit equal to 10. Certain attributes, such as the description, were intentionally omitted to maintain focus on the most relevant and concise information without compromising the intended insights. Table <ref type="table">4</ref> presents the results of the experiment. Each cell contains a mark: ✓ for correct answers, i.e., the model correctly filtered and integrated the relevant PoIs according to the prompt's conditions, providing all possible results or at least 10 when they are more than 10 possible PoIs, and without any PoI that violate the constraint; ≈ for partially correct answers, i.e., responses with formatting errors, answers that omit PoIs that meet the constraints but include at least 5 PoIs, or 50% of the total if fewer than 10 exist, and answers that include less than 10% of entities that violate the prompt constraints, such as labeling a PoI as free when it is not, or failing to reflect required occupancy levels; and for all other cases.</p><p>In the 10-PoI scenario, all questions were correctly answered. This demonstrates how the LLM is capable of understanding NGSI-LD entities and using them to respond to open-ended questions (QE1), performing filters based on parameters using natural language, and understanding concepts such as "relevant" (QE2), quantity ranges (QE3), or "free" (QE4). The model was also able to restrict itself to contextual information (QE5-QE8), as in this scenario this PoI is not provided. Although GPT-4.1 knows that Museo del Prado is in Madrid, in all its answers it indicates that it does not know it because it was not provided in the context. Lastly, it was shown that the LLM is also capable of interpreting all logical requests involving combinations of OR, AND, and NOT (QE9-QE12).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A comprehensive overview of large language models</title>
		<editor>Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian</editor>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fine-tuning or retrieval? comparing knowledge injection in LLMs</title>
		<author>
			<persName><forename type="first">Oded</forename><surname>Ovadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menachem</forename><surname>Brief</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshik</forename><surname>Mishaeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Elisha</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.emnlp-main.15</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</editor>
		<meeting>the 2024 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024-11">November 2024</date>
			<biblScope unit="page" from="237" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS &apos;20</title>
		<meeting>the 34th International Conference on Neural Information Processing Systems, NIPS &apos;20<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey on rag meeting llms: Towards retrieval-augmented large language models</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujuan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangbo</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengyun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3637528.3671470</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD &apos;24</title>
		<meeting>the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD &apos;24<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="6491" to="6501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Future trends and current state of smart city concepts: A survey</title>
		<author>
			<persName><forename type="first">Ayca</forename><surname>Kirimtat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Krejcar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Attila</forename><surname>Kertesz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fatih Tasgetiren</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2020.2992441</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="86448" to="86467" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Urban digital twin challenges: A systematic review and perspectives for sustainable smart cities</title>
		<author>
			<persName><forename type="first">Charlotte</forename><surname>Weil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">Elias</forename><surname>Bibri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Régis</forename><surname>Longchamp</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.scs.2023.104862</idno>
		<ptr target="https://doi.org/10.1016/j.scs.2023.104862" />
	</analytic>
	<monogr>
		<title level="j">Sustainable Cities and Society</title>
		<idno type="ISSN">2210-6707</idno>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>François Golay, and Alexandre Alahi</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An architecture of an intelligent digital twin in a cyber-physical production system</title>
		<author>
			<persName><forename type="first">Ashtari</forename><surname>Behrang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Talkhestani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nada</forename><surname>Lindemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nasser</forename><surname>Sahlab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Jazdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schloegl</surname></persName>
		</author>
		<author>
			<persName><surname>Weyrich</surname></persName>
		</author>
		<idno type="DOI">10.1515/auto-2019-0039</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="762" to="782" />
		</imprint>
	</monogr>
	<note>at -Automatisierungstechnik</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Towards urban general intelligence: A review and outlook of urban foundation models</title>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conceptualizing smart city applications: Requirements, architecture, security issues, and emerging trends</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K M</forename><surname>Bahalul Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharat</forename><surname>Bhushan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Dhiman</surname></persName>
		</author>
		<idno type="DOI">10.1111/exsy.12753</idno>
		<ptr target="https://doi.org/10.1111/exsy.12753" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2022">12753. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Future smart cities: requirements, emerging technologies, applications, challenges, and future aspects</title>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Abdul Rehman Javed</surname></persName>
		</author>
		<author>
			<persName><surname>Shahzad</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cities.2022.103794</idno>
		<ptr target="https://doi.org/10.1016/j.cities.2022.103794" />
	</analytic>
	<monogr>
		<title level="j">Cities</title>
		<idno type="ISSN">0264-2751</idno>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page">103794</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Saif ur Rehman, Yousaf Bin Zikria, Imran Razzak, Zunera Jalil, and Guandong Xu</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Urban foundation models: A survey</title>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.1145/3637528.3671453</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD &apos;24</title>
		<meeting>the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD &apos;24<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="6633" to="6643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploring context window of large language models via decomposed positional vectors</title>
		<author>
			<persName><forename type="first">Zican</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Mackey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Paquet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="10320" to="10347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A survey on rag with llms</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hussam</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saba</forename><surname>Munawar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Cruz</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2024.09.178</idno>
		<ptr target="https://doi.org/10.1016/j.procs.2024.09.178" />
	</analytic>
	<monogr>
		<title level="m">28th International Conference on Knowledge Based and Intelligent information and Engineering Systems</title>
		<imprint>
			<publisher>KES</publisher>
			<date type="published" when="2024">2024. 2024</date>
			<biblScope unit="volume">246</biblScope>
			<biblScope unit="page" from="3781" to="3790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hallucinations in llms: Understanding and addressing challenges</title>
		<author>
			<persName><forename type="first">Gabrijela</forename><surname>Perković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antun</forename><surname>Drobnjak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivica</forename><surname>Botički</surname></persName>
		</author>
		<idno type="DOI">10.1109/MIPRO60963.2024.10569238</idno>
	</analytic>
	<monogr>
		<title level="m">2024 47th MIPRO ICT and Electronics Convention (MIPRO)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2084" to="2088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Retrieval-augmented generation for ai-generated content: A survey</title>
		<author>
			<persName><forename type="first">Penghao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinhan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunteng</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangcheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">RAPTOR: Recursive abstractive processing for tree-organized retrieval</title>
		<author>
			<persName><forename type="first">Parth</forename><surname>Sarthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Abdullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Tuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubh</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Ruicheng Yin, Changze Lv, Xiaoqing Zheng, and Xuanjing Huang. Searching for best practices in retrieval-augmented generation</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Qian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Passage re-ranking with bert</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient and effective passage search via contextualized late interaction over bert</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><surname>Colbert</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401075</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;20</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multihop-RAG: Benchmarking retrieval-augmented generation for multi-hop queries</title>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Conference on Language Modeling</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unifying large language models and knowledge graphs: A roadmap</title>
		<author>
			<persName><forename type="first">Linhao</forename><surname>Shirui Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiapu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2024.3352100</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3580" to="3599" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">From local to global: A graph rag approach to query-focused summarization</title>
		<author>
			<persName><forename type="first">Darren</forename><surname>Edge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ha</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Newman</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apurva</forename><surname>Mody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Truitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dasha</forename><surname>Metropolitansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">Osazuwa</forename><surname>Ness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Larson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939754</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Retrieval augmented generation (rag) and beyond: A comprehensive survey on how to make your llms use external data more wisely, 2024b. Sergey Vichev and Angel Marchev. Ragsql: Context retrieval evaluation on augmenting text-to-sql prompts</title>
		<author>
			<persName><forename type="first">Siyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luna</forename><forename type="middle">K</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="DOI">10.1109/IS61756.2024.10705186</idno>
	</analytic>
	<monogr>
		<title level="m">2024 IEEE 12th International Conference on Intelligent Systems (IS)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls</title>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binhua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiying</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhe</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">C C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reynold</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS &apos;23</title>
		<meeting>the 37th International Conference on Neural Information Processing Systems, NIPS &apos;23<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">When search engine services meet large language models: Visions and challenges</title>
		<author>
			<persName><forename type="first">Haoyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengnan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaiqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumi</forename><surname>Helal</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSC.2024.3451185</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Services Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4558" to="4577" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Prompt engineering in large language models</title>
		<author>
			<persName><forename type="first">Ggaliwango</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nakayiza</forename><surname>Hellen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daudi</forename><surname>Jjingo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joyce</forename><surname>Nakatumba-Nabende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Intelligence and Cognitive Informatics</title>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">Jeena</forename><surname>Jacob</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Selwyn</forename><surname>Piramuthu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Przemyslaw</forename><surname>Falkowski-Gilski</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Open-ti: Open traffic intelligence with augmented language model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13042-024-02190-8</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning &amp; Cybernetics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="4761" to="4786" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A retrieval-augmented generation approach for data-driven energy infrastructure digital twins</title>
		<author>
			<persName><forename type="first">Saverio</forename><surname>Ieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Loconte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Loseto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Ruta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Floriano</forename><surname>Scioscia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Marche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianna</forename><surname>Notarnicola</surname></persName>
		</author>
		<idno type="DOI">10.3390/smartcities7060121</idno>
	</analytic>
	<monogr>
		<title level="j">Smart Cities</title>
		<idno type="ISSN">2624-6511</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3095" to="3120" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Towards human-ai collaborative urban science research enabled by pre-trained large language models</title>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoying</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Fan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Evaluating the effectiveness of large language models in representing textual descriptions of geometry and spatial relations</title>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving first-stage retrieval of point-of-interest search by pre-training models</title>
		<author>
			<persName><forename type="first">Lang</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiqiang</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3631937</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<idno type="ISSN">1046-8188</idno>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2023-12">December 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m">FIWARE Foundation. FIWARE 4 CITIES</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simulation based performance evaluation of fiware iot platform for smart agriculture</title>
		<author>
			<persName><forename type="first">Kari</forename><surname>Kolehmainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pirazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juha-Pekka</forename><surname>Soininen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juha</forename><surname>Backman</surname></persName>
		</author>
		<idno type="DOI">10.5220/0011918700003482</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Internet of Things, Big Data and Security -IoTBDS</title>
		<meeting>the 8th International Conference on Internet of Things, Big Data and Security -IoTBDS</meeting>
		<imprint>
			<publisher>INSTICC, SciTePress</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="73" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Giuseppe Fico, and María Fernanda Cabrera Umpiérrez. Enhancing industrial digitalisation through an adaptable component for bridging semantic interoperability gaps</title>
		<author>
			<persName><forename type="first">Diego</forename><forename type="middle">F</forename><surname>Carvajal-Flores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Abril-Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Buhid</surname></persName>
		</author>
		<idno type="DOI">10.3390/app14062309</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Next generation meta operating systems (nemo) and data space: envisioning the future</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Segou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><forename type="middle">S</forename><surname>Skias</surname></persName>
		</author>
		<author>
			<persName><surname>Terpsichori-Helen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodore</forename><surname>Velivassaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enric</forename><surname>Zahariadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rubén</forename><surname>Pages</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosaria</forename><surname>Ramiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panagiotis</forename><forename type="middle">A</forename><surname>Rossini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Karkazis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Muniz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Contreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Del Rio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Belesioti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spyridon</forename><surname>Chochliouros</surname></persName>
		</author>
		<author>
			<persName><surname>Vantolas</surname></persName>
		</author>
		<idno type="DOI">10.1145/3685651.3686661</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Eclipse Security, AI, Architecture and Modelling Conference on Data Space, eSAAM &apos;24</title>
		<meeting>the 4th Eclipse Security, AI, Architecture and Modelling Conference on Data Space, eSAAM &apos;24<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="41" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploiting the internet resources for autonomous robots in agriculture</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Emmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roemi</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Gonzalez-De Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Francia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Golfarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuliano</forename><surname>Vitali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendrik</forename><surname>Sandmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hustedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Merve</forename><surname>Wollweber</surname></persName>
		</author>
		<idno type="DOI">10.3390/agriculture13051005</idno>
	</analytic>
	<monogr>
		<title level="j">Agriculture</title>
		<idno type="ISSN">2077-0472</idno>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Smart governance and e-public administration in smart cities</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Alvarez-Espinar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iris</forename><surname>Yuping Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhail</forename><surname>Khan</surname></persName>
		</author>
		<idno type="DOI">10.1049/PBBE005E_ch10</idno>
		<imprint>
			<biblScope unit="page" from="199" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Applying digital twins for the management of information in turnaround event operations in commercial airports</title>
		<author>
			<persName><forename type="first">Javier</forename><surname>Conde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Munoz-Arcentales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Rojo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joaquín</forename><surname>Salvachúa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Huecas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Álvaro</forename><surname>Alonso</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.aei.2022.101723</idno>
		<ptr target="https://doi.org/10.1016/j.aei.2022.101723" />
	</analytic>
	<monogr>
		<title level="j">Advanced Engineering Informatics</title>
		<idno type="ISSN">1474-0346</idno>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page">101723</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Performance evaluation of fiware: A cloud-based iot platform for smart cities</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saguna</forename><surname>Saguna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christer</forename><surname>Åhlund</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jpdc.2018.12.010</idno>
		<ptr target="https://doi.org/10.1016/j.jpdc.2018.12.010" />
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<idno type="ISSN">0743-7315</idno>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="250" to="261" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Using fiware and blockchain in smart cities solutions</title>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Loss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preet</forename><surname>Har</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nélio</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederico</forename><surname>Cacho</surname></persName>
		</author>
		<author>
			<persName><surname>Lopes</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10586-022-03732-x</idno>
		<ptr target="https://doi.org/10.1007/s10586-022-03732-x" />
	</analytic>
	<monogr>
		<title level="j">Cluster Computing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2115" to="2128" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Modeling digital twin data and architecture: A building guide with fiware as enabling technology</title>
		<author>
			<persName><forename type="first">Javier</forename><surname>Conde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrés</forename><surname>Munoz-Arcentales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Álvaro</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonsoles</forename><surname>López-Pernas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joaquín</forename><surname>Salvachúa</surname></persName>
		</author>
		<idno type="DOI">10.1109/MIC.2021.3056923</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="7" to="14" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Overcoming the barriers of using linked open data in smart city applications</title>
		<author>
			<persName><forename type="first">Javier</forename><surname>Conde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Munoz-Arcentales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnny</forename><surname>Choque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Huecas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Álvaro</forename><surname>Alonso</surname></persName>
		</author>
		<idno type="DOI">10.1109/MC.2022.3206144</idno>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="109" to="118" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A digital twin architecture for intelligent public transportation systems: A fiware-based solution</title>
		<author>
			<persName><forename type="first">Alessandra</forename><surname>De Benedictis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franca Rocco</forename><surname>Di Torrepadula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandra</forename><surname>Somma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web and Wireless Geographical Information Systems</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="165" to="182" />
		</imprint>
	</monogr>
	<note>Maryam Lotfian and Luigi Libero Lucio Starace</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The architectural implications of autonomous driving: Constraints and acceleration</title>
		<author>
			<persName><forename type="first">Shih-Chieh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang-Hong</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Skach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><forename type="middle">E</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjia</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Mars</surname></persName>
		</author>
		<idno type="DOI">10.1145/3296957.3173191</idno>
	</analytic>
	<monogr>
		<title level="j">SIGPLAN Not</title>
		<idno type="ISSN">0362-1340</idno>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="751" to="766" />
			<date type="published" when="2018-03">March 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Soft real-time hadoop scheduler for big data processing in smart cities</title>
		<author>
			<persName><forename type="first">Ciprian</forename><surname>Barbieru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florin</forename><surname>Pop</surname></persName>
		</author>
		<idno type="DOI">10.1109/AINA.2016.122</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 30th International Conference on Advanced Information Networking and Applications (AINA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="863" to="870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Awq: Activation-aware weight quantization for on-device llm compression and acceleration</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Gibbons</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Pekhimenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</editor>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="87" to="100" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
