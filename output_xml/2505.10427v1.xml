<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Influence of prior and task generated emotions on XAI explanation retention and understanding</title>
				<funder ref="#_S4MJz6m">
					<orgName type="full">Deutsche Forschungsgemeinschaft (DFG, German Research Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2025-05-15">15 May 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Birte</forename><surname>Richter</surname></persName>
							<email>birte.richter@uni-bielefeld.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Medical Assistance Systems</orgName>
								<orgName type="department" key="dep2">Medical School OWL</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Cognitive Interaction Technology (CITEC)</orgName>
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<region>ORCID</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Medical Assistance Systems</orgName>
								<orgName type="department" key="dep2">Medical School OWL</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Cognitive Interaction Technology (CITEC)</orgName>
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<region>ORCID</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Aksonova</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Medical Assistance Systems</orgName>
								<orgName type="department" key="dep2">Medical School OWL</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Britta</forename><surname>Wrede</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Medical Assistance Systems</orgName>
								<orgName type="department" key="dep2">Medical School OWL</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Cognitive Interaction Technology (CITEC)</orgName>
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<region>ORCID</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Influence of prior and task generated emotions on XAI explanation retention and understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-05-15">15 May 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">9DB215AD99C46E91FA42664418B102A3</idno>
					<idno type="arXiv">arXiv:2505.10427v1[cs.HC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-19T11:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The explanation of AI results and how they are received by users is an increasingly active research field. However, there is a surprising lack of knowledge about how social factors such as emotions affect the process of explanation by a decision support system (DSS). While previous research has shown effects of emotions on DSS supported decision-making, it remains unknown in how far emotions affect cognitive processing during an explanation. In this study, we, therefore, investigated the influence of prior emotions and task-related arousal on the retention and understanding of explained feature relevance. To investigate the influence of prior emotions, we induced happiness and fear prior to the decision support interaction. Before emotion induction, user characteristics to assess their risk type were collected via a questionnaire. To identify emotional reactions to the explanations of the relevance of different features, we observed heart rate variability (HRV), facial expressions, and selfreported emotions of the explainee while observing and listening to the explanation and assessed their retention of the features as well as their influence on the outcome of the decision task. Results indicate that (1) task-unrelated prior emotions do not affected the ratantion but may affect the understanding of the relevance of certain features in the sense of an emotion-induced confirmation bias, (2) certain features related to personal attitudes yielded arousal in individual participants, (3) this arousal affected the understanding of these variables.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Related Work</head><p>As artificial intelligence (AI) systems increasingly support human decision-making across diverse domains-from healthcare to finance and beyond-there is a growing demand for these systems to be not only accurate, but also explainable. Explainable AI (XAI) aims to make machine-generated decisions transparent and interpretable, allowing users to understand and potentially trust the reasoning behind automated recommendations. A key goal of XAI research is to ensure that users can recall and understand the explanations provided by decision support systems (DSSs), particularly when those explanations concern the relevance of specific input features.</p><p>While early work has focused on providing mathematical explanations for AI researchers themselves, lay users as well as domain experts (i.e., medical experts) have been identified as an important target group. This shift in focus has lead to a re-evaluation of existing research, identifying the need for more interactive approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. However, the underlying assumption in this research has mostly been that interaction takes place with a rational decision maker who follows purely logical considerations. Thus, support has been intended to <ref type="bibr" target="#b0">(1)</ref> provide the human decision maker with relevance information about certain features, and (2) to avoid cognitive biases such as confirmation bias <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b0">1]</ref>. Yet, it is well known that human decisionmaking is heavily influenced by emotions <ref type="bibr" target="#b3">[4]</ref>.</p><p>More recently, emotions have been investigated in the context of XAI and decision-making. In <ref type="bibr" target="#b9">[10]</ref> it was shown that humans respond differently to explanations depending on their emotional state. Individuals with low arousal levels followed advice more when no explanation was given, whereas individuals with high arousal levels followed advice more when a guided explanation was given, i.e., an explanation that contained a context-sensitive selection of the features that were explained. It was concluded that arousal is more critical in how explanations are received than valence or the emotion category.</p><p>Furthermore, <ref type="bibr" target="#b4">[5]</ref> have observed that negative affect can be observed when explanations are given for an easy task and positive affect in case of explanation of an AI in a difficult task, indicating that affect valence may be a useful variable to determine the explanation strategies in a specific context, i.e., whether explanations should be given. Similarly, <ref type="bibr" target="#b2">[3]</ref> found in a vignette study that negative feelings would result from wrong advice and positive feelings from correct advice. In a further study, they found that emotions evoked by explanations increased or decreased trust <ref type="bibr" target="#b1">[2]</ref>. Emotions together with workload can correlate with "explanatory efficacy" <ref type="bibr" target="#b5">[6]</ref>.</p><p>[7] investigated whether XAI systems can intervene to regulate the explainees' emotions. Here, a nudging strategy was investigated. It was found that nudging strategies to emotional debiasing are effective, yet not sufficient for rational decision-making.</p><p>However, clear guidelines regarding how emotions affect the understanding in a decision-making situation are still missing. This work contributes to the field by addressing this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Research Questions</head><p>It is important to note that emotions can arise as contextual factors, i.e., from unrelated tasks, or from the interaction with the system concerning the task at hand. While both emotions are likely to have influence on the human's processing, it is important to separate the effects of prior task-unrelated emotions from those that are closely related with the task at hand.</p><p>In this research, we therefore investigate tree research questions:</p><p>• RQ1a: Do task-unrelated emotions influence the retention of explained feature relevance? • RQ1b: Do task-unrelated emotions influence the understanding of explained feature relevance? • RQ2: Which features trigger emotional reactions during explanation? (task-related emotions) • RQ3a: Do task-related emotions influence retention of the explanation features? • RQ3b: Do task-related emotions influence understanding of the explanation features?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>To investigate the influence of emotions on retention and understanding of explanations, we conducted an interaction study with a decision support system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Measurements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Independent Variables</head><p>Two types of emotional influences were considered: (1) prior taskunrelated emotionsand (2) task-related emotions, defined as emotional responses elicited directly by the explanation itself.</p><p>Prior task-unrelated emotions. The prior task-unrelated emotions are induced before the explanation and unrelated to its content. We chose a between-subjects design, with participants assigned to either a fear or a happiness condition, as an induced emotion. The emotion induction itself is described in section 3.4.2.</p><p>Task-related emotions. We measure the emotional reactions by using the EmoNet arousal data during the feature presentation. Based on the arousal value a, an anomaly (emotional reaction) in the arousal state is detected by zscore =</p><p>a -rollmean roll sd <ref type="bibr" target="#b0">(1)</ref> in combination with the rolling z-score with k = 2.5 and a window of 500ms.</p><formula xml:id="formula_0">emotionalreaction = 1, if z &gt; 2.5 0, else<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Dependent Variables</head><p>Retention. Retention was measured based on the participant's verbal recall of the features they remembered as relevant to the AI's decision, as conveyed through the explanation. Participants were explicitly asked to provide verbal input, allowing them to articulate their reasoning processes. The spoken responses were automatically transcribed using Whisper, a deep neural network-based automatic speech recognition system. The recognized words were then manually mapped to the ten predefined variable names, accounting for minor inaccuracies in naming. Understanding. To assess the extent to which participants understood the meaning of the variables, they were asked-via a graphical user interface (see Fig. <ref type="figure" target="#fig_4">4</ref>)-to indicate the contribution of each variable to the overall decision of the decision support system (DSS). Specifically, participants were instructed to indicate whether a given variable contributed to a higher or lower risk estimate. This was implemented by allowing users to move each variable to the left (indicating lower risk) or to the right (indicating higher risk). We interpreted this recalled information as a cue for (retained) understanding of the explanation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Control Variables and Additional Measurements</head><p>In addition to the primary measures, several supplementary variables were recorded for exploratory and control purposes:</p><p>• Gender: Participant gender (male/female/divers?) was recorded.</p><p>• STAI: State-Trait Anxiety Inventory (STAI), providing a measure of participants' baseline anxiety disposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• mDes:</head><p>• Hear Rate Variability (HRV) • EmoNet Results: In addition to arousal scores, full output vectors from the EmoNet model were stored for each facial frame, enabling detailed emotional state tracking over time.</p><p>• System Events: System-level events (e.g., start of an explanation)</p><p>were logged for quality control and alignment of multimodal data streams. • Videos: All participant sessions were video recorded for potential qualitative analysis and cross-validation of facial expression data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decision Suport System</head><p>The system centers around an embodied conversational agent named Flobi, who provides a personalized assessment of an individual's risk profile based on a set of predefined input features. This risk assessment is subsequently used in the context of a Holt and Laury lottery task, where participants make incentivized decisions under risk. The agent's evaluation serves as a form of decision support, offering guidance while allowing participants to ultimately make their own choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Participants</head><p>A total of 24 participants took part in the study. Of these, 15 were assigned to the fear condition and 9 to the happy condition. The sample included 8 male and 16 female participants. All participants were students at the Bielefeld university.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experimental Setting -Procedure</head><p>The experiment consisted of six phases (cf. Fig. <ref type="figure" target="#fig_3">3</ref>.4. We recorded the users' heart rate variability, video and audio, and their facial expressions as computed by EmoNet. In the first phase, a questionnaire was administered that requested data from the user to assess the user's risk type. The risk type classification was achieved by a linear scoring scheme integrating the user's numerical answers, yielding risk a type value between 0 and 9. After an emotion induction sequence, the actual risk task was explained to the user and the user could provide his/her risk selection, i.e., selecting a high or low risk on a scale from 0 to 9. After the user's first risk decision, the system would present its risk suggestion to the user, based on the evaluation of the user's risk type. More specifically, a user yielding a high value for a high-risk propensity would receive a suggestion of a high-risk choice and vice versa, also on a scale from 0 to 9. This suggestion was followed by an explanation of all ten variables and their relevance to the estimated risk type of the user. For example, being female was an indicator towards less risk propensity, whereas being male was an indicator for a high-risk propensity. We analyzed the HRV and Facial Expressions during theses episodes to detect arousal. After this explanation, the user could revise his/her decision. In the last phase, the users were asked to (verbally) name the features that they remembered from the explanation. After this, they were presented the features one after another and were asked to determine whether a certain value of this feature was an indicator for higher or lower risk propensity. We used this information as a proxy for understanding.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Questionnaire</head><p>During the first phase of the interaction, the participants were asked to fill out an online questionnaire, with the virtual robot Floka being present and asking the questions. A total of ten questions were asked. We refer to these questions as the "variables" or "features" that the system uses to compute the risk type of the participant.</p><p>Based on results from empirical studies reported in the literature for each variable, a scoring scheme was developed that assigned a score for each answer indicating higher or lower risk propensity. Overall, this resulted in a linear scoring scheme -the more scores, the higher the risk propensity. Scores were assigned based on a median value reported in the literature above or below which a higher (+1) or lower (0) risk score was given. This resulted in a risk type value between 0 and 9 for each participant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Emotion Induction</head><p>To investigate the effect of prior non-task related emotions on retention and understanding, we induced emotions via a biographic event recall similar to the one used in <ref type="bibr" target="#b9">[10]</ref>. The participants were randomly assigned to one of the two emotions: Fear and Happiness. For the induction, they were asked to remember an actual event where they experienced fear (or happiness). They were given 5 minutes time to mentally replay the situation to induce the emotion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Risk decision: First choice</head><p>Directly after the emotion induction, the participants were explained the risk task and asked to provide their first decision. This first decision is important information to determine if the DSS' suggestion and explanation has an effect on the participant's (second) decision. That is, if the participant changes her selection in the direction of the system's suggestion, this is an indicator of advice taking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">XAI decision and explanation</head><p>In the explanation phase (phase 4, see above), Flobi would then explain for each variable, whether the explainee's (self-rated) value (e.g., of her political orientation) was an indicator adding to a higher or lower probability of the explainee as being rather more risk friendly. In this case, Flobi would say: "Because your political orientation is rather left, you are presumably more risk friendly". Fig. <ref type="figure" target="#fig_3">3</ref>.4.4 shows the GUI, where all variables are depicted with their respective contributions to the AI system's decision. This would be repeated for all 11 variables. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.5">Assessment of user's retention and understanding of the explanations and the task</head><p>In the last phase the uers' retention and understanding were assessed using the procedures described in subsection 3.1.2. See Fig. <ref type="figure" target="#fig_4">4</ref> for the GUI to sort the features according to the remembered influence it had on the risk type classifcation. 4 Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Emotion Manipulation Check</head><p>To examine differences in affective responses between induction conditions, we compared SAM Valence and Arousal scores across groups (Fear vs. Happy). On the Valence scale, which ranges from 1 ("unpleasant feeling") to 5 ("pleasant feeling"), participants in the Happy condition reported higher valence ratings (median ≈ 4), indicating more pleasant emotional states compared to the Fear group (median ≈m 3). Similarly, on the Arousal scale (1 = "calm", 5 = "aroused"), the Fear group tended to report higher arousal levels than the Happy group, suggesting that the fear induction elicited more physiologically activating emotional responses. These trends align with theoretical expectations -fear typically evokes high arousal and negative valence, while happiness is associated with positive valence and lower arousal. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Influence of task-unrelated prior emotion on retention</head><p>To answer the RQ1a (Do task-unrelated emotions influence the retention of explained feature relevance?) the mean recall of the verbally and visually explained features was measured for each condition by analyzing the verbal answer to the question which features the user remembered. Note that this was an open question requiring the users to actively retrieve and formulate the name of the features while ignoring the meaning it had on the outcome. Since we recorded the participant's voice during the whole experiment, we were able to capture their comments also during the explanation of the features.</p><p>As noted above, due to a programming error, one feature ("Einstellung bzgl. Zukunft" -attitude towards future) was reported wrongly for the majority of participants. In these cases, some participants would comment the mistake spontaneously through a verbal utterance. However, some participants also commented features that were communicated correctly. For the participants, there was no difference between these cases -they experienced both cases as a mistake from the system. As these comments indicated an epistemic reaction (surprise) we also investigated their effect on retention. Figure <ref type="figure" target="#fig_6">6</ref> shows the retention for each task-unrelated induced emotion (left). The right side shows the average verbal labeling of features that participants explicitly identified as incorrect. A one-way ANOVA was conducted to compare the mean retention between the two induced prior emotions (Fear, Happiness). The analysis revealed no significant difference (F (1, 22) = 0.1619, p = .6913). Thus, the retention of features is not influenced by a prior emotion.</p><p>However, those feature explanations that were perceived as erroneous by some participants may have yielded a better retention as they attracted attention. To investigate if the induced emotion had an effect on the perceived incorrect features, a one-way ANOVA was conducted to examine the effect of the induction condition on the number of recall of the perceived incorrect features. The analysis revealed no significant main effect of induction, F (1, 22) = 2.59, p = .122. However, as can be seen in Fig. <ref type="figure" target="#fig_6">6</ref> participants in the condition "Happy" remembered almost twice as many features they had commented as wrong as those in the "Fear" condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Influence of task-unrelated prior emotion on understanding</head><p>Addressing RQ1b (Do task-unrelated emotions influence the understanding of explained feature relevance?), the explainees were asked to indicate for each variable which influence it had in their own case on their risk propensity as estimated by the AI system (see Fig. <ref type="figure" target="#fig_4">4</ref>). An ANOVA was conducted to examine the effects of taskunrelated emotions (emotion induction) and explained feature and their interactions with the outcome variable understanding.</p><p>There was a significant main effect of the explained feature, F (9, 220) = 3.51, p &lt; .001, suggesting that the feature categories differed significantly in their association with the outcome understanding. The main effect of emotion induction was marginally significant, F (1, 220) = 3.23, p = .074, indicating a potential trend for the induction condition, i.e. the task-unrelated emotion, to influence responses. The interaction effect induction emotion × explained feature interaction was not statistically significant, F (9, 220) = 1.62, p = .111.</p><p>For a more qualitative analysis, we visualized the matching ("match") vs. non-matchin ("miss") answers of the participants with the explantation they had received in the previous phase from Flobi (see Fig. <ref type="figure" target="#fig_7">7</ref>). Interestingly, we see that the feature that was explained wrongly to most of the participants ("Einstellung bzgl. Zukunft") was remembered differently by the participants with different induced emotions. While 80% of the participants of the Fear condition agreed with Flobi's (wrong!) explanation, only 44% of the participants of the Happy condition did so. This is somewhat surprising, as one would expect that fear is generally associated with suspicion, leading to scrutinizing offered information and suggestions. Yet, in this case, it seems that participants in the Happy condition remembered their own decision better.</p><p>A different interpretation might be, that there are two effects of emotion on retention: (1) Users do not remember their answer to these specific questions about an uncertain future correctly, as they were given in a neutral state. Rather, they "remember" their current emotionally tainted attitude towards the future: in the condition Fear this would be negative, in the condition Happy this would be positive. Indeed, in the ATF it is being argued that happiness gives high attribution to certainty, fear towards low certainty. (2) Users judge the impact of these variables according to their current emotion: users in the Fear condition believe that being uncertain about the future reduces the risk propensity (although research shows a different relationship: high uncertainty about the future increases risk propensity, low uncertainty decreases it), whereas users in the Happy condition believe that being uncertain about the future increases risk propensity. This might be seen as some kind of confirmation bias or transfer, as this estimated influence on risk propensity corresponds to their own risk propensity at that time: being happy (rather than being certain about the future) increases risk propensity, whereas being fearful (rather than being uncertain about the future) decreases risk propensity.</p><p>Although it is not certain, what exactly causes these different judgment results, it is clear that the emotional state can affect how the influence of certain variables on the outcome is remembered or judged. Yet, it remains unclear how such an effect could be detected in interaction with an intelligent, explainable AI system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of explanations on arousal</head><p>To determine the effect of explanations on arousal, we defined the emotional reactions -or arousal -by using the Emonet arousal data during the feature presentation in combination with the rolling z-score with k = 2.5 and a window of 500ms (cf. equations ( <ref type="formula">1</ref>) and ( <ref type="formula" target="#formula_0">2</ref>)). In this way, we determined the peaks of arousal that stood out from the preceding arousal values, indicating emotional reactions. Fig. <ref type="figure" target="#fig_12">11</ref> shows the arousal values as computed by EmoNet as a red line, plotted over time, for participant 25. The feature names (rotated vertically) denote the beginning of the verbal (and visual) explanation of the according feature. For example, the explanation of the feature "Geschlecht" ("gender") starts at the second 0.</p><p>Vertical black lines indicate a positive z-score and therefore an emotional reaction. For example, the black line at second 4 indicates an arousal bout right at the beginning of the feature explanation for "Politische Einstellug" ("political attitude"). The yellow lines indicate a rapidly downfall of the measured arousal. If a feature explanation segment contained one (or more) bouts of arousal -as indicated by the black lines -it was counted as a feature explanation causing an emotional reaction (or arousal).</p><p>Fig. <ref type="figure" target="#fig_9">9</ref> shows the proportion of participants per feature for whom arousal was measured. As can be seen, there are differences in frequency of arousal for the different features. The features "gender" ("Geschlecht") and "even temper of the last 4 weeks" ("Ausgeglichenheit...") yielded most frequent bouts of arousal with almost 60% of the participants whereas "risk propensity job" ("Risikobereitschaft Beruf"), "livelong learning" ("lebenslanges Lernen"), and "current health" ("Gesundheitszustand gegewaertig") yielded least frequent arousal with about 20%. This indicates that features differ in their potential to evoke arousal. What the underlying reasons for the arousal are, remains unclear, so far. But there may be intrinsic (e.g., the personal relevance of this feature for each participant) as well as extrinsic (e.g. recency effect, length of word / ease of word) reasons. In addition to these differences we also see different frequencies of arousal between participants in the Fear and the Happy group. Most striking is the difference in the feature "attitude towards future" ("Einstellung bzgl. Zukunft") which raised arousal in about 25% of the participants in the Fear group compared to about 45% in the Happy group. Most strikingly, it was this last feature where an error occurred. Thus, arousal would be expected in both groups. However, this cannot be observed in the Fear group, from where arousal would be expected most frequently.</p><p>Thus, overall we see that explanations can induce arousal. However, so far no clear pattern as to what factors actually cause the arousal is recognizable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effect of explanation-induced arousal on retention</head><p>Figure <ref type="figure" target="#fig_11">10</ref> visualizes the mean retention as a function of the number of emotional reactions that a feature explanation evoked. The size of the bullet visualizes the number of occurrences. Here, the explanations of all 10 features for all 24 participants (10 x 24 = 240) have been taken into account. For example, in the left figure the largest dot a numberemotionalreactions = 0 indicates that the mean retention for those 100 (given by the size of the dot) feature explanations that yielded 0 emotional reactions (i.e. bouts of arousal) was 25On the right side the graph is split into the reactions of the participants from the Fear and the Happy condition.</p><p>To examine whether emotional reactions were associated with participants' retention of individual features, we fitted a generalized linear mixed model (GLMM) with a binomial distribution and logit link to predict the binary outcome retention. The fixed effects included emotional reactions (i.e. arousal), emotion induction (i.e. Happy vs Fear), and the explained feature, while a random intercept was included for participant ID (N = 240 observations, 24 participants). The binary dependent variable indicated whether a feature was verbally recalled (retention = 1) or not (retention = 0). Model estimation was performed using the glmer() function from the lme4 package in R.</p><p>The predictors emotional reactions, emotion induction, did not reach significance (p &gt; .10). This means that neither arousal nor the induced emotion had an influence on the recall of a feature.</p><p>The model showed a significant negative intercept (β = -2.95, SE = 0.89, z = -3.33, p &lt; .001), suggesting a low baseline probability of the retention of all features. Among the fixed effects, the following explained feature variables were significant predictors and more likely to recall verbally: This means that the features Gender, Current health status, and Political orientation are predictors for retention. This is an interesting result as Gender was the feature with the most frequent bouts of arousal, whereas Current health status yielded the least frequent bouts of arousal which indicates that arousal alone may not be a good predictor of retention. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Effect of explanation-induced arousal on understanding</head><p>While we were not able to show a significant effect of arousal on retention it is possible that arousal affects understanding. We applied the same approach as for retention and fitted a generalized linear mixed model (GLMM) with a binomial distribution and logit link to predict the binary outcome understanding. The fixed effects included emotional reactions, emotion induction, and the explained feature, while ID was modeled as a random intercept to account for individual variability. Model estimation was performed using the glmer() function from the lme4 package in R.</p><p>The model showed a significant positive intercept (β = 1.07, SE = 0.52, p = .040), indicating a relatively high baseline probability of the outcome understanding. That is, the participants had a good understanding of what effect each feature had on the outcome of the system.</p><p>The emotional reaction is significantly negatively associated with the outcome (β = -0.45, SE = 0.19, p = .015), suggesting that higher levels of positive emotional reactions were associated with lower understanding. Thus, too much arousal or too many bouts of arousal hinder understanding.</p><p>The effect of the emotion induction was not significant (β = 0.64, p = .108), indicating no clear effect of the induced emotions Fear and Happiness on the outcome in this model.</p><p>Among the explained features, only "Concerns about one's own economic situation" had a significant negative effect (β = ˘2.31, p &lt; .001) on the understanding.</p><p>The random intercept for ID had a variance of 0.28 (SD = 0.53), indicating some variability in baseline response tendencies across participants. Thus, we see individual effects on the capability to understand the meaning of the effect of a feature on the outcome of the DSS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In the following, we will discuss our results regarding our initial research questions.</p><p>RQ1a: Do task-unrelated emotions influence the retention of explained feature relevance? RQ3a: Do task-related emotions influence retention of the explanation features?</p><p>Our results indicate that neither task-unrelated prior emotions nor task-generated emotional reactions (or arousal) were significant predictors of retention. That is, although there is evidence that a certain amount of arousal in general improves retention of information, this was not the case in the current study. There are many possible explanations for this. One explanation might be, that the complex explanation situation was novel and required a high cognitive load due to the new way of explaining the relevance of features. Another explanation might be that the feature variables themselves may not have been understood by the participants. Some variable names are very long and might be difficult to remember, so that participants were not able to map certain variable or feature names to the questions they had been asked in the initial questionnaire. This would require an addition explanation layer that allows the participant to ask for an explanation concerning the different (or globally most relevant) features.</p><p>RQ2: Which features trigger emotional reactions during explanation? (task-related emotions)</p><p>Our results indicate that certain individual characteristics -such as gender, current health status, and political orientation -are significant predictors for the recall of explained features. Further research needs to investigate what characteristics render features so salient that they are remembered better than others.</p><p>RQ1b: Do task-unrelated emotions influence the understanding of explained feature relevance?</p><p>We found a main effect of the explained feature on understanding but only a marginal effect that task-unrelation emotions influence understanding. This indicates that the features differed significantly in how well they were understood by the participants. However, the influence of task-unrelated emotions on understanding was only marginal.</p><p>RQ3b: Do task-related emotions influence understanding of the explanation features?</p><p>Most interestingly, we found that the emotional reaction, i.e., arousal, is significantly negatively associated with understanding, suggesting that higher levels of positive emotional reactions were associated with lower understanding. This is in accordance with other findings that indicate that too much arousal (or too many bouts of arousal, in our case) hinders understanding. Thus, it is an important goal to finde the right amount of arousal in order to foster understanding in a DSS scenario.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Study Setup during the first Hold and Laury decission (staged)</figDesc><graphic coords="1,365.28,248.45,122.49,144.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Experimental procedure with six phases.</figDesc><graphic coords="3,305.13,435.85,245.08,160.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Visual representation while Flobi was explaining which variables contributed to which risk type classification of the explainee. The red arrows indicated that the explainee's value of this variable contributed to an estimation of a lower risk type whereas a blue arrow indicated evidence for a higher risk type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Visualization of the user interface to answer the question whateffect the user's value of the presented variable had on the system's estimation of the user's risk type. This task was used to measure the explainee's retention of each variable.</figDesc><graphic coords="4,42.11,104.63,245.02,146.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Enter Caption</figDesc><graphic coords="4,42.11,544.18,245.08,153.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. (left) Mean feature retention divided for task-unrelated prior emotion. (right) Mean retention of 'wrong' feature divided for task-unrelated prior emotion.</figDesc><graphic coords="4,315.14,304.45,110.28,137.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. A comparison of the understanding of each presented feature, as tested by the recall task shown in Fig. 4,differentiated by task-unrelated induced emotion.</figDesc><graphic coords="5,42.11,100.22,245.09,134.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Arousal over the Feature Presentaion Time with emotional reaction detection</figDesc><graphic coords="5,305.13,381.19,245.09,134.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Enter Caption</figDesc><graphic coords="6,42.11,71.12,245.09,134.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>•</head><label></label><figDesc>Gender: β = 3.11, SE = 0.93, z = 3.34, p &lt; .001 • Current health status: β = 2.87, SE = 0.92, z = 3.11, p = .002 • Political orientation: β = 2.67, SE = 0.92, z = 2.90, p = .004</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. The mean retention of the feature divided on number of emotional reactions during the feature presentation for all (left) and divided by condition (right).</figDesc><graphic coords="6,315.14,177.12,110.28,137.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. The mean understanding of the feature divided for number of emotional reactions during the feature presentation for all (left) and divided by condition (right).</figDesc><graphic coords="7,52.13,71.12,110.28,137.85" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research was funded by the <rs type="funder">Deutsche Forschungsgemeinschaft (DFG, German Research Foundation</rs>): <rs type="grantNumber">TRR 318/1 2021-438445824</rs> "<rs type="projectName">Constructing Explainability</rs>".</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_S4MJz6m">
					<idno type="grant-number">TRR 318/1 2021-438445824</idno>
					<orgName type="project" subtype="full">Constructing Explainability</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Revealing the dynamics of medical diagnostic reasoning as step-by-step cognitive process trajectories</title>
		<author>
			<persName><forename type="first">D</forename><surname>Battefeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wehner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>House</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kellinghaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wellmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Affective analysis of explainable artificial intelligence in the development of trust in ai systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bernardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Seva</surname></persName>
		</author>
		<idno type="DOI">10.54941/ahfe1002861</idno>
	</analytic>
	<monogr>
		<title level="m">Intelligent Human Systems Integration (IHSI 2023): Integrating People and Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploration of emotions developed in the lnteraction with explainable ai</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bernardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Seva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 15th International Symposium on Computational Intelligence and Design (ISCID)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="143" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Affect, emotion, and decision making. Organizational behavior and human decision processes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="47" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Toward affective xai: facial affect analysis for understanding explainable human-ai interactions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Guerdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3796" to="3805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved explanatory efficacy on human affect and workload through interactive process in artificial intelligence</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="189013" to="189024" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Can ai regulate your emotions? an empirical investigation of the influence of ai explanations and emotion regulation on human decision-making factors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Lammert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">World Conference on Explainable Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page">2025</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Explanation as a social practice: Toward a conceptual framework for the social design of ai systems</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Rohlfing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Scharlau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Buhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Buschmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Esposito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grimminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Häb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="717" to="728" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What is missing in xai so far? An interdisciplinary perspective</title>
		<author>
			<persName><forename type="first">U</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wrede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KI-Künstliche Intelligenz</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="315" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human emotions in ai explanations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Thommes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lammert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wrede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Explainable Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Longo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Seifert</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="270" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-63803-9_15</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Designing theory-driven user-centric explainable ai</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI conference on human factors in computing systems</title>
		<meeting>the 2019 CHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
