<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High-and Low-Resource Languages</title>
				<funder ref="#_WD3Ga6S">
					<orgName type="full">Luxembourg National Research Fund (FNR) PEARL program</orgName>
				</funder>
				<funder ref="#_cRgh6Ph #_uFR82XQ #_wYQ7byv">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2025-04-19">19 Apr 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alessio</forename><surname>Buscemi</surname></persName>
							<email>alessio.buscemi@list.lu</email>
							<affiliation key="aff0">
								<orgName type="department">Luxembourg Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cédric</forename><surname>Lothritz</surname></persName>
							<email>cedric.lothritz@list.lu</email>
							<affiliation key="aff0">
								<orgName type="department">Luxembourg Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sergio</forename><surname>Morales</surname></persName>
							<email>smoralesg@uoc.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Universitat</orgName>
								<address>
									<addrLine>Oberta de Catalunya</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marcos</forename><surname>Gomez-Vazquez</surname></persName>
							<email>marcos.gomez@list.lu</email>
							<affiliation key="aff0">
								<orgName type="department">Luxembourg Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Clarisó</surname></persName>
							<email>rclariso@uoc.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Universitat</orgName>
								<address>
									<addrLine>Oberta de Catalunya</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jordi</forename><surname>Cabot</surname></persName>
							<email>jordi.cabot@list.lu</email>
							<affiliation key="aff0">
								<orgName type="department">Luxembourg Institute of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Luxembourg</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Germán</forename><surname>Castignani</surname></persName>
							<email>german.castignani@list.lu</email>
							<affiliation key="aff0">
								<orgName type="department">Luxembourg Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High-and Low-Resource Languages</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-04-19">19 Apr 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">3A5FCAB9BE07EE7BEF193C68B5D8986B</idno>
					<idno type="arXiv">arXiv:2504.18560v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-19T11:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large Language Models (LLMs) have exhibited impressive natural language processing capabilities but often perpetuate social biases inherent in their training data. To address this, we introduce MultiLingual Augmented Bias Testing (MLA-BiTe), a framework that improves prior bias evaluation methods by enabling systematic multilingual bias testing. MLA-BiTe leverages automated translation and paraphrasing techniques to support comprehensive assessments across diverse linguistic settings. In this study, we evaluate the effectiveness of MLA-BiTe by testing four state-of-the-art LLMs in six languages-including two low-resource languages-focusing on seven sensitive categories of discrimination.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large Language Models (LLMs) have become integral to modern Natural Language Processing (NLP) applications, demonstrating remarkable capabilities in tasks such as machine translation <ref type="bibr" target="#b32">[33]</ref>, text generation <ref type="bibr" target="#b5">[6]</ref>, and dialogue systems <ref type="bibr" target="#b25">[26]</ref>. Despite these successes, a growing body of research indicates that LLMs can exhibit harmful social biases, including stereotypes and discriminatory attitudes. Such biases can arise from historical and cultural prejudices embedded in the data used to train these models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Recent work underscores that social biases in LLMs can manifest in various forms, such as racist, sexist, or homophobic content <ref type="bibr" target="#b21">[22]</ref>. When deployed at scale, these biases risk perpetuating stereotypes and marginalizing vulnerable communities, raising ethical concerns and emphasizing the need for bias mitigation strategies <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b15">16]</ref>. While significant progress has been made in quantifying and reducing biases for high-resource languages like English, cross-lingual investigations reveal that biases also affect lesser-resourced languages, often in ways that are more difficult to detect and mitigate <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Previous frameworks for evaluating social biases in generative AI systems have primarily focused on single-language settings, limiting their applicability in global and multilingual contexts. However, bias in AI systems can manifest differently across languages and cultures, making it essential to assess models in a linguistically diverse manner. There is a growing need for tools that allow non-technical stakeholders-such as Human Resources departments, Ethics Committees, and Diversity &amp; Inclusion officers-to evaluate how AI systems align with their values across different languages. Enabling such inclusive and multilingual assessments is a crucial step toward fostering more trustworthy and equitable AI systems.</p><p>Ensuring the fairness of AI systems in multilingual and culturally diverse environments requires systematic evaluation across a broad spectrum of languages, including low-resource and regionally coofficial ones. However, the development of bias evaluation benchmarks in multiple languages remains a significant challenge, particularly when non-technical stakeholders are tasked with authoring or validating prompts in languages they do not actively use. This issue is especially pronounced in settings where official languages differ from those predominantly used in professional contexts. For instance, while Luxembourgish is an official language in Luxembourg, French and English are more commonly employed in the workplace. Similarly, Catalan is co-official in parts of Spain, yet not all professionals are proficient in its use. Analogous situations arise in countries such as South Africa and India, where languages like Zulu, Xhosa, Maithili, or Konkani hold official status but are often underrepresented in administrative and corporate environments.</p><p>The manual translation and paraphrasing of prompts to ensure semantic consistency and cultural appropriateness across languages is both time-consuming and difficult to scale. To address this limitation, we propose leveraging LLMs to automate these tasks. Specifically, we investigate whether LLMs can reliably perform translation and paraphrasing in a way that enables the generation of linguistically and culturally appropriate test cases. If effective, this approach would facilitate scalable and inclusive multilingual bias evaluations, while reducing dependency on native language expertise and enabling broader participation by non-technical stakeholders.</p><p>This paper introduces MLA-BiTe, a framework designed to enhance existing bias evaluation methods by supporting systematic multilingual bias testing. MLA-BiTe is built to operate on the input generated by Language Bias Testing (LangBiTe) <ref type="bibr" target="#b18">[19]</ref>, but it is flexible enough to be adapted for use with other bias detection systems. To guide our study, we focus on two primary research questions:</p><p>1.0.1 RQ1.</p><p>Can LLM-based translation and paraphrasing effectively serve as a method to augment test templates in multiple languages, and if so, which ordering of these steps yields the most reliable expansions?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.0.2">RQ2.</head><p>Based on the hypothesis that LLM-based translation and paraphrasing augmentation effectively enable multilingual bias testing, do low-resources languages have more biases than high-resources languages?</p><p>To address RQ1, we leverage In-Context Learning (ICL) capabilities of LLMs to expand the pool of languages that LangBiTe supports and systematically generate paraphrases of existing test templates. By preserving their semantic meaning, we ensure consistency when comparing different augmentation strategies (i.e., paraphrasing then translating vs. translating then paraphrasing).</p><p>To investigate RQ2, we then compare the outcomes of these augmented test templates across both high-resource and low-resource languages. By integrating automated translation and prompt augmentation, MLA-BiTe enables a broader analysis of how biases manifest in diverse linguistic contexts. This is particularly impactful in enterprise or public-sector settings, where organizations must meet multilingual obligations but lack technical or linguistic capacity to do so manually.</p><p>The contributions of our work can be summarized as follows:</p><p>1. We present MLA-BiTe, which automates the translation and augmentation of templates for testing social biases in LLMs.</p><p>2. We conduct a series of assessments to evaluate whether LLM-based translation and paraphrasing offers a reliable strategy for augmenting test templates in multiple languages (addressing RQ1), and how the ordering of paraphrasing and translation affects these outcomes.</p><p>3. We examine how low-resource languages (Catalan and Luxembourgish) compare to highresource languages (English, Spanish, French, and German) in terms of detected biases (addressing RQ2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>This section explores the limitations of current approaches in detecting biases across languages. It also provides a concise overview of LangBiTe, i.e. the target bias-testing framework which serves as a blueprint for MLA-BiTe, highlighting its utility in multilingual settings and its current shortcomings. Finally, this section briefly discusses the state of the art for augmenting datasets and generating synthetic data to support bias detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bias detection in text-to-text models</head><p>LLMs have achieved widespread popularity and are becoming pervasive for text classification, content generation, language translation, and text summarization, among many other tasks. However, because their training typically relies on large datasets derived from web crawls, they often fail to address ethical concerns and tend to mirror biases prevalent on the Internet <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8]</ref>. In this sense, the European Union AI Act <ref type="bibr" target="#b31">[32]</ref> enforces EU members to establish guidelines and procedures for developers to avoid 'discriminatory impacts and unfair biases prohibited by Union or national law' in their proprietary AI software. There are many recent research studies proposing different approaches and prompt datasets for detecting bias in text-to-text LLMs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref>. Nevertheless, most of the testing prompts are written in English, and few are targeting LLMs in other languages (e.g., <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27]</ref>). Additionally, LLMs are sensitive to prompt variations, thus using a limited set of prompts may affect the effectiveness of the evaluation <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">LangBiTe: An open-source tool to automate bias testing</head><p>LangBiTe follows a sequential process for detecting bias in text-to-text models, based on a set of ethical concerns (e.g., gender discrimination, racism) and sensitive communities that could potentially be favored or harmed (e.g., men and women, White and Black people). LangBiTe automatically: (1) selects a subset of prompt templates from a prompt library as per those ethical concerns; (2) for each prompt template, generates a test case addressing each of the sensitive communities; (3) prompts the LLMs under testing; and (4) builds reports with insights derived from the LLMs responses.</p><p>LangBiTe includes 3 curated prompt template libraries in English, Spanish and Catalan, each of which containing over 300 prompts and templates for detecting ageism, gender discrimination, LGBTQIA+phobia, political preferences, religious bias, racism, and xenophobia. Users can customize and build their own prompt template libraries. Every new template must target an ethical concern, include an optional prefix to precede the core text of the prompt, contain the text of the prompt itself, and output formatting instructions for the LLM response. Moreover, a template has an associated oracle that provides an expected valid, non-biased response from an LLM.</p><p>A template may include placeholders, in the format {&lt;COMMUNITY&gt;(&lt;NUM&gt;)?}, to be instantiated with the ethical concern's communities. The &lt;NUM&gt; part is included in templates that evaluate several sensitive communities of the same ethical concern (e.g., "{SEXUAL_ORIENTATION1} and {SEXUAL_ORIENTATION2} people should have the same civil rights").</p><p>The construction of the original English template library followed a process involving several stakeholders from different expertise backgrounds. Later, it was manually translated into Spanish and Catalan. As such, this procedure requires the participation of native speakers of the languages to be supported by LangBiTe, hindering its scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>MLA-BiTe operates exclusively on inputs provided to the underlying framework, such as the PromptTemplates employed by LangBiTe. Because its core logic is decoupled from the specific framework implementation, MLA-BiTe can readily accommodate inputs from other prompt-based bias evaluation frameworks without necessitating alterations to their internal code structures.</p><p>Specifically, within LangBiTe, translation and paraphrasing procedures are implemented at the template level, not at the individual prompt level-that is, prior to the instantiation of template placeholders with targeted communities. This choice is justified because a single template with p placeholders intended for filling from a set of n target communities can yield up to n! p!(n-p)! distinct test prompts. Performing translation and paraphrasing at the template level rather than at the prompt level significantly enhances the efficiency and scalability of the approach.</p><p>Moreover, translating and paraphrasing at the individual prompt level would result in prompts derived from the same template being syntactically divergent. This divergence would complicate the interpretation of results, making it challenging to discern whether a failed test prompt is due to variations in the ordering of community placeholders or subtle syntactic differences. By applying operations at the template level, the approach ensures that generated test prompts are syntactically uniform, thereby enhancing the comparability and interpretability of the evaluation outcomes.</p><p>Algorithm 1 outlines the overall workflow of MLA-BiTe. The tool takes as input a list of PromptTemplates (P T ), an LLM that acts as both translator and paraphraser, the set of target languages L for translating the original P T , and the desired number of paraphrases P for each translation. It is worth noting that separate LLMs could be used for translation and paraphrasing. However, for simplicity, this work assumes the use of a single LLM for both tasks.</p><p>Initially, the translator is set up using the LLM , and the paraphraser is configured with the same LLM , along with the specified number of desired paraphrases P (lines 1-2). The list of generated PromptTemplates, GP T , is initialized as empty (line 3). Next, each pt in P T is translated by the translator into each language in L (line 5). The translated output, transl_pt, is then paraphrased P times using the paraphraser (line 6). Please refer to Section 4.6 for additional information regarding the choice of this pipeline. Finally, the newly generated PromptTemplates are appended to GP T (line 7).</p><p>It is important to note that if L is empty, meaning no translation is needed, transl_pt will be identical to pt. Similarly, if no augmentation is required (i.e., P = 0, paraph_pt will be the same as transl_pt.</p><p>Section 3.1 and Section 3.2 provide additional details for, respectively, the translation and paraphrasing steps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Translation</head><p>Algorithm 2 describes in detail the translation step. First, the output dictionary, transl_pt, is initialized (line 1). Then, the translation into each l of L is treated independently (line 2-9). The template is the first to be translated (line 3). The prompt used for the translation is reported and described in section 8.</p><p>The next step is to initialize an auxiliary component of the translator, the affixTranslator (line 4). As outlined in <ref type="bibr" target="#b18">[19]</ref>, templates can be preceded by a prefix and followed by a suffix, which encapsulate the text provided to the LLM and help specify the expected output. The affixTranslator is responsible for translating these elements to align with the language of the template. Since neither the prefix nor the suffix possesses unique features or placeholders, the affixTranslator is tasked with performing a straightforward translation -also with the recommendation of ensuring the precise semantic meaning is preserved (line 5-6).</p><p>Prefixes and suffixes are often consistent across multiple templates. To optimize efficiency, the affixTranslator does not translate them repeatedly. Instead, it checks for an existing dictionary mapping translations from the original language to the target language. If the entry is found, it applies the stored translation; if not, it generates the translation, adds it to the dictionary, and reuses it as needed.</p><p>This approach reduces costs-specifically by avoiding redundant inference for the same task-and enhances consistency in the output for templates that share identical affixes in the original language. paraph_pt[l] ← create_pts(tpt, paraphs) 7: end for It is to be noted that given the limited number of prefixes and suffixes, this dictionary could be populated manually. However, for users defining new affixes in one (or few) language for their tests, this method provides a way to further automate the process.</p><p>Another component of the translator, expectedValueTranslator, is responsible for translating the expected values. It takes as input a dictionary of expected values and translates each entry (line 7-8). Similar to the affixTranslator, this process is not performed repeatedly; instead, it verifies if translations already exist and reuses them when available.</p><p>Finally, a list including the translated prefix t_prefix, the translated template t_template, the translated suffix t_suffix and the translated expected values t_expVal is added as value to the key l in transl_pt (line 10).</p><p>It is important to note that each model output is filtered using regular expressions to remove unwanted text before the translation, such as "The translation is ...". After initial testing and several trial-and-error iterations on the LLMs considered in this work, we developed a set of regular expressions that process the outputs with near-perfect accuracy (&gt;98%). However, it is important to note that each LLM generates responses in slightly different formats. Therefore, additional work will be required to accommodate further LLMs-particularly reasoning models, which often append their reasoning process to the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Paraphrasing</head><p>Algorithm 3 describes the paraphrasing step. First, the output dictionary, paraph_pt, is initialized (line 1). Each translated PromptTemplate is then evaluated independently, by iterating on each key l and value tpt of transl_pt (line 2-7). The template is extracted from the tpt, since it is the only field to be paraphrased (line 3). Then, its grammatical number is identified by the paraphraser (line 4).</p><p>The grammatical number refers to the grammatical feature indicating whether a noun, verb, or sentence pertains to a singular or plural subject. Through preliminary testing, we observed that despite employing best practices for prompting to constrain the LLM to follow the grammatical number of the original sentence, some paraphrases would deviate. For instance, given the template "Are {GENDER1} better than {GENDER2}?", the model might produce a paraphrase such as "Is a {GENDER1} better than a {GENDER2}?". When applied in LangBiTe with community-specific placeholders like men and women, this paraphrase results in grammatically incorrect test prompts, such as "Is a men better than a women?".</p><p>Empirical testing across multiple LLMs revealed that explicitly identifying and specifying the grammatical number within the prompt leads to significantly more robust paraphrasing results compared to general instructions like "preserve the grammatical number of the original sentence".</p><p>The template, the grammatical number gn, and P are subsequently passed to the paraphraser in the prompt used to produce the paraphrases (line 5), which is reported and described in Section 3.2.</p><p>Finally, the paraphrases (paraphs) generated by the model are utilized to create new Prompt-Templates specific to the language l (line 6). In particular, P PromptTemplates are created, each corresponding to a paraphrase that serves as the template, while the remaining fields, such as the prefix, expectedValue, etc., are directly copied from the original PromptTemplate.</p><p>Similarly to the translation process, each model output is filtered using regular expressions to remove unwanted text before and after the desired output format, which has been omitted from the prompt above for brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment setup and preliminary results</head><p>In this section, we describe the evaluation setup addressing RQ1, which focuses on assessing whether LLM-based translation and paraphrasing can effectively augment test templates across multiple languages, and which ordering of these steps yields the most reliable expansions. This includes a preliminary evaluation phase to select the most suitable LLM and configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>The implementation of MLA-BiTe was carried out using Python 3.11. Four non-reasoning state-ofthe-art LLMs were queried via different APIs. Details of the employed LLMs and their respective APIs are provided in Table <ref type="table" target="#tab_2">1</ref>. All tests were conducted from 5 to 7 February 2025, using the most up-to-date version of each model available at that time. We set the temperature to 1 for all models, striking a balance between creativity and predictability. This configuration allows the models to generate a diverse range of translations and paraphrases while maintaining coherence and reliability. All other parameters were left at their default values to ensure consistency across experiments.</p><p>The initial step involves identifying the most suitable model for translating and paraphrasing the templates. This selection was based on preliminary tests, the details of which are provided in Section 4.5 and Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Test set</head><p>All tests were conducted using the test cases published on the LangBiTe GitHub repository <ref type="bibr" target="#b17">[18]</ref>, specifically those covering the sensitive categories/concerns: Ageism, Lgbtiqphobia, Politics, Racism, Religion, Sexism, and Xenophobia. The concern labeled Sexual ambiguity, available only in English, was excluded from the evaluation. This concern relies on linguistic constructs that are not directly translatable or meaningful in many other languages-such as third-person singular pronouns with ambiguous gender.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model selection: translation preliminary tests</head><p>The translation evaluation was conducted on the candidate LLMs presented in Section 4.1 by testing their performance in translating a subset of test cases in English, Spanish, and Catalan published on the LangBiTe GitHub repository. Specifically, 20% of the templates were randomly sampled from the Spanish test cases (i.e., 61 test cases), and the corresponding test cases (identified by their IDs) were later retrieved for the other two languages. We then translated each test case from one of the three languages into the other two, resulting in a total of six distinct translations per test case.</p><p>The primary evaluation metric is the number of successful translations -defined as instances where the LLM followed the instruction and the correct translation was extracted from its response. Table <ref type="table" target="#tab_3">2</ref> presents the percentage of successful translations for each model. GPT-4o and Gemini 1.5 Flash produced translations in all tested cases. In contrast, Llama 3 405B failed to generate translations for a few instances, while Claude 3.5 exhibited nearly 10% non-compliance. Furthermore, we conducted an evaluation to compare the quality of the machine-generated translations against the human-translated versions of the test cases. To ensure a thorough evaluation, we employed two complementary metrics. The first metric, cosine similarity is used to assess the semantic alignment between two translations, capturing the extent to which the meaning conveyed by the machine translation aligns with that of the human reference. This metric ranges from -1 (completely dissimilar) to 1 (perfectly similar) <ref type="bibr" target="#b27">[28]</ref>. Please note that cosine similarity is calculated based on the embeddings generated from the human-translated version and the LLM-translated version. To produce these embeddings, we used paraphrase-multilingual-mpnet-base-v2, a sentence transformer available on Hugging Face that specializes in generating multilingual semantic embeddings <ref type="bibr" target="#b24">[25]</ref>.</p><p>The second metric, the Bilingual Evaluation Understudy (BLEU) score, evaluates the quality of machine translation by comparing n-grams of the candidate translation against one or more reference translations. The BLEU score ranges from 0 to 1, where 0 indicates no overlap between the candidate and reference translations, and 1 indicates a perfect match. In our case, a lower BLEU score is actually preferred, as it implies that the paraphrases are structurally different from the original -which is desirable for evaluating robustness, as long as the semantic meaning is preserved.</p><p>Given the primary focus of this work on semantic similarity, cosine similarity plays a critical role. The preservation of the core meaning in each test case is essential to ensure alignment with the user-defined ground truth -i.e., the expected results as defined by LangBiTe -and to support a robust evaluation. The results are shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>The figure demonstrates that the performance of all the evaluated models is relatively similar, GPT-4o achieving the highest scores on average. Additionally, it is noteworthy that the bidirectional translation between Spanish and Catalan consistently outperforms translations involving other language pairs, indicating a higher level of linguistic alignment or model optimization for this specific pair. This trend highlights the importance of considering language-specific characteristics and potential model fine-tuning for related languages in evaluating translation tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model selection: augmentation preliminary tests</head><p>As detailed in Section 3.2, all paraphrases for a single test case are generated using a single prompt to encourage variety. The paraphrasing process is therefore influenced by the number of paraphrases requested, with a higher number requiring the model to exhibit greater creativity to ensure diversity while maintaining the semantic meaning and format of the original template.</p><p>To assess this, we evaluated the LLMs on the paraphrasing task under three configurations: P =2, P =5 and P =10 paraphrases. Each paraphrased template was compared to the original template using cosine similarity and BLEU. Figure <ref type="figure" target="#fig_2">2</ref> shows the aggregated results, representing the average results across all three languages for this evaluation. Further detailed language-specific results are discussed in the Appendix.  The results indicate that the size of P does not significantly influence the syntactic or semantic proximity of the paraphrases to the original template. With an average cosine similarity ranging from 0.85 to 0.95, the models highly preserve the original semantic meaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Model selection</head><p>Based on the results presented in Section 4.3 and Section 4.4, GPT-4o was selected for translation and paraphrasing in the main tests presented in Section 5. This choice was motivated by its reliable instruction-following and, although it did not achieve the highest performance on every paraphrasing and translation task, the model yielded the best average results, particularly when emphasizing cosine similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Pipeline selection</head><p>After selecting the model for translation and paraphrasing, the final step before conducting the main experiments is to determine the optimal order of paraphrasing and translation, as this will influence the quality of the final output. In this regard, we consider the bidirectional translation between English (EN) and Spanish (ES), and Spanish and Catalan (CA), with the number of paraphrases P =5. In the paraphrasing-to-translation pipeline (P2T ), we utilize the paraphrase results RP previously collected and outlined in Section 4.4, translating them into the target language. For the translation-to-paraphrasing pipeline (T2P), we select a subset of the translations previously gathered and detailed in Section 4.3, ensuring they correspond to the same templates used to generate RP .</p><p>To assess the optimal order of the pipeline, we employ the methodology described in Section 4.3 and Section 4.4. Specifically, we calculate the cosine similarity between each sentence generated by the pipeline and its corresponding human-written input. The results of this evaluation are illustrated in Figure <ref type="figure" target="#fig_3">3</ref>, which presents boxplots summarizing the distribution of cosine similarity scores.</p><p>As evident from Figure <ref type="figure" target="#fig_3">3</ref>, the P2T pipeline exhibits a marginally higher median cosine similarity when the translation direction is from English to Spanish or from Spanish to Catalan. Conversely, the T2P pipeline slightly outperforms P2T in both reverse cases.</p><p>These findings suggest that the order of translation and paraphrasing has a negligible impact on the overall output quality. For the purposes of this study, we have opted to use the T2P pipeline for the main evaluation. However, further investigation is required to generalize these conclusions and explore potential nuances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Main performance evaluation</head><p>In Section 4, we addressed RQ1, demonstrating that LLM-based translation and paraphrasing effectively augment bias-testing templates. We also observed that applying paraphrasing before translation yields slightly better results than the reverse. In this section, we address RQ2, namely whether low-resource languages exhibit more bias than high-resource languages when tested with augmented multilingual templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Language selection</head><p>In this work, we focus on two major Indo-European language families, specifically the Romance and West Germanic families. In particular, we select six languages including both high-and low-resource languages, for their linguistic diversity, geographic coverage, and availability of ground truth data:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Romance languages:</head><p>• Spanish (ES): A high-resource language mainly spoken in Spain and numerous countries in Latin and South America. Ground truth data for Spanish is available from the original LangBiTe study.</p><p>• Catalan (CA): A low-resource language spoken in Eastern Spain and Andorra, for which ground truth is also available.</p><p>• French (FR): The former lingua franca, mainly spoken in numerous countries in Western Europe and in Western and Central Africa, as well as in Eastern Canada. We will use it for cross-validation of Romance language results.</p><p>West Germanic languages:</p><p>• English (EN): The current lingua franca in many domains and the dominant language for most language models. Ground truth data is available from the original LangBiTe study.</p><p>• German (DE): A high-resource language mainly spoken in Germany, Austria, Switzerland, and Luxembourg.</p><p>• Luxembourgish (LB): A low-resource language spoken in Luxembourg closely related to German, which helps to cross-validate the findings for Catalan on low resource languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Evaluation</head><p>The set of templates described in Section 4.2 was used for the main experiment. English served as the source language, from which the test cases were translated into the target languages. For the paraphrasing component, we set the number of variations to P = 1. The communities analyzed in this study are the same as those considered in <ref type="bibr" target="#b18">[19]</ref>. Figure <ref type="figure" target="#fig_5">4</ref> presents a series of spider (radar) plots illustrating each LLM's performance across the sensitive categories for each language included in this study. Hereafter, we define each unique concern-language combination as a test batch. Within each plot, the radial axes represent the percentage of tests passed by a given model for a particular test batch, thus enabling a direct comparison of how effectively different LLMs handle sensitive content.</p><p>Note that these results reflect only tests for which valid and interpretable answers were obtained. Although the framework allows up to three retries per test, some responses remained unprocessable. As described in <ref type="bibr" target="#b18">[19]</ref>, LangBiTe evaluates answers by searching for predefined, case-specific keywords and includes templates requiring structured responses (e.g., in JSON). However, not all AI models consistently follow such formatting instructions; some produce outputs that deviate from the requested structure, possibly due to limitations in their training or insufficient understanding of the formatting constraints. Such unprocessable answers are discarded from the final evaluation.</p><p>Overall, 64.3% of test batches experienced zero processing failures, and 21.4% showed failure rates of 10% or less. The remaining 14.3% of test batches exhibited failure rates above 10%. A detailed list of encountered errors is provided in Section 8.</p><p>Several noteworthy observations emerge from Figure <ref type="figure" target="#fig_5">4</ref>. First, English and Spanish consistently yield the highest or most stable scores across the bias categories, irrespective of the model. This finding aligns with earlier results indicating that widely used languages with substantial training corpora tend to produce more accurate automated bias-detection outcomes. By contrast, Catalan and Luxembourgish exhibit greater variability in categories such as Politics and Racism, likely because smaller or lower-resource languages contain sparser training data that may limit the models' ability to handle culturally specific terms and nuances.  The models themselves also vary in their performance. GPT-4o generally achieves high scores across most categories-particularly Ageism, Sexism, and Xenophobia-indicating strong coverage of related keywords and contexts. Gemini 1.5 Flash often excels in Religion and Lgbtiqphobia, suggesting it can effectively capture nuanced expressions of bias across languages in these domains. Meanwhile, Claude 3.5 Sonnet typically maintains moderate to high consistency in Sexism and Racism across multiple languages but sometimes fluctuates in Politics, reflecting challenges associated with localized political terminology. Llama3 405B demonstrates comparatively mixed results: it excels in certain instances of Racism and Ageism, yet may underperform in categories such as Politics or Xenophobia for lower-resource languages.</p><p>For categories like Lgbtiqphobia and Xenophobia, all four LLMs exhibit relatively high detection rates in most languages. This consistency may stem from the more universal nature of terms referring to LGBTIQ+ identities or xenophobic attitudes. By contrast, Politics emerges as the most variable concern, with each model showing inconsistencies across different languages.</p><p>Similarly, Sexism and Ageism produce mid-range consistency across models, suggesting that while many overtly disparaging or discriminatory terms are well covered, subtler connotations may elude straightforward keyword matching or demand deeper contextual understanding. Lastly, Religion tends to be comparatively stable across both languages and models, presumably due to shared or borrowed religious terminology and the availability of well-established keywords that more readily transfer from English prompts to other languages.</p><p>Figure <ref type="figure" target="#fig_7">5</ref> aggregates the results shown in Figure <ref type="figure" target="#fig_5">4</ref> by language and model, alongside the mean outcomes. As depicted, Llama3 405B is the most biased LLM overall, while GPT-4o and Claude 3.5 Sonnet exhibit the strongest overall performance, with scores around 75%. Regarding  performance by language, models generally perform best on high-resource languages, achieving their highest average scores in English, and appear to exhibit more social biases when tested on lower-resource languages. Notably, Luxembourgish stands out as the language with the highest discrimination rates overall. GPT-4o on Catalan, however, is an outlier, achieving the second-best score among all language-model pairs. Nevertheless, because GPT-4o was chosen as the translation and paraphrasing model according to the results reported in Section 4, its output may provide GPT-4o with a slight advantage in the bias-detection task. Further work is required to evaluate this potential effect. Given the variance observed in Figure <ref type="figure" target="#fig_5">4</ref> across different bias categories, it is also evident that choosing an LLM may require a case-by-case approach. Individual models can exhibit strong performance in some categories while underperforming in others, especially when targeting localized cultural or linguistic nuances. Hence, a nuanced selection process that accounts for both language and bias category may be necessary to optimize bias detection and mitigation.</p><p>In conclusion, and in direct response to RQ2, these findings suggest that LLMs exhibit higher social biases when data augmentation is performed for low-resource languages. Nonetheless, the particular model best suited for each task may vary depending on the specific bias category and language under consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this section, we complement the results presented in section 5 by conducting a Pearson correlation analysis on the performance of the same model/concern pairs across different languages. This analysis highlights both common patterns and divergences in behavior across languages. The outcomes, depicted in Figure <ref type="figure" target="#fig_8">6</ref>, reveal that, contrary to initial expectations, LLMs do not consistently exhibit comparable biases in linguistically related languages. For instance, while German and English (both West-Germanic languages) display the highest performance similarity across all language comparisons, the biases observed in Luxembourgish are more closely aligned with those detected in Spanish and Catalan than with German or English.</p><p>A more granular examination of individual bias dimensions (see Figure <ref type="figure" target="#fig_5">4</ref>) further underscores these unexpected findings. Notably, LLMs display marked performance variations across several categories of bias, including ageism, Lgbtiqphobia, racism, and sexism. For example, GPT-4o performs comparatively poorly in the racism category for Catalan and French, whereas Gemini 1.5  From Figure <ref type="figure" target="#fig_5">4</ref> it also emerges that political bias is a notable outlier to our observations. In evaluating the political bias of language models, it is essential to highlight the limitations of LangBiTe's default template library, and their obtained paraphrases, particularly when the queries are predominantly centered around U.S politics and require a neutral stance. What we see in Figure <ref type="figure" target="#fig_5">4</ref> is that most models take an ideological side when prompted about U.S. political issues, whereas the oracles expect no positioning at all. Nevertheless, while LangBiTe's templates provide valuable insights into U.S.-related political leanings from generative AI models, they may not fully capture the differences and complex nuances of political discourse in other countries and languages. Political ideologies and the framing of society matters can vary significantly across diverse national or regional contexts. In addition, political ideologies and stances tend to evolve over time and are generally too complex to be placed on a one-dimensional spectrum <ref type="bibr" target="#b14">[15]</ref>. Consequently, results derived from an American-centric dataset might not offer a comprehensive assessment of a model's potential bias on a global scale.</p><p>As mentioned in Section 5, not all LLMs duly follow LangBiTe's formatting instructions, with some deviating from the required structure. This leads to computing errors, since the output may not be correctly interpreted-not even by the LLM-as-judge. Such structured output formatting instructions are included in templates that ask for probabilities of particular aspects, events, or traits for different sensitive communities. Most of these templates are targeting sexism (42 out of 65 templates) and racism (46 out of 98), leading to a higher number of errors in evaluating these ethical concerns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future work</head><p>In this work, we have tested MLA-BiTe on four LLMs across six languages. Future work includes:</p><p>1) Expanding the Evaluation to More LLMs: We aim to include additional LLMs in our evaluation, specifically to analyze how performance varies with model size.</p><p>2) Extending Language Coverage: As discussed in Section 5.1, the languages used in this study belong to European families. Future work will extend the evaluation to extra-European languages, with a focus on low-resource languages. This poses additional challenges, as many of these languages exhibit linguistic characteristics that differ significantly from those of Indo-European languages, such as complex systems of grammatical number, noun class, or verb morphology. These feature may require tailored strategies for reliable evaluation.</p><p>3) Integrating Image Generation Capabilities: We plan to extend the framework to cover image generation. In this context, multilingual, augmented prompts could be used to produce images through ImageBiTe <ref type="bibr" target="#b20">[21]</ref>. This extension would allow us to investigate how the distribution of generated images varies according to the language in which the prompt is formulated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Enhancing Answer Processing and Evaluation:</head><p>We also aim to identify strategies to improve the processing of LLM-generated answers. In particular, we plan to strengthen the LLM-as-a-judge component to reduce the number of unprocessed executions and improve the robustness of the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5)</head><p>Exploring Cultural-Aware Translation: Lastly, we aim to investigate translation strategies that respect cultural norms and values specific to the target language and society. For instance, prompts or examples involving food may need to avoid certain ingredients depending on cultural or religious context. Such strategies could help mitigate risks of offending or alienating different user groups, ensuring that automated translations remain both accurate and respectful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This study introduced MLA-BiTe, a framework that improves prior bias evaluation methods by enabling systematic multilingual bias testing. MLA-BiTe leverages automated translation and paraphrasing techniques to support comprehensive assessments across diverse linguistic settings. For this study, we adapted the framework to generate input templates compatible with the Lang-BiTe framework <ref type="bibr" target="#b19">[20]</ref>, which we subsequently used to validate our method. Under this setting, we tested MLA-BiTe on a representative set of both high-resource languages (e.g., English, Spanish, French, German) and low-resource languages (e.g., Catalan, Luxembourgish). These languages were selected to encompass a range of linguistic characteristics and resource availability; however, they do not represent the full extent of languages supported by the framework. Our first research question concerned whether LLM-based translation and paraphrasing methods can effectively augment bias-testing templates. We found that they enhance the overall comprehensiveness of multilingual bias evaluation, with the strategy of paraphrasing before translation delivering marginally better outcomes.</p><p>Our second research question focused on whether low-resource languages exhibit higher degrees of bias compared to high-resource languages. Our performance evaluation reveals that, indeed, LLMs generally attain higher and more stable bias-detection scores in languages with extensive training data. In contrast, lower-resource languages display greater variability, particularly for nuanced bias categories like Politics and Racism, corroborating prior work suggesting that richer training corpora often lead to more consistent results across bias domains.</p><p>Aggregated findings indicate that some models demonstrate robust performance in most categories, whereas others show variability, highlighting how model architecture and training data composition can influence biases. Moreover, correlation analyses found no clear pattern of parallel bias trends among linguistically similar languages, suggesting that cross-linguistic bias transfer is more complex than simple language-family groupings might imply.</p><p>In summary, translation and paraphrasing substantially bolster bias-detection robustness in multilingual contexts, and lower-resource languages remain more prone to biases. Nonetheless, individual results depend heavily on which model-language pair and bias category are being considered. Consequently, selecting an LLM for bias-detection tasks should be approached on a case-by-case basis.</p><p>Future work will expand both model and language coverage and investigate applications in other domains, including bias evaluation in image-generation systems. Additional research might further explore and cross-modality approaches to address the nuanced challenges posed by low-resource languages and complex bias categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices Translation prompt</head><p>The following prompt is used for translating automatically the templates as described in Section 3.1.</p><p>You must provide a translation in {language} of the following sentence: "{sentence}" It is CRITICAL to maintain the exact semantic meaning. If there are placeholders in the format {{PLACEHOLDER}}, it is CRITICAL not to translate them. If the sentence is a yes/no question, the translation must also be. If the sentence involves probabilities, the translation must also.</p><p>In the prompt, sentence=template and language=l. The prompt template was developed through multiple iterations to ensure clarity, precision, and minimal ambiguity. Short, explicit instructions written in an imperative tone were chosen to guide the model effectively. The use of capitalized "CRITICAL" emphasizes the importance of semantic accuracy, leveraging the model's training to recognize capitalization as a signal for urgency or emphasis.</p><p>Additional instructions are included to address specific cases. For instance, placeholders in the format {{PLACEHOLDER}} are explicitly marked as non-translatable to maintain consistency with LangBiTe's standardized community markups in English. Similarly, binary questions are explicitly required to retain their structure, as the expected output is designed to correspond to the binary format. The same principle is extended to instructions for sentences involving probabilities, ensuring that the translated sentence mirrors the semantic and structural nuances of the original. These specifications are seamlessly integrated into the template for consistent and context-aware translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paraphrasing prompt</head><p>The following prompt is used for paraphrasing automatically the templates as described in section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>You must provide exactly {n_paraphrases} different paraphrases of the following sentence: "sentence"</head><p>It is CRITICAL to maintain the same language as the sentence. It is CRITICAL to make the paraphrase as {grammar_number}. It is CRITICAL to maintain the exact semantic meaning, as well as all the placeholders in the format {{PLACEHOLDER}}. If the sentence is a yes/no question, the paraphrase must also be. If the sentence involves probabilities, the paraphrase must also.</p><p>In the prompt, sentence=template, grammar_number=gn and n_paraphrases=P . The prompt was crafted through iterative refinement to ensure precision and minimal ambiguity, similar to the translation prompt. Additional instructions address essential aspects of paraphrasing. The requirement to maintain the same language as the input ensures linguistic consistency, while the specification to paraphrase as {grammar_number} reinforces grammatical alignment. For placeholders in the format {{PLACEHOLDER}}, we follow the same strategy as in the translation prompt to ensure they are preserved and not modified. Similarly, for sentences involving probabilities or binary questions, the same approach as in the translation prompt is applied.</p><p>It is to be noted that asking the LLM to generate all paraphrases in one prompt encourages it to seek variety, as the model understands it is being asked for multiple distinct outputs in one go. This can lead to more diverse paraphrasing. In contrast, iterative paraphrasing (one at a time) risks producing similar outputs, as the LLM may have less context to infer the need for variety. However, the effectiveness of these approaches can also depend on the specific LLM being used and the prompt design. Clear and explicit instructions in iterative paraphrasing might mitigate the risk of similarity, but the upfront approach generally aligns better with the goal of maximizing variety <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model selection: paraphrases evaluation</head><p>Figure <ref type="figure" target="#fig_9">7</ref> breaks down the aggregated results from Section 4.4 by language. Overall, model performance for paraphrasing appears consistent across the evaluated languages, with no clear languagespecific trends emerging-except for Claude 3.5, which consistently underperforms across all evaluations according to the BLEU metric. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unprocessable executions</head><p>Table <ref type="table" target="#tab_1">3</ref> presents the mean rate of unprocessable executions grouped by model. Answers generated by Gemini 1.5 Flash are the most reliably processed by the LangBiTe framework, while those from Llama3 405B exhibit the highest fault rate. According to Table <ref type="table" target="#tab_7">4</ref>, topics related to Racism and Sexism result in the highest processing fault rates. In contrast, answers concerning Xenophobia and Politics yield the lowest rates.</p><p>Table <ref type="table" target="#tab_8">5</ref> highlights a significant variation in performance across languages. English, used as the source language for test cases, shows the lowest fault rate. The highest rates are observed for Luxembourgish and Spanish, while Catalan has the second-lowest fault rate after English. Overall, these results suggest no clear correlation between the availability of resources for a language and the likelihood of generating answers that cannot be processed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LLM</head><p>%Unprocessable responses Claude 3.5 Sonnet 8.0 Gemini 1.5 Flash 2.9 Llama3 405B 10.5 GPT-4o 3.6 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The BLEU scores and cosine similarities for translations between each of the tested languages and the other two, as generated by the selected LLMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>0.800 0.825 0.850 0.875 0.900 0.925 0.950 0.975 10.825 0.850 0.875 0.900 0.925 0.950 0.975 10.825 0.850 0.875 0.900 0.925 0.950 0.975 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: BLEU and cosine similarities for paraphrasing across all the tested languages, with the number of paraphrases P in [2,5,10].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of cosine similarity scores for selected translations at P = 5, used to compare the performance of the two proposed pipelines, P2T and T2P.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Each spider plot illustrates the percentage of passed tests for each LLM in one of the seven sensitive categories examined in this paper, spanning all six languages analyzed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Aggregated results by language and model.</figDesc><graphic coords="14,170.59,87.63,290.64,120.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Heatmap of the Pearson Correlation of the performance achieved on the same model/concern pair across different languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Paraphrasing performance by language and variations different values of P across the evaluated models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 3</head><label>3</label><figDesc>Paraphrasing Input: paraphraser, transl_pt: translated PromptTemplate, P : number of paraphrases Output: paraph_pt: paraphrased PromptTemplate</figDesc><table /><note><p>1: paraph_pt ← {} 2: for l, tpt in transl_pt do 3: template ← tpt.get_template() 4: gn ← paraphraser.identify_grammar_n(template) 5: paraphs ← paraphraser.paraphrase(template, gn, P ) 6:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Candidate LLM</figDesc><table><row><cell>Model</cell><cell cols="2">#Parameters API</cell></row><row><cell cols="2">Claude 3.5 Sonnet Undisclosed</cell><cell>Anthropic</cell></row><row><cell>Gemini Pro 1.5</cell><cell>Undisclosed</cell><cell>Google Deepmind</cell></row><row><cell>Llama3 405b</cell><cell>405 billion</cell><cell>Replicate</cell></row><row><cell>GPT-4o</cell><cell>Undisclosed</cell><cell>OpenAI</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Successful translations made by the candidate LLMs</figDesc><table><row><cell>Model</cell><cell>%Successful translations</cell></row><row><cell cols="2">Claude 3.5 Sonnet 90.4%</cell></row><row><cell>Gemini 1.5 Flash</cell><cell>100%</cell></row><row><cell>Llama3 405b</cell><cell>99.2%</cell></row><row><cell>GPT-4o</cell><cell>100%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Percentage of unprocessable responses by LLM.</figDesc><table><row><cell>Concern</cell><cell>%Unprocessable responses</cell></row><row><cell>ageism</cell><cell>5.14</cell></row><row><cell>lgbtiqphobia</cell><cell>0.31</cell></row><row><cell>politics</cell><cell>0.07</cell></row><row><cell>racism</cell><cell>18.24</cell></row><row><cell>religion</cell><cell>5.44</cell></row><row><cell>sexism</cell><cell>14.34</cell></row><row><cell>xenophobia</cell><cell>0.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Percentage of unprocessable responses by concern.</figDesc><table><row><cell cols="2">Language %Unprocessable tests</cell></row><row><cell>CA</cell><cell>4.1</cell></row><row><cell>DE</cell><cell>5.9</cell></row><row><cell>EN</cell><cell>3.3</cell></row><row><cell>ES</cell><cell>9.2</cell></row><row><cell>FR</cell><cell>5.1</cell></row><row><cell>LU</cell><cell>9.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Percentage of unprocessable tests by language.Finally, table6shows in detail the mean percentage of unprocessable responses by test batch.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work has been partially funded by the <rs type="funder">Luxembourg National Research Fund (FNR) PEARL program</rs> (grant agreement <rs type="grantNumber">16544475</rs>); the research network <rs type="grantNumber">RED2022-134647-T</rs> and the project <rs type="grantNumber">PID2023-147592OB-I00</rs> "<rs type="projectName">SE4GenAI</rs>", both funded by <rs type="grantNumber">MCIN/AEI/10.13039/501100011033</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_WD3Ga6S">
					<idno type="grant-number">16544475</idno>
				</org>
				<org type="funding" xml:id="_cRgh6Ph">
					<idno type="grant-number">RED2022-134647-T</idno>
				</org>
				<org type="funded-project" xml:id="_uFR82XQ">
					<idno type="grant-number">PID2023-147592OB-I00</idno>
					<orgName type="project" subtype="full">SE4GenAI</orgName>
				</org>
				<org type="funding" xml:id="_wYQ7byv">
					<idno type="grant-number">MCIN/AEI/10.13039/501100011033</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using natural sentence prompts for understanding biases in language models</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Alnegheimish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2824" to="2830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluating the underlying gender bias in contextualized word embeddings</title>
		<author>
			<persName><forename type="first">Christine</forename><surname>Basta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noe</forename><surname>Casas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Gender Bias in NLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="33" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big?</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</title>
		<meeting>the 2021 ACM conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Man is to computer programmer as woman is to homemaker? debiasing word embeddings</title>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">T</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving language models by retrieving from trillions of tokens</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saffron</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loren</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albin</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michela</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><surname>Sifre</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2112.04426" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Chatgpt vs gemini vs llama on multilingual sentiment analysis</title>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Buscemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Proverbio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.01715</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Roguegpt: dis-ethical tuning transforms chatgpt4 into a rogue ai in 158 words</title>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Buscemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Proverbio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.15009</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Marked personas: Using natural language prompts to measure stereotypes in language models</title>
		<author>
			<persName><forename type="first">Myra</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">61st Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1504" to="1532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bold: dataset and metrics for measuring biases in open-ended language generation</title>
		<author>
			<persName><forename type="first">Jwala</forename><surname>Dhamala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyapriya</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAccT</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="862" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">RealTox-icityPrompts: Evaluating neural toxic degeneration in language models</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Samuel Gehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3356" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Social bias evaluation for large language models requires prompt variations</title>
		<author>
			<persName><forename type="first">Rem</forename><surname>Hida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.03129</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Measuring bias in contextualized word representations</title>
		<author>
			<persName><forename type="first">Keita</forename><surname>Kurita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nidhi</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st Workshop on Gender Bias in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="166" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">From zero to hero: On the limitations of zero-shot cross-lingual transfer with multilingual transformers</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Lauscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinit</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Glavaš</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00633</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The myth of left and right: How the political spectrum misleads and harms america</title>
		<author>
			<persName><forename type="first">Verlan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyrum</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Holistic evaluation of language models</title>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilara</forename><surname>Soylu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09110</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Holistic evaluation of language models</title>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilara</forename><surname>Soylu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yian</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09110</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><surname>Langbite</surname></persName>
		</author>
		<ptr target="https://github.com/SOM-Research/LangBiTe" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A DSL for testing LLMs for fairness and bias</title>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Clarisó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Cabot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MODELS</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">LangBiTe: A platform for testing bias in large language models</title>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Clarisó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Cabot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.18558</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ImageBiTe: A framework for evaluating representational harms in text-to-image models</title>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Clarisó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Cabot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on AI Engineering -Software Engineering for AI</title>
		<meeting>the 4th International Conference on AI Engineering -Software Engineering for AI</meeting>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note>Pending publication</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Moin</forename><surname>Nadeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Bethke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><surname>Stereoset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09456</idno>
		<title level="m">Measuring stereotypical bias in pretrained language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shailja</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><forename type="middle">Narayan</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.16430</idno>
		<title level="m">A comprehensive survey of bias in LLMs: Current landscape and future directions</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sentence-bert: Sentence embeddings using siamese bertnetworks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1908.10084" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Recipes for building an open-domain chatbot</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13637</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Jayanta</forename><surname>Sadhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesha</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Rani</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rifat</forename><surname>Shahriyar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.03536</idno>
		<title level="m">Social bias in large language models for Bangla: An empirical study on gender and religious bias</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Term-weighting approaches in automatic text retrieval</title>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Evaluating large language models with fmeval</title>
		<author>
			<persName><forename type="first">Pola</forename><surname>Schwöbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Bilal Zafar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.12872</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The woman worked as a babysitter: On biases in language generation</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3407" to="3412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</title>
		<author>
			<persName><forename type="first">Aarohi</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abu</forename><surname>Awal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Shoeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abubakar</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Adam R Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrià</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Garriga-Alonso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04615</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The artificial intelligence act</title>
		<ptr target="https://artificialintelligenceact.eu" />
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>European Union</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BiasAsker: Measuring the bias in conversational ai system</title>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinjia</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazhen</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESEC/FSE</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="515" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Ethical and social risks of harm from language models</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribeth</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conor</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myra</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borja</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atoosa</forename><surname>Kasirzadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04359</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Ethical and social risks of harm from language models</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribeth</forename><surname>Rauh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04359</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution: Evaluation and debiasing methods</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
