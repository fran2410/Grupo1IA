<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MIPHEI-VIT: MULTIPLEX IMMUNOFLUORESCENCE PREDICTION FROM H&amp;E IMAGES USING VIT FOUNDATION MODELS A PREPRINT</title>
				<funder ref="#_qTVyJ69">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_8kMZ7xU">
					<orgName type="full">JU</orgName>
				</funder>
				<funder>
					<orgName type="full">Horizon 2020</orgName>
				</funder>
				<funder>
					<orgName type="full">Sanofi. Furthermore</orgName>
				</funder>
				<funder>
					<orgName type="full">ANRT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2025-05-15">15 May 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guillaume</forename><surname>Balezo</surname></persName>
							<email>guillaume.balezo@minesparis.psl.eu</email>
						</author>
						<author>
							<persName><forename type="first">Roger</forename><surname>Trullo</surname></persName>
							<email>r.trullo@instadeep.com</email>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><forename type="middle">Pla</forename><surname>Planas</surname></persName>
							<email>albert.plaplanas@sanofi.com</email>
						</author>
						<author>
							<persName><forename type="first">Etienne</forename><surname>Decencière</surname></persName>
							<email>etienne.decenciere@minesparis.psl.eu</email>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Walter</surname></persName>
							<email>thomas.walter@minesparis.psl.eu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">InstaDeep*</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Centre de Morphologie Mathématique Mines Paris PSL Fontainebleau</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Center for Computational Biology</orgName>
								<orgName type="institution">PSL</orgName>
								<address>
									<addrLine>Mines Paris</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department" key="dep1">Institut Curie</orgName>
								<orgName type="department" key="dep2">INSERM U1331</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MIPHEI-VIT: MULTIPLEX IMMUNOFLUORESCENCE PREDICTION FROM H&amp;E IMAGES USING VIT FOUNDATION MODELS A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-05-15">15 May 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">F3FA416E37DC3266A9C5708BE4373CA6</idno>
					<idno type="arXiv">arXiv:2505.10294v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-19T11:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computer Vision</term>
					<term>Histopathology</term>
					<term>Image Translation</term>
					<term>Foundation model</term>
					<term>In silico labelling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Histopathological analysis is a cornerstone of cancer diagnosis, with Hematoxylin and Eosin (H&amp;E) staining routinely acquired for every patient to visualize cell morphology and tissue architecture. On the other hand, multiplex immunofluorescence (mIF) enables more precise cell type identification via proteomic markers, but has yet to achieve widespread clinical adoption due to cost and logistical constraints. To bridge this gap, we introduce MIPHEI (Multiplex Immunofluorescence Prediction from H&amp;E), a U-Net-inspired architecture that integrates state-of-the-art ViT foundation models as encoders to predict mIF signals from H&amp;E images. MIPHEI targets a comprehensive panel of markers spanning nuclear content, immune lineages (T cells, B cells, myeloid), epithelium, stroma, vasculature, and proliferation. We train our model using the publicly available ORION dataset of restained H&amp;E and mIF images from colorectal cancer tissue, and validate it on two independent datasets. MIPHEI achieves accurate cell-type classification from H&amp;E alone, with F1 scores of 0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20, substantially outperforming both a state-of-the-art baseline and a random classifier for most markers. Our results indicate that our model effectively captures the complex relationships between nuclear morphologies in their tissue context, as visible in H&amp;E images and molecular markers defining specific cell types. MIPHEI offers a promising step toward enabling cell-type-aware analysis of large-scale H&amp;E datasets, in view of uncovering relationships between spatial cellular organization and patient outcomes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The analysis of Hematoxylin and Eosin (H&amp;E)-stained tissue slides is a cornerstone in the diagnosis of many pathologies, including cancer, providing insights into cell types, cellular phenotypes, tissue architecture and their alterations due to disease.</p><p>Multiplex immunofluorescence (mIF) <ref type="bibr" target="#b0">Andreou et al. [2022]</ref>, <ref type="bibr" target="#b1">Tan et al. [2020]</ref>, <ref type="bibr" target="#b2">Im et al. [2019]</ref> imaging is a powerful technique that improves the analysis of tissue sections by providing detailed information beyond what H&amp;E staining can reveal. mIF achieves this by simultaneously visualizing and quantifying multiple protein markers within a single tissue section, utilizing fluorescently labeled antibodies that bind to specific proteins, allowing for the identification of cell types based on marker expression. This capability makes mIF useful across various domains, including cancer biology, immunology, and infectious disease, where understanding spatial cell organization is critical. By combining molecular and spatial information at single-cell resolution, mIF supports detailed characterization of tissue microenvironments and cellular interactions.</p><p>The evolution of immunolabeling techniques from traditional immunohistochemistry (IHC) <ref type="bibr" target="#b3">Duraiyan et al. [2012]</ref> has led to more advanced mIF techniques like PhenoCycler <ref type="bibr" target="#b4">Black et al. [2021]</ref>, which enables the detection of up to 100 markers through multiple imaging cycles, each capturing 4 channels. This iterative process can however degrade tissue integrity. Among recent advancements, the Orion scanner introduced by Lin et al. <ref type="bibr">Lin et al. [2023a]</ref> allows the simultaneous detection of up to 20 markers in a single cycle, preserving tissue quality while still providing rich multiplexed information. Orion also allows capturing high-quality, restained H&amp;E images from the same tissue section.</p><p>While mIF imaging offers several advantages, it also presents important challenges. Preparing and processing mIF slides is time-consuming and labor-intensive, requiring expensive reagents and specialized equipment, which are not always available in all clinical settings, limiting its accessibility. Compared to other techniques for spatial biology, such as Imaging Mass Cytometry (IMC) <ref type="bibr" target="#b6">Mann et al. [2001]</ref> and VisiumHD spatial transcriptomics Oliveira et al. <ref type="bibr">[2024]</ref>, mIF detects fewer molecular markers but offers much higher spatial resolution. Like these technologies, mIF is constrained by high costs, which prevents it from being adopted in clinical practice.</p><p>In contrast, H&amp;E slides are routinely generated in clinical practice. Since different cell types are characterized by distinct protein expression patterns, we hypothesize that certain markers can be predicted from cell morphology captured in H&amp;E. With the availability of high-quality datasets obtained from technologies like the ORION scanner, which captures aligned H&amp;E and mIF images with a high number of markers, we can now train AI models to infer mIF data from H&amp;E images. This would allow us to identify those proteins that are predictable from morphological cues and perform these predictions on large retrospective cohorts, including clinical trial data, for which an acquisition of mIF data would not be feasible. Relating the predicted mIF data to outcome or treatment response can then contribute to biomarker discovery and hypothesis generation in oncology.</p><p>In this study, we aim to predict the expression of key markers from H&amp;E, covering nuclear content (Hoechst), vasculature (CD31), immune populations (CD45, CD68, CD4, FOXP3, CD8a, CD45RO, CD20, PD-L1, CD3e, CD163), epithelial and stromal structures (E-cadherin, Pan-CK, SMA), and proliferation (Ki67). To achieve this, we introduce MIPHEI: Multiplex Immunofluorescence Prediction from H&amp;E-stained Whole Slide Images, a U-Net-inspired architecture integrating state-of-the-art foundation models as encoders. Unlike traditional cell type classification models that rely on manual labels or pseudo-labels, our method is trained directly on aligned mIF data, avoiding biases from predefined annotations. It is also highly modular and does therefore not rely on specific nucleus segmentation and cell classification methods.</p><p>The key contributions of this study are as follows:</p><p>• Prediction of Numerous Markers from H&amp;E: We identify the proteomic markers and cell types predictable from H&amp;E images from a wide range of 15 markers.</p><p>• Rigorous Validation and Reproducibility: We demonstrate generalization of the method through validation with cell-level metrics for accurate cell type identification on both internal and two external datasets, moving beyond pixel-level evaluation. Our study sets a new benchmark framework for H&amp;E to mIF translation.</p><p>• New State-of-the-art: Our model outperforms previous models and a random baseline on 15 markers, most of which are indiscernible to pathologists.</p><p>• ViT Foundation Models in U-Net Architecture: one major methodological novelty relies in the integration of Vision Transformer (ViT) <ref type="bibr" target="#b8">Dosovitskiy et al. [2020]</ref> foundation models as the backbone in a U-Net architecture for histopathological image translation, leveraging advanced representation learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Virtual Staining</head><p>Image-to-image translation involves transforming images from one domain to another, with Pix2Pix <ref type="bibr" target="#b10">Isola et al. [2017]</ref> being one of the most well-known methods in this domain for paired images. Utilizing conditional adversarial networks, these techniques enable high-quality image synthesis, supporting tasks such as style transfer, super-resolution, and domain adaptation, and have driven significant advancements across various applications, as demonstrated by <ref type="bibr" target="#b11">Wang et al. Wang et al. [2018]</ref>.</p><p>In recent years, image-to-image translation techniques have become increasingly popular for life science applications, such as predicting super-resolved microscopy images <ref type="bibr" target="#b12">Wang et al. [2019]</ref> or bright-field-like images from holographic images <ref type="bibr" target="#b13">Wu et al. [2019]</ref>. In histopathology, such virtual staining techniques have been used to predict immunohistochemistry (IHC) data <ref type="bibr" target="#b14">Sun et al. [2023]</ref>, <ref type="bibr" target="#b15">DoanNgan et al. [2022]</ref>, to do virtual multiplexing <ref type="bibr" target="#b16">Wu et al. [2023]</ref>, <ref type="bibr" target="#b17">Bao et al. [2021]</ref>, to predict mIF images from IHC images <ref type="bibr" target="#b18">Ghahremani et al. [2022]</ref>, or both H&amp;E <ref type="bibr">Rivenson et al. [2019]</ref>, <ref type="bibr" target="#b20">Bai et al. [2023]</ref>, <ref type="bibr" target="#b21">Zhang et al. [2020]</ref>, <ref type="bibr" target="#b22">Cao et al. [2023]</ref> and mIF <ref type="bibr" target="#b23">Christiansen et al. [2018]</ref> from unlabeled autofluorescence images.</p><p>In the scope of this study, the most relevant work involves translating H&amp;E images to mIF, which directly addresses the challenge of performing cell type calling from standard histological stains. A notable example is <ref type="bibr">SHIFT Burlingame et al. [2020]</ref> based on conditional generative adversarial network (cGAN) to predict markers like DAPI, pan-cytokeratin, and α-smooth muscle actin. Another relevant work is ImmunoAIzer <ref type="bibr" target="#b25">Bian et al. [2021]</ref> using a semi-supervised adversarial approach to predict CD3, CD20, PanCK, and DAPI channels from H&amp;E, trained on both aligned and unaligned data. More recently, the same author published <ref type="bibr">HEMIT Bian et al. [2024]</ref>, releasing a dataset and a method designed for translating H&amp;E to mIHC images targeting DAPI, CD3, and pan-cytokeratin (Pan-CK) markers. The authors propose a hybrid ViT-CNN generator architecture combining CNNs with Swin Transformers <ref type="bibr" target="#b27">Liu et al. [2021]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ViT on Dense Prediction Tasks</head><p>ViTs <ref type="bibr" target="#b8">Dosovitskiy et al. [2020]</ref> are now widely used in image classification and often outperform CNNs when pretrained with advanced contrastive self-supervised learning (SSL) methods such as MoCov3 <ref type="bibr" target="#b28">Chen et al. [2021]</ref>, iBOT <ref type="bibr" target="#b29">Zhou et al. [2021], and</ref><ref type="bibr">DINOv2 Oquab et al. [2023]</ref>. These SSL approaches have been adapted to histopathology, enabling recent foundation models to achieve strong and robust performance across diverse downstream tasks. CTransPath <ref type="bibr" target="#b31">Wang et al. [2022]</ref> uses MoCov3 <ref type="bibr" target="#b28">Chen et al. [2021]</ref> with a Swin Transfomer architecture <ref type="bibr" target="#b27">Liu et al. [2021]</ref>, and was trained on 15.6 million tiles from opensource datasets. UNIv2 <ref type="bibr" target="#b32">Chen et al. [2024]</ref> leverages DINOv2 to train a ViT-H/14 model on 200 million tiles from 3.5k proprietary H&amp;E and IHC slides. H-optimus-0 <ref type="bibr">Saillard et al. [2024]</ref> is a ViT-G/14 variant with SSL pretraining based on iBOT <ref type="bibr" target="#b29">Zhou et al. [2021]</ref> and DINOv2 <ref type="bibr" target="#b30">Oquab et al. [2023]</ref>, trained on hundreds of millions of tiles from 500,000+ H&amp;E whole slide images. These models are among the state-of-the-art in computational pathology, with DINOv2-based models like UNIv2 and H-optimus-0 outperforming ImageNet-pretrained encoders and CTransPath on unseen datasets. While plain ViTs are effective for image-level tasks, they often struggle with dense prediction tasks like image translation due to limited ability to capture fine local details, unlike CNNs which excel at leveraging local continuity and multiscale features. To overcome this, hybrid CNN-ViT architectures have been proposed. <ref type="bibr">CellViT Hörst et al. [2024]</ref>, for example, uses the UNETR architecture <ref type="bibr" target="#b35">Hatamizadeh et al. [2022]</ref>, which integrates a ViT encoder into a U-Net-like design, employing convolutional transpose blocks to create hierarchical features, offering a simple adaptation. ViTMatte <ref type="bibr" target="#b36">Yao et al. [2024]</ref> combines a plain ViT with a convolutional module for pyramidal feature extraction and detail refinement, using ViT token features only as the bottleneck. More advanced models like ViT-Adapter <ref type="bibr" target="#b37">Chen et al. [2022]</ref> and <ref type="bibr">ViT-CoMer Xia et al. [2024]</ref> go further by enabling richer multi-scale interactions between ViT and CNN components, but at higher computational cost. While these hybrid methods have mainly been applied to segmentation, we hypothesized they could also benefit dense image translation tasks like mIF prediction, especially when using foundation models trained on large, diverse datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>We present the datasets used in this study, with Figure <ref type="figure" target="#fig_2">3</ref> providing additional details on tissue distribution, domain shifts, and dataset composition. The preprocessed data for ORION and HEMIT datasets is available on Zenodo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Orion CRC Dataset (Train/Val/Test)</head><p>The ORION colorectal cancer (CRC) dataset <ref type="bibr">Lin et al. [2023b,a]</ref> was acquired with a novel system capturing 18-channel immunofluorescence (IF) images alongside H&amp;E staining on the same tissue samples. This dataset includes 41 Whole Slides Images (WSIs) with both H&amp;E and mIF data. The 15 markers (with additional DNA stain) of interest for this study are: Hoechst, CD31, CD45, CD68, CD4, FOXP3, CD8a, CD45RO, CD20, PD-L1, CD3e, CD163, E-cadherin, Pan-CK, SMA and Ki67. The PD-1 channel was not used due to poor signal quality. The H&amp;E images were acquired with an Aperio GT450 microscope (Leica Biosystems) and registered to the IF images in the original study, at a resolution of 0.325 microns per pixel (mpp).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HEMIT Dataset (Test)</head><p>The HEMIT dataset <ref type="bibr" target="#b26">Bian et al. [2024]</ref> consists of H&amp;E and multiplex-immunohistochemistry (mIHC) images the same tissue sections, acquired using Mantra system scanner (PerkinElmer, Waltham, MA, USA) with DAPI, CD3, and Pan-CK markers. It includes 5,292 paired 1024×1024-pixel patches aligned at pixel level at 40× magnification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">IMMUcan CRC Dataset (Test)</head><p>IMMUcan (Integrated iMMUnoprofiling of large adaptive CANcer patient cohorts) Hong et al. <ref type="bibr">[2020]</ref> is a European initiative launched in 2019 to advance Tumor Micro Environment (TME) profiling. We use only the CRC cases, comprising 35 registered H&amp;E and mIF slides from two cohorts, which include both advanced and less severe stages. H&amp;E slides were scanned with a Hamamatsu NanoZoomer 2.0-HT (0.5 mpp), and mIF images were acquired on a PerkinElmer Vectra Polaris. The mIF panel includes DAPI, CD3, CD8, CD4, FOXP3, and Pan-CK. Unlike HEMIT and ORION, the H&amp;E and mIF slides in IMMUcan come from consecutive sections, requiring a dedicated evaluation protocol detailed later in Metric Overview. Although currently private, the dataset is expected to become publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Data Preprocessing</head><p>We designed a preprocessing pipeline tailored to our H&amp;E-to-mIF prediction task, addressing WSI registration, artifact removal, autofluorescence subtraction with normalization, and pseudo-label extraction, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">H&amp;E to mIF registration on consecutive cuts</head><p>Accurate registration between H&amp;E and mIF images is crucial for training and evaluating our models. For Orion and HEMIT datasets, data was already registered <ref type="bibr">Lin et al. [2023b]</ref>, <ref type="bibr" target="#b26">Bian et al. [2024]</ref>. For the IMMUcan dataset, we used Valis Gatenbee et al. <ref type="bibr">[2023]</ref> for registration of consecutive H&amp;E and mIF slides.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Tile Selection</head><p>To reduce artifacts from restaining and acquisition in the ORION dataset, we filtered aligned H&amp;E and mIF tiles using several quality control steps. We used thresholding on an empty mIF channel-without antibody staining but capturing shared noise-to identify artifacts affecting all mIF channels. Poor H&amp;E quality tiles were identified by clustering H-optimus-0 embeddings and discarding clusters associated to obvious artifacts. We also manually annotated misaligned tiles and trained a CNN to detect them automatically from H&amp;E and DAPI images. In total, about 40k tiles (10%) were excluded.</p><p>For the IMMUcan dataset, where H&amp;E and mIF are from consecutive sections, we selected well-aligned tiles to ensure reliable correlation analysis based on three criteria: Pearson correlation of aligned nuclei density maps from 32×32 patches (threshold=0.25 per tile), tissue overlap (IoU &gt; 0.5), and tissue percentage (&gt; 40%). We extracted 1024 × 1024 pixel tiles at 20x (0.26 mm²), retaining 17k tiles covering 44.4 cm².</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Autofluorescence Subtraction &amp; Data Normalization</head><p>Autofluorescence (AF), captured as an independent channel I AF , refers to light naturally emitted by the tissue across channels, introducing noise in other markers. We subtract the AF from each marker channel I c IF , using λ c and b c which were manually adjusted using a Napari <ref type="bibr" target="#b41">Sofroniew et al. [2025]</ref> tool we developed:</p><formula xml:id="formula_0">I c cor = max(0, I c IF -λ c • I AF + b c )</formula><p>Next, each channel is normalized using the 99.9th percentile q c 0.999 , computed per marker from the distribution of foreground pixel intensities across the training set, and then log-transformed:</p><formula xml:id="formula_1">I c norm = 255 • log min(I c cor , q c 0.999 ) q c 0.999 + 1</formula><p>This logarithmic transformation compresses high-intensity values and reduces the impact of extreme outliers, while normalization ensures a consistent dynamic range across markers. The autofluorescence Napari tool, selected parameters, code, model, and data access instructions are available on our GitHub, within the Sanofi Public organization, under specific license conditions including a limitation to non-commercial use only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Single-Cell Pseudo-Label Extraction</head><p>To establish a ground truth for evaluating our model's ability to identify marker-positive cells from H&amp;E images, we used a standard cell type calling approach on mIF images <ref type="bibr">Lin et al. [2023a]</ref>. We first segmented nuclei from the DAPI channel using Cellpose <ref type="bibr" target="#b42">Stringer et al. [2021]</ref> fine-tuned on our data. As a proxy for cell regions, we dilated the nuclear regions by 2 µm. Single-cell analysis was performed by extracting mean fluorescence intensities, followed by unsupervised gating using a Gaussian Mixture Model (GMM) to distinguish positive from negative cells <ref type="bibr" target="#b43">Zhang et al. [2022]</ref>, with posterior probabilities estimated from the GMM. Although effective, this approach is sensitive to artifacts, approximate boundaries, and signal spillover. To improve robustness, we implemented a hierarchical gating strategy making sure that the biological marker hierarchy was preserved. For instance, given that CD3 positive cells are also CD45 positive, we kept CD3 positivity only for CD45 positive cells. By applying these rules, we obtained a high confidence annotation of our cells.</p><p>For the IMMUcan dataset, we used single-cell data provided by the consortium, which was generated using in-house clustering and nucleus segmentation. Since the same nuclei are not necessarily present in consecutive H&amp;E and mIF sections, we applied Hoverfast <ref type="bibr" target="#b44">Liakopoulos et al. [2024]</ref> for nucleus segmentation from H&amp;E images, allowing us to perform the cell-level correlation analysis explained in section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>This section outlines our overall approach for predicting mIF images from H&amp;E images. We used the Orion dataset for training, and HEMIT and IMMUcan datasets for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Architecture</head><p>Our model utilizes a U-Net generator to perform the image translation task from H&amp;E to mIF signals (Figure <ref type="figure" target="#fig_1">2</ref>). The architecture supports various encoder types, including CNNs like ConvNeXt v2 <ref type="bibr" target="#b45">Woo et al. [2023]</ref>, Swin Transformer <ref type="bibr" target="#b27">Liu et al. [2021]</ref>, and plain ViTs <ref type="bibr" target="#b8">Dosovitskiy et al. [2020]</ref>, enabling integration of recent foundation models in histology, such as CTransPath, Univ2, and H-optimus-0 <ref type="bibr" target="#b31">Wang et al. [2022]</ref>, <ref type="bibr" target="#b32">Chen et al. [2024]</ref>, <ref type="bibr">Saillard et al. [2024]</ref>. CNN-based encoders naturally align with the classical U-Net design, providing pyramidal features. For ViTs, which maintain fixed feature dimensions across layers, we explore two alternatives to generate multi-scale features: (1) convolutional transpose for upsampling token features, as used in <ref type="bibr">UNETR Hatamizadeh et al. [2022]</ref> and <ref type="bibr">CellViT Hörst et al. [2024]</ref>, and (2) a hybrid approach inspired by ViTMatte <ref type="bibr" target="#b36">Yao et al. [2024]</ref>, where a convolutional stream extracts pyramidal features while ViT features serve as the bottleneck. Unlike original ViTMatte, which concatenates a trimap to the input image, we use only the H&amp;E RGB image and omit convolutional necks and window attention. For simplicity, we still refer to this variant as VitMatte.</p><p>Building on these encoded features, the decoder reconstructs outputs using nearest interpolation for upsampling, combined with skip connections, dual 3×3 convolutional layers, batch normalization, and ReLU activation. Multiple output heads predict various mIF signals from the same decoder outputs, with a final Tanh activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Loss Function</head><p>We train MIPHEI using a weighted Mean Square Error (MSE) loss to ensure accurate translation from H&amp;E to mIF signals while accounting for varying signal intensities and prevalence across markers. Each marker's loss is scaled by the inverse of its standard deviation to balance contributions <ref type="bibr" target="#b46">Mai et al. [2021]</ref>.</p><p>Let L MSE,j denote the MSE loss for the j th marker, σ j denote the standard deviation, M be the total number of markers, and λ the weight of the reconstruction loss. The weighted MSE loss is then defined as: </p><formula xml:id="formula_2">L wMSE = λ M j 1 σ 2 j L MSE,j<label>4</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Here we present the experimental workflow that we set up to assess the performance and robustness of our image translation model from H&amp;E to mIF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Data Configuration</head><p>We split the Orion dataset by slide into training (37), validation (2), and test (2) sets. For HEMIT, we used its original train-validation-test splits, while for IMMUcan, all tiles were treated as test set.</p><p>All models are trained on H&amp;E-mIF images from ORION extracted as 256x256 pixel tiles at 0.5 mpp. For normalization, target data is scaled to the range [-0.9, 0.9] to prevent saturation at extreme values in Tanh activation. Input H&amp;E RGB data is normalized using the mean and standard deviation of the foundation model employed.</p><p>To enhance generalization, augmentations include spatial transformations, such as horizontal and vertical flips and coarse dropout (zeroing out a random box in both input and target), along with color augmentations like stain augmentation, random brightness and contrast adjustments, gaussian blur, and gaussian noise. These augmentations simulate variations in stain intensity, color distributions, and common imaging artifacts. To improve robustness, we trained a CycleGAN <ref type="bibr" target="#b48">Zhu et al. [2017]</ref> following Runz et al. <ref type="bibr" target="#b49">Runz et al. [2021]</ref> to translate Orion H&amp;E images toward the IMMUcan style, capturing more complex domain shift, and used both original and precomputed translated tiles as augmentation during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Training Configuration</head><p>We adopt similar Pix2Pix hyperparameters, given its role as a classical image translation model. We use the Adam optimizer with a learning rate of 2 × 10 -4 , kept constant in the first half of training before linearly decaying to zero. A linear warmup phase is applied to the generator's learning rate for the first 400 iterations to improve stability and adaptation, as recommended for ViTs by Dosovitskiy et al. <ref type="bibr" target="#b8">Dosovitskiy et al. [2020]</ref>. On top of data augmentations, regularization includes weight decay (1 × 10 -5 ), gradient clipping (max norm 1), and dropout (0.1). Non-pretrained weights are initialized using a normal distribution with a gain of 0.02, and biases are set to zero. All training are conducted with a batch size of 16. All experiments were performed on a single A100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Metric Overview</head><p>To evaluate our image translation model from H&amp;E to mIF, we used both standard pixel-level and cell-level metrics. At the pixel level, we employed PSNR and SSIM. One of the most important aspect of our model is to capture cell-type information, which is not adequately measured by these metrics. We therefore designed a cell-level fidelity metric.  For this, we extract predicted single-cell data by averaging predicted marker expression within each cell, and classify them using a logistic regression trained on validation set cells. The classifier is then applied to the test set and predictions are compared to pseudo-labels as described in subsection 3.4.4. We use a classifier instead of direct gating on predicted mIF channels to improve robustness to individual channel errors and account for calibration differences between predicted and target fluorescence intensities.</p><p>We assess the discriminative power of our model's predictions by computing Cell AUC (Receiver Operating Characteristic Area Under the Curve), using the predicted mean intensity per nucleus as a score compared to pseudo-labels. To account for class imbalance and better assess model performance, we report F1 score using the cell classifier predictions, as most markers have a lower frequency of positive cells.</p><p>We further analyze our model by computing cell count correlations on IMMUcan for CD3, CD8, CD4, FOXP3, and Pan-CK. Since consecutive tissue sections in the IMMUcan dataset are biologically similar but not identical, direct pixel-level alignment is not feasible. However, the minimal spatial separation between sections preserves overall tissue architecture and cellular distribution, allowing for meaningful region-level comparisons. We compute the correlation between predicted and target positive cell counts across registered tiles, using predictions from the logistic regressionbased single-cell classification. Following preprocessing in D.1, we compute Pearson correlation coefficients between </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>This section presents an ablation study to identify critical model components, comparisons with the state-of-the-art HEMIT method, and a detailed analysis of marker-specific performance. We selected HEMIT for benchmarking as it addresses a similar problem, provides public code and data for direct comparison, and outperforms baselines like Pix2Pix used in other related work such as SHIFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Ablation studies</head><p>To identify critical components and configurations significantly impacting performance, we trained all models for 15 epochs (∼276k iterations) with batch size 16 by default. We evaluated the impact of adding a discriminator in a Pix2Pix-like setup, using UNETR as the generator and the standard patch-based discriminator from Pix2Pix. We found that the discriminator slightly reduces pixel-level metrics, and consistently cell-type classification accuracy. We hypothesize the model favors realism over fidelity (Table <ref type="table" target="#tab_0">1</ref>), thus leading to more realistic looking images, but not providing more accurate predictions. For this reason, we excluded the discriminator from further experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Model Architecture</head><p>We next compared four U-Net-inspired architectures for our mIF prediction model: two convolution-based encoders pre-trained on ImageNet-ResNet50 (37M parameters) and ConvNeXt v2 Large (203M parameters)-and two U-Net variants that both use the H-optimus-0 foundation model as encoder but differ in their ViT integration strategies-UNETR (1.17B parameters, 36M trainable via LoRA) and ViTMatte (1.14B parameters, 6.6M trainable via LoRA). U-Net with a ResNet50 encoder showed lower performance, likely due to its smaller size and lack of domainspecific pretraining. In contrast, U-Net-ConvNeXt achieved strong results, benefiting from greater model capacity and attention-like mechanisms. Transformer-based variants using H-optimus-0 encoder significantly outperformed convolutional ones, emphasizing the advantage of employing pretrained foundation models for the encoder component in our task. UNETR and ViTMatte achieved similar performance, but ViTMatte converged much faster, reaching UNETR's 10-epoch performance after the first one only. This is likely due to its convolutional 'detail capture' stream, which provides intermediate skip features that complement ViT representations and aid spatial reconstruction more effectively than UNETR's token-based upsampling. Overall, ViTMatte offers the best trade-off between performance and training efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Foundation Model Benchmark</head><p>We evaluated three foundation-model encoders available for our image translation task (UNETR with LoRA): CTransPath, Univ2, and H-optimus-0. CTransPath was significantly outperformed by Univ2 and H-optimus-0, which provided similar results with a slight advantage for H-optimus-0, which we therefore selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4">Finetuning Strategies</head><p>Finally, we evaluated the finetuning strategies described in subsection 4.3 using UNETR with the H-optimus-0 encoder. Our results show that LoRA improves performance over a frozen encoder, achieving a higher test cell AUC (0.431 vs. 0.413). Based on this, we adopt LoRA as our fine-tuning strategy.</p><p>Based on our ablation study, the best-performing configuration for MIPHEI is ViTMatte with the H-optimus-0 encoder, adapted using LoRA, and trained without a GAN strategy, providing an optimal balance between performance, efficiency, and convergence speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparison with State of the Art (HEMIT model)</head><p>To benchmark our proposed model, MIPHEI, against the state-of-the-art HEMIT, we evaluate cross-dataset generalization using five models: (1) MIPHEI, trained on ORION, (2) the official HEMIT checkpoint trained on the HEMIT dataset, (3) HEMIT* trained on ORION using the HEMIT-architecture, but with the MIPHEI training scheme (weighted MSE loss, no discriminator), (4) HEMIT-Orion, trained on ORION using the original HEMIT pipeline, and</p><p>(5) a stratified random model as a baseline, assigning cell type probabilities based on their distribution in the test datasets, with metrics averaged over 100 runs to account for variability in random sampling. Since most cell subtypes are not identifiable from H&amp;E alone by a pathologist, the task is challenging, and the random baseline provides a reference to show that our model outperforms chance-level expectations. Evaluation is conducted on the ORION test set (in-domain for models trained on ORION), on the HEMIT validation and test sets and the IMMUcan dataset, both serving as external out-of-domain benchmarks. The results of performance evaluation of all tested models is shown in Table <ref type="table" target="#tab_1">2</ref>, Figure <ref type="figure" target="#fig_6">5</ref>). HEMIT-Orion, trained with the original HEMIT pipeline, outperformed the random model, but showed the lowest performance across all markers except PD-L1, for which all methods performed poorly. We hypothesize that this was due to the use of the L1 loss, which may be less effective for markers with low intensity or high background proportion, as suggested by the stronger results of HEMIT* trained with our setup. On the other hand, MIPHEI achieved the best overall performance, surpassing both HEMIT* and HEMIT-Orion across nearly all markers. The only exception was Ki67, where HEMIT* performed slightly better, achieving a Cell F1 score 0.3% higher than MIPHEI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Evaluation on ORION test set</head><p>These results confirm that MIPHEI provides the most robust predictions within its training domain. The improved performance of HEMIT* over HEMIT-Orion further demonstrates the effectiveness of our training pipeline, even when applied to an alternative architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Evaluation on HEMIT dataset</head><p>The HEMIT dataset serves as an external test set for assessing the robustness of our approach. We compare MIPHEI (train: ORION), with the HEMIT model (train: HEMIT, released checkpoint). As Figure <ref type="figure" target="#fig_2">3</ref>.B suggests, the HEMIT validation and test sets come from different domains, but both are contained within the training domain, with the test domain being more strongly represented. Hence, we report cell-level performances separately for each. We use logistic regression as the cell classifier, trained on 5% of the training cells for MIPHEI and all training cells for HEMIT. As MIPHEI predicts 15 ORION markers while HEMIT predicts only Pan-CK and CD3, each model's classifier is trained on the full set of markers predicted by its respective model.</p><p>On the HEMIT test set, MIPHEI slightly outperforms HEMIT by +3% F1 score for Pan-CK (Figure <ref type="figure" target="#fig_6">5</ref>), but HEMIT significantly outperforms MIPHEI on CD3, with a +22% Cell F1 score. However, on the HEMIT validation set, MIPHEI surpasses HEMIT for both markers, achieving +18% F1 for Pan-CK and +17% for CD3. This is a strong result, as MIPHEI was evaluated on an external dataset with only minimal adaptation of the auxiliary cell classification stage, using just 5% of the training data. In contrast, the HEMIT model was trained on data that included images from both the validation and test domains of the HEMIT dataset (Figure <ref type="figure" target="#fig_2">3</ref>.C). MIPHEI demonstrates stronger generalization, likely due to its foundation model encoder, enabling better adaptation to unseen domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Evaluation on IMMUcan dataset</head><p>We further evaluate model generalization on the IMMUcan dataset by performing a correlation analysis between predicted cell type counts from H&amp;E and pseudo-label counts from consecutive mIF sections. Pearson correlation is computed over 0.26 mm² regions for MIPHEI, HEMIT*, and HEMIT. Each model uses its own logistic regression cell classifier, trained on ORION validation cells for MIPHEI and HEMIT*, and on all HEMIT training cells for HEMIT.</p><p>Across all markers, MIPHEI achieves the highest Pearson correlation, outperforming both HEMIT* and HEMIT, further confirming its superiority in cross-domain generalization (Figure <ref type="figure" target="#fig_6">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Marker-Level Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">In-domain performance analysis</head><p>We have demonstrated that MIPHEI consistently outperforms random predictions for all markers, confirming its ability to capture meaningful morphological cues from H&amp;E to estimate protein expression. However, the predictability varies across markers and cell types (Figure <ref type="figure" target="#fig_6">5</ref>.A).</p><p>Markers with the highest performance include epithelial markers E-cadherin (F1 0.903) and Pan-CK (F1 0.884), which label epithelial cells forming well-defined clusters and glandular structures in H&amp;E. The immune marker CD45 (F1 0.681) also performs well. While it mainly identifies lymphocytes, which are easily recognizable in H&amp;E, it also includes cells like monocytes, which are harder to spot, making prediction a bit more challenging.</p><p>Markers with moderate performance included SMA (F1 0.564), which labels smooth muscle cells, and CD31 (F1 0.386), which marks endothelial cells. Their more subtle morphological features may have contributed to lower accuracy: smooth muscle cells have a spindle-shaped appearance but can be confused with fibroblasts, while endothelial cells are typically sparse and located within vessel structures, making them harder to distinguish. Immune subtype markers CD3e, CD45RO, CD4, CD8a, and CD20 (F1 0.229-0.572) showed moderate performance, with broader T-cell markers like CD3e achieving better performance than more specific ones like CD8a, which are harder to identify. While lymphocytes are visible in H&amp;E, their subtypes remain indistinguishable to pathologists, highlighting our model's value on this difficult task. Macrophage markers CD68 (F1 0.362) and CD163 (F1 0.206) faced similar challenges, as their heterogeneity complicates identification, with CD163 being a macrophage subtype of CD68, further complicating prediction. Markers with the lowest performance are FOXP3 (F1 0.114) and PD-L1 (F1 0.048), both challenging for different reasons. FOXP3 marks rare regulatory T-cells, a highly specific CD4+ subtype, making it one of the most difficult immune markers to predict. PD-L1, a functional marker, lacks a clear association with a single cell type and shows irregular expression across various cells, making probably prediction from H&amp;E nearly impossible.</p><p>In summary, structural markers and broad immune markers seem predictable with high accuracy. Markers defining specific immune subtypes are harder to predict, and some functional markers are not predictable with an accuracy high enough to be applicable in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Out-of-domain performance analysis</head><p>Our external validation on HEMIT and IMMUcan confirms that MIPHEI, trained on ORION, generalizes well to datasets with domain shifts in mIF technology and H&amp;E appearance, requiring minimal adaptation.</p><p>On the HEMIT dataset, analysis is limited by the small number of available markers, but our model performs well with only minimal adaptation of the cell classifier using 5% of the training cells.</p><p>On the IMMUcan dataset, we observe strong correlation for CD4 (0.80) CD3e (0.73) and CD8 (0.69), and moderate correlation for FOXP3 (0.60) and Pan-CK (0.63). We also observe overdetection for CD3e (i.e. cells with CD3e prediction and measured 0 expression) and underdetection of Pan-CK (cells with predicted 0-expression and positive according to the measurement). Manual inspection showed that this was mainly due to low-quality tiles with high auto-fluorescence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>In this study, we present MIPHEI, a method trained to predict 16 mIF channels from standard H&amp;E slides. For this, we proposed a U-Net with a ViT-based foundation model as encoder and demonstrated that this architecture outperforms previously proposed methods. Moreover, we showed that MIPHEI generalizes well across datasets. We attribute this to the robust and transferable encodings learned by the foundation model, which was exposed to millions of tiles during pretraining. Our findings highlight the value of leveraging foundation models for mIF prediction, and potentially for other image translation tasks in histology.</p><p>We also introduced a validation strategy focused on single-cell metrics, which we believe are the most relevant for this task. Pixel-wise accuracy may be unreliable, as H&amp;E images are usually not informative about cytoplasmic boundaries. Instead, the biologically meaningful information lies in protein expression levels within individual cells, protein positivity, or the resulting cell type. We reflect this in our evaluation framework, which we provide as part of this study.</p><p>Accurate, domain-robust mIF prediction opens the door to a range of applications. While direct clinical deployment for diagnostics may remain challenging, MIPHEI proves to be a powerful tool for mining large retrospective cohorts-enabling the identification of cell types without relying on manual annotation. Ultimately, this can allow to identify associations between specific cell populations, their spatial arrangements, and clinically relevant outcomes such as survival or treatment response. MIPHEI thus holds promise for hypothesis generation and exploratory analysis. Furthermore, extending this approach to predict clinically relevant scores, such as the Immunoscore, represents a compelling direction for future work.</p><p>Our study is not free of limitations. Although the number of training tiles was substantial, they were derived from only 41 slides. Access to larger and more diverse datasets would likely improve the model's robustness to domain shifts and biological variability. Additionally, we observed that the cell classification model still requires fine-tuning on a small set of labeled cells when applied to out-of-domain data. As such, if a domain shift is anticipated, some mIF data will still be needed for calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we present MIPHEI, a deep learning framework that predicts mIF images from H&amp;E using ViT foundation models as encoders within a U-Net architecture. MIPHEI outperforms state-of-the-art models on both internal and external datasets, demonstrating strong generalization across staining protocols and imaging conditions. We evaluated MIPHEI across 15 protein markers and associated cell types. It achieves high accuracy for epithelial (Pan-CK, Ecadherin) and broad immune markers (CD45, CD3e), performs moderately on more specific immune subtypes (CD8a, CD4, CD45RO, CD68, CD163) and stromal markers (CD31, α-SMA), and struggles with FOXP3 and PD-L1 due to their complexity and small number of positive cells.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Preprocessing pipeline: (a) H&amp;E and mIF images are aligned using Valis Gatenbee et al. [2023]. Tissue regions are then selected via Otsu thresholding on H&amp;E, and a trained CNN filters misaligned tiles caused by the restaining and acquisition process. (b) Autofluorescence is subtracted from mIF images, followed by DAPI-based nuclei segmentation and nuclei dilation to approximate cell boundaries. (c) Pseudo-labels are generated by computing per-cell mean marker expression and applying GMM clustering to define marker positivity, determining labels (e.g., CD3e + ).</figDesc><graphic coords="3,219.14,102.51,58.68,58.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: MIPHEI architecture: A U-Net-inspired model, based on VitMatte, is trained to predict mIF images from H&amp;E, using the H-optimus-0 ViT foundation model, Tanh activation, and a custom weighted MSE loss to correct for the unbalanced distribution across markers.</figDesc><graphic coords="4,153.90,72.00,304.22,157.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Dataset Overview: (a) Distribution of tissue samples and tiles across datasets and splits. (b) 2D UMAP visualization of H-Optimized-0 embeddings, highlighting domain shifts across our datasets. (c) Normalized cell type distribution across datasets and splits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.3 Foundation Model Training Strategies While CNN-based encoders within the U-Net architecture are fully fine-tuned in all experiments, we explore two efficient training strategies for large ViT-based foundation model encoders: (1) decoder-only training with a frozen foundation encoder, leveraging robust pretrained features while reducing trainable parameters; (2) Low-Rank Adaptation (LoRA) Hu et al. [2022] with rank = 8 and α = 1, which adapts a pretrained ViT encoder by adding trainable low-rank matrices to the Query and Value projections of the attention layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Prediction pipeline and prediction visualization: (a) Inference pipeline: mIF images are first generated using a trained U-Net model. Predicted single-cell data are then extracted by averaging predicted mIF signals within each nucleus, using nuclei masks from an external segmentation model. Finally a cell classifier, trained on validation set cells, predicts cell types. (b) Prediction examples from our best model: Predicted mIF images and cell types (shown as colored cell boundaries) are compared to target mIF images and annotated cell types from the same restained tissue section in the Orion (CD3e, CD8a) and HEMIT (Pan-CK, CD3) datasets. (c) IMMUcan large-area visualization: Nuclei predictions on H&amp;E alongside clustered nuclei from the corresponding consecutive mIF sections.</figDesc><graphic coords="8,207.32,328.47,133.85,84.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Evaluation analysis: (a) Orion test set: Performance comparison across all Orion markers between MIPHEI, HEMIT*, HEMIT (all trained on Orion train set) and a stratified random model predicting classes based on cell type proportions. Markers are grouped by general functions, with hierarchical relationships indicated by arrows. Cell classification model is a logistic regression model trained on Orion validation cells. (b) HEMIT dataset: Comparison between MIPHEI (trained on Orion), the HEMIT model (trained on HEMIT train set) and a stratified random model. Cell classification model is a logistic regression: trained on 5% of training cells for MIPHEI, and all available training cells for the HEMIT model. (c) IMMUcan (consecutive sections): Pearson correlation and regression plots (with linear fits) between predicted and pseudo-labeled cell type counts from 17k tiles using MIPHEI.</figDesc><graphic coords="9,75.81,379.02,90.33,74.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation Study: Performance Metrics Across Model Configurations on ORION test set</figDesc><table><row><cell>Configuration</cell><cell></cell><cell>PSNR SSIM Cell</cell><cell>Cell F1</cell></row><row><cell></cell><cell></cell><cell>AUC</cell><cell></cell></row><row><cell cols="4">Impact of GAN Discriminator (UNETR H-optimus-0 LoRA)</cell></row><row><cell>Generator only</cell><cell></cell><cell>27.86 0.840 0.868</cell><cell>0.431</cell></row><row><cell>Pix2Pix (GAN)</cell><cell></cell><cell>27.00 0.830 0.817</cell><cell>0.410</cell></row><row><cell cols="4">Impact of Foundation Model Encoder (UNETR with LoRA)</cell></row><row><cell>CTransPath</cell><cell></cell><cell>26.96 0.837 0.812</cell><cell>0.351</cell></row><row><cell>Univ2</cell><cell></cell><cell>27.73 0.838 0.862</cell><cell>0.424</cell></row><row><cell>H-optimus-0</cell><cell>(se-</cell><cell>27.86 0.840 0.868</cell><cell>0.431</cell></row><row><cell>lected)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Encoder Finetuning Method (UNETR H-optimus-0)</cell></row><row><cell>Frozen encoder</cell><cell></cell><cell>27.44 0.836 0.857</cell><cell>0.413</cell></row><row><cell>LoRA (selected)</cell><cell></cell><cell>27.86 0.840 0.868</cell><cell>0.431</cell></row><row><cell></cell><cell cols="2">Impact of Architecture</cell><cell></cell></row><row><cell>U-Net-ResNet50</cell><cell></cell><cell>26.54 0.832 0.812</cell><cell>0.344</cell></row><row><cell cols="2">U-Net-ConvNeXtv2</cell><cell>27.40 0.839 0.840</cell><cell>0.379</cell></row><row><cell>Large</cell><cell></cell><cell></cell><cell></cell></row><row><cell>HEMIT*</cell><cell></cell><cell>27.08 0.837 0.831</cell><cell>0.360</cell></row><row><cell>UNETR</cell><cell></cell><cell>27.86 0.840 0.868</cell><cell>0.431</cell></row><row><cell>MIPHEI (Best)</cell><cell></cell><cell>27.78 0.837 0.876</cell><cell>0.438</cell></row><row><cell>6.1.1 Impact of Discriminator</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Overall Cell-Level Performances on Test Sets</figDesc><table><row><cell>Cell</cell><cell>MIPHEI</cell><cell>HEMIT</cell><cell>HEMIT*</cell><cell>HEMIT</cell><cell></cell></row><row><cell>Metric</cell><cell>(Orion)</cell><cell>(HEMIT)</cell><cell>(Orion)</cell><cell cols="2">(Orion) Random</cell></row><row><cell></cell><cell></cell><cell cols="2">ORION Dataset (Test Set)</cell><cell></cell><cell></cell></row><row><cell>AUC</cell><cell>0.876</cell><cell>-</cell><cell>0.831</cell><cell>0.701</cell><cell>-</cell></row><row><cell>F1</cell><cell>0.438</cell><cell>-</cell><cell>0.360</cell><cell>0.253</cell><cell>0.140</cell></row><row><cell></cell><cell cols="4">HEMIT Dataset (Average Validation &amp; Test Sets)</cell><cell></cell></row><row><cell>AUC</cell><cell>0.844</cell><cell>0.863</cell><cell>0.764</cell><cell>0.598</cell><cell>-</cell></row><row><cell>F1</cell><cell>0.701</cell><cell>0.663</cell><cell>0.471</cell><cell>0.481</cell><cell>0.333</cell></row><row><cell></cell><cell></cell><cell cols="2">IMMUcan Dataset</cell><cell></cell><cell></cell></row><row><cell>Pearson</cell><cell>0.690</cell><cell>-</cell><cell>0.667</cell><cell>0.422</cell><cell>-</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work uses IMMUcan data funded by IMI2 <rs type="funder">JU</rs> (grant <rs type="grantNumber">821558</rs>) with support from <rs type="funder">Horizon 2020</rs> and <rs type="institution">EFPIA</rs>.</p></div>
			</div>
			<div type="funding">
<div><head>Funding</head><p>This work was sponsored by <rs type="funder">ANRT</rs> and <rs type="funder">Sanofi. Furthermore</rs>, <rs type="person">T. Walter</rs> acknowledges funding from <rs type="funder">Agence Nationale de la Recherche</rs> under the <rs type="programName">France 2030 program</rs>, with the reference number <rs type="grantNumber">ANR-24-EXCI-0004</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8kMZ7xU">
					<idno type="grant-number">821558</idno>
				</org>
				<org type="funding" xml:id="_qTVyJ69">
					<idno type="grant-number">ANR-24-EXCI-0004</idno>
					<orgName type="program" subtype="full">France 2030 program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest</head><p>G. Balezo, R. Trullo, A. Pla Planas are/or were Sanofi employees and may hold shares and/or stock options in the company. E. Decencière and T. Walter have nothing to disclose.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiplexed imaging in oncology</title>
		<author>
			<persName><forename type="first">Chrysafis</forename><surname>Andreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weissleder</surname></persName>
		</author>
		<author>
			<persName><surname>Moritz F Kircher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="527" to="540" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Overview of multiplex immunohistochemistry/immunofluorescence techniques in the era of cancer immunotherapy</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjna</forename><surname>Nilesh Nerurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><forename type="middle">Yun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duoduo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felicia</forename><surname>Wee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Chun Tatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Yeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hon</forename><surname>Kiat</surname></persName>
		</author>
		<author>
			<persName><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Communications</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="135" to="153" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An introduction to performing immunofluorescence staining</title>
		<author>
			<persName><forename type="first">Kyuseok</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Mareninov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Palma</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">H</forename><surname>Yong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Applications of immunohistochemistry</title>
		<author>
			<persName><forename type="first">Jeyapradha</forename><surname>Duraiyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeshwar</forename><surname>Govindarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karunakaran</forename><surname>Kaliyappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murugesan</forename><surname>Palanisamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pharmacy and Bioallied Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">Suppl 2</biblScope>
			<biblScope unit="page" from="307" to="S309" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Codex multiplexed tissue imaging with dna-conjugated antibodies</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darci</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">W</forename><surname>Hickey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Kennedy-Darling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Venkataraaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Samusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><forename type="middle">M</forename><surname>Goltsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garry</forename><forename type="middle">P</forename><surname>Schürch</surname></persName>
		</author>
		<author>
			<persName><surname>Nolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature protocols</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3802" to="3835" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">High-plex immunofluorescence imaging and traditional histology of the same tissue section for discovering image-based biomarkers</title>
		<author>
			<persName><forename type="first">Jia-Ren</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-An</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Campton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shannon</forename><surname>Coy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clarence</forename><surname>Yapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juliann</forename><forename type="middle">B</forename><surname>Tefft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Mccarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><forename type="middle">L</forename><surname>Ligon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">J</forename><surname>Rodig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature cancer</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1036" to="1052" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analysis of proteins and proteomes by mass spectrometry</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">C</forename><surname>Hendrickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Pandey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of biochemistry</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="437" to="473" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Characterization of immune cell populations in the tumor microenvironment of colorectal cancer using high definition spatial profiling</title>
		<author>
			<persName><forename type="first">F</forename><surname>Michelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">P</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meii</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anushka</forename><surname>Gottscho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">E</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Syrus</forename><surname>Pilipauskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandhini</forename><surname>Mohabbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><surname>Sukovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2024" to="2026" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Virtual alignment of pathology image series for multi-gigapixel whole slide images</title>
		<author>
			<persName><forename type="first">Ann-Marie</forename><surname>Chandler D Gatenbee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhya</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ottilie</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><surname>Swinyard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Robbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Slebos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eoghan</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noemi</forename><surname>Mulholland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Marusyk</surname></persName>
		</author>
		<author>
			<persName><surname>Leedham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4502</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName><surname>Ting-Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning enables cross-modality super-resolution in fluorescence microscopy</title>
		<author>
			<persName><forename type="first">Hongda</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Rivenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhensong</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harun</forename><surname>Günaydın</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><forename type="middle">A</forename><surname>Bentolila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Comert</forename><surname>Kural</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aydogan</forename><surname>Ozcan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="110" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Brightfield holography: cross-modality deep learning enables snapshot 3d imaging with bright-field contrast using a single hologram</title>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunvant</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Rivenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayfer</forename><surname>Calis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aydogan</forename><surname>Ozcan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Light: Science &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bi-directional feature fusion generative adversarial network for ultra-high resolution pathological image virtual re-staining</title>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhineng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiongjun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3904" to="3913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Label-free virtual her2 immunohistochemical staining of breast tissue using deep learning</title>
		<author>
			<persName><forename type="first">Darrowmorgan</forename><surname>Doanngan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leehan</forename><surname>Angus</surname></persName>
		</author>
		<author>
			<persName><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BME frontiers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">7-up: Generating in silico codex from a small set of immunofluorescence markers</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandro</forename><forename type="middle">E</forename><surname>Trevino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Honesty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D'</forename><surname>Blaize</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Angio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">E</forename><surname>Preska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">W</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Charville</surname></persName>
		</author>
		<author>
			<persName><surname>Dalerba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS nexus</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">171</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Random multi-channel image synthesis for multiplexed immunofluorescence imaging</title>
		<author>
			<persName><forename type="first">Shunxing</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riqiang</forename><surname>Ho Hin Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophie</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilwoo</forename><surname>Chiron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><forename type="middle">A</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><forename type="middle">T</forename><surname>Coburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bennett</forename><forename type="middle">A</forename><surname>Roland</surname></persName>
		</author>
		<author>
			<persName><surname>Landman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI Workshop on Computational Pathology</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="36" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning-inferred multiplex immunofluorescence for immunohistochemical image quantification</title>
		<author>
			<persName><forename type="first">Parmida</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arie</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Vanguri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Greenwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Angelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><forename type="middle">J</forename><surname>Hollmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saad</forename><surname>Nadeem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature machine intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="401" to="412" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Virtual histological staining of unlabelled tissue-autofluorescence images via deep learning</title>
		<author>
			<persName><forename type="first">Hongda</forename><surname>Yair Rivenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhensong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">E</forename><surname>Günaydın</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Zuckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">E</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><surname>Sisk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="466" to="477" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning-enabled virtual histological staining of biological samples</title>
		<author>
			<persName><forename type="first">Bijie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Pillar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aydogan</forename><surname>Ozcan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Light: Science &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Digital synthesis of histological stains using micro-structured and multiplexed virtual staining of label-free tissue</title>
		<author>
			<persName><forename type="first">Yijie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Rivenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apostolos</forename><surname>Delis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aydogan</forename><surname>Ozcan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Light: Science &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">78</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Label-free intraoperative histology of bone tissue via deep-learning-assisted ultraviolet photoacoustic microscopy</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Scott D Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yide</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong V</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="124" to="134" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">In silico labeling: predicting fluorescent labels in unlabeled images</title>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Eric M Christiansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashkan</forename><surname>Michael Ando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaia</forename><surname>Javaherian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Skibinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Lipnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alison</forename><forename type="middle">O</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevan</forename><surname>'neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="792" to="803" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shift: speedy histological-to-immunofluorescent translation of a tumor signature enabled by deep learning</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">A</forename><surname>Burlingame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Mcdonnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">F</forename><surname>Schau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Thibault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Lanciault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brett</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Corless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><forename type="middle">W</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young</forename><surname>Hwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">17507</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Immunoaizer: A deep learning-based computational framework to characterize cell distribution and gene mutation in tumor microenvironment</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanfan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxin</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancers</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1659</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hemit: H&amp;e to multiplex-immunohistochemistry image translation with dual-branch pix2pix generator</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beth</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Fergie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="184" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9640" to="9649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Jinghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07832</idno>
		<title level="m">ibot: Image bert pre-training with online tokenizer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Darcet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Théo</forename><surname>Moutakanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huy</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Szafraniec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasil</forename><surname>Khalidov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Haziza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.07193</idno>
		<title level="m">Learning robust visual features without supervision</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transformerbased unsupervised contrastive learning for histopathological image classification</title>
		<author>
			<persName><forename type="first">Xiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">102559</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards a general-purpose foundation model for computational pathology</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Richard J Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><forename type="middle">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Drew Fk Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">H</forename><surname>Jaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><surname>Shaban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="850" to="862" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Saillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Llinares-López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zelda</forename><surname>Mariet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cahané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Philippe</forename><surname>Vert</surname></persName>
		</author>
		<ptr target="https://github.com/bioptimus/releases/tree/main/models/h-optimus/v0" />
		<title level="m">H-optimus-0, 2024</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cellvit: Vision transformers for precise cell segmentation and classification</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Hörst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Rempe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Heine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantin</forename><surname>Seibold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Keyl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Baldini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Selma</forename><surname>Ugurel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Siveke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Grünwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Egger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page">103143</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unetr: Transformers for 3d medical image segmentation</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Hatamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishwesh</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Myronenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bennett</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daguang</forename><surname>Holger R Roth</surname></persName>
		</author>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF winter conference on applications of computer vision</title>
		<meeting>the IEEE/CVF winter conference on applications of computer vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Vitmatte: Boosting image matting with pre-trained plain vision transformers</title>
		<author>
			<persName><forename type="first">Jingfeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page">102091</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Vision transformer adapter for dense predictions</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.08534</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Vit-comer: Vision transformer with convolutional multi-scale feature interaction for dense predictions</title>
		<author>
			<persName><forename type="first">Chunlong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="5493" to="5502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-An</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Campton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shannon</forename><surname>Coy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clarence</forename><surname>Yapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juliann</forename><forename type="middle">B</forename><surname>Tefft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Mccarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Ligon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">J</forename><surname>Rodig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Reese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tad</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Santagata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">K</forename><surname>Sorger</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.7637988</idno>
		<ptr target="https://doi.org/10.5281/zenodo.7637988" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Immucan: Broad cellular and molecular profiling of the human tumor microenvironment</title>
		<author>
			<persName><forename type="first">S</forename><surname>Henoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Liechti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><forename type="middle">M</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Immucan</forename><surname>Morfouace</surname></persName>
		</author>
		<author>
			<persName><surname>Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Research</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">16_Supplement</biblScope>
			<biblScope unit="page" from="1699" to="1699" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">napari: a multi-dimensional image viewer for python</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sofroniew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bokota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nunez-Iglesias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sobolewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sweet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gaifas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doncila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamauchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Weber Mendonça</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vierdag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Monko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Royer</surname></persName>
		</author>
		<author>
			<persName><surname>Can</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I S</forename><surname>Solak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harrington</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.15421167</idno>
		<ptr target="https://doi.org/10.5281/zenodo.15421167" />
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cellpose: a generalist algorithm for cellular segmentation</title>
		<author>
			<persName><forename type="first">Carsen</forename><surname>Stringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michalis</forename><surname>Michaelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Pachitariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Identification of cell types in multiplexed in situ images by combining protein expression and spatial information using celesta</title>
		<author>
			<persName><forename type="first">Weiruo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><forename type="middle">E</forename><surname>Reticker-Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zinaida</forename><surname>Good</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Samusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saumyaa</forename><surname>Saumyaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="759" to="769" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Hoverfast: an accurate, highthroughput, clinically deployable nuclear segmentation tool for brightfield digital pathology images</title>
		<author>
			<persName><forename type="first">Petros</forename><surname>Liakopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Massonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonatan</forename><surname>Bonjour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tekes</forename><surname>Medya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Mizrakli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><forename type="middle">A</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><forename type="middle">H</forename><surname>Cuendet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Seipel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doron</forename><surname>Michielin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Merkler</surname></persName>
		</author>
		<author>
			<persName><surname>Janowczyk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.14028</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Convnext v2: Co-designing and scaling convnets with masked autoencoders</title>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoubhik</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="16133" to="16142" />
		</imprint>
	</monogr>
	<note>So Kweon, and Saining Xie</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Khamies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Paull</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.04497</idno>
		<title level="m">Batch inverse-variance weighting: Deep heteroscedastic regression</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lora: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Normalization of he-stained histological images using cycle consistent generative adversarial networks</title>
		<author>
			<persName><forename type="first">Marlen</forename><surname>Runz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rusche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Martin R Weihrauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cleo-Aron</forename><surname>Hesser</surname></persName>
		</author>
		<author>
			<persName><surname>Weis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diagnostic Pathology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
