<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D-Fixup: Advancing Photo Editing with 3D Priors</title>
				<funder ref="#_qTCaYGG #_fNxqfBf #_dt5abuS">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_rcDqHBe">
					<orgName type="full">NIFA</orgName>
				</funder>
				<funder ref="#_gZ5vw99">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2025-05-15">15 May 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yen-Chi</forename><surname>Cheng</surname></persName>
							<email>yenchic3@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Krishna</forename><surname>Kumar Singh</surname></persName>
						</author>
						<author>
							<persName><roleName>USA</roleName><forename type="first">Adobe</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jae</forename><forename type="middle">Shin</forename><surname>Yoon</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Guerrero</surname></persName>
							<email>guerrero@adobe.com</email>
						</author>
						<author>
							<persName><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chi</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
							<email>aschwing@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Liang-Yan</forename><surname>Gui</surname></persName>
							<email>lgui@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Matheus</forename><surname>Gadelha</surname></persName>
							<email>gadelha@adobe.com</email>
						</author>
						<author>
							<persName><forename type="first">Nanxuan</forename><surname>Zhao</surname></persName>
							<email>nanxuanz@adobe.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">ALEXANDER SCHWING</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">LIANG-YAN GUI</orgName>
								<orgName type="institution" key="instit2">University of Illinois Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">MATHEUS GADELHA</orgName>
								<orgName type="institution" key="instit2">Adobe Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">NANXUAN ZHAO</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Adobe Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>Illinois</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">Adobe Research</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="institution">Adobe Research</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<addrLine>Liang-Yan Gui</addrLine>
									<settlement>Urbana</settlement>
									<region>Illinois</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>Illinois</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff12">
								<orgName type="institution">Adobe Research</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff13">
								<orgName type="institution">Adobe Research</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D-Fixup: Advancing Photo Editing with 3D Priors</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-05-15">15 May 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">B8C9921C43E2AE99CDE66DD2085B9D77</idno>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
					<idno type="arXiv">arXiv:2505.10566v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-19T11:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image editing</term>
					<term>3D</term>
					<term>Diffusion Model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite significant advances in modeling image priors via diffusion models, 3D-aware image editing remains challenging, in part because the object is only specified via a single image. To tackle this challenge, we propose 3D-Fixup, a new framework for editing 2D images guided by learned 3D priors. The framework supports difficult editing situations such as object translation and 3D rotation. To achieve this, we leverage a training-based approach that harnesses the generative power of diffusion models. As video data naturally encodes real-world physical dynamics, we turn to video data for generating training data pairs, i.e., a source and a target frame. Rather than relying solely on a single trained model to infer transformations between source republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ig. 1. 3D-aware photo editing. Given a source image with user-specified 3D transformations, our model generates a new image that follows the user's edit while preserving the input identity. The 3D edit is visualized via the transformation between the original mesh (pink) and the edited mesh (cyan).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative image editing is a growing research field that promises intuitive edits of objects in images, even if information about these objects or the scene that contains them is incomplete. For example, moving an object from one side of the image to the other, or rotating it as shown in Figure <ref type="figure">1</ref>, may require knowledge about lighting, shadows, and occluded parts of the scene that are not available in the image. For these edits, generative models can hallucinate the missing information to obtain a plausible result. Current generative editing methods focus on appearance edits or 2D edits of image patches. However the objects we typically manipulate in images are projections of 3D objects. Some natural edits for 3D objects, like outof-plane rotations or 3D translations, are thus not possible in most current approaches. However, 3D editing is crucial in applications such as e-commerce, where a 3D object may need to be shown from multiple angles, or digital media production, where it gives artists the ability to re-configure or relight a 3D scene shown in an image, without having to explicitly reconstruct the entire 3D scene, simplifying the creative process. By developing such a 3D editing method on natural images, we aim to bridge the gap between 2D and 3D workflows, making realistic edits more accessible for real-world applications.</p><p>The current challenge for 3D editing of objects in images lies in maintaining consistent object appearance across different angles and lighting conditions, which is essential for creating seamless edits. Existing approaches have attempted to address these issues using either optimization or feed-forward deep net techniques. Optimizationbased approaches, such as Image Sculpting <ref type="bibr" target="#b32">[Yenphraphai et al. 2024]</ref>, begin by constructing a rough 3D shape of the object, followed by directly editing the 3D shape, and finally performing refinement to obtain the final edits. While this method achieves high-quality results, it is computationally intensive and slow, limiting its practical applications. In contrast, feed-forward approaches like 3DIT <ref type="bibr" target="#b13">[Michel et al. 2024] and</ref><ref type="bibr">Magic Fixup [Alzayer et al. 2024</ref>] leverage conditional diffusion models to guide the editing process, making them relatively fast and efficient. However, these methods are primarily limited by their dependence on 2D data and synthetic training sets, which either lack depth and spatial understanding or real-world data understanding. Besides, the reliance on text prompts restricts the precision and granularity of the user control, often leading to outputs that may diverge from the user's intention.</p><p>To address these challenges, we propose a feed-forward method that utilizes real-world video data enriched with 3D priors, allowing for realistic 3D-aware editing of objects in natural images. We design a novel data generation pipeline to overcome the challenge of collecting large-scale 3D-aware image editing datasets in real-world scenarios. Our pipeline generates training data by leveraging 3D transformations estimated between frames in a video, and combines those with the priors obtained from an image-to-3D model. This intermediate 3D guidance serves as a crucial bridge, enabling the model to learn 3D-aware editing without requiring explicit 3D annotations for every frame. By utilizing both the dynamic information from videos and the structural insights provided by 3D priors, our approach captures real-world physical dynamics while facilitating fine-grained control over edits. This innovative design allows the model to generalize effectively to natural scenes, bridging the gap between synthetic and real-world applications. In Figure <ref type="figure">1</ref>, we show some 3D-aware edits performed by our approach which allows fine-grained 3D user control while preserving object identity.</p><p>Our contributions are threefold: (1) we develop a novel data pipeline for generating 3D-aware training data from real-world videos, bridging 2D inputs with 3D editing capabilities; (2) we design an efficient feed-forward model that performs precise 3D editing on natural images using this 3D guidance; and (3) we conduct extensive evaluations, demonstrating that our method achieves realistic 3D edits and outperforms state-of-the-art approaches.</p><p>2 Related Work 2.1 3D-Aware Generative Image Editing 3D-aware image editing methods provide 3D control for image objects while maintaining consistency w.r.t. object identity, pose, and lighting during editing. Object3DIT <ref type="bibr" target="#b13">[Michel et al. 2024</ref>] is one of the earliest 3D-aware editing methods, directly operating on 3D transformation parameters as a condition in a feed-forward deep-net architecture. The method is however limited by its small synthetic training set, reducing its generality and the precision of its edits. Diffusion Handles <ref type="bibr" target="#b18">[Pandey et al. 2024]</ref> and GeoDiffuser <ref type="bibr" target="#b20">[Sajnani et al. 2024]</ref> are recent approaches that are general and precise, but use inference-time optimization to align the output to a particular edit, limiting robustness and inference speed. The space of supported 3D transformations is somewhat limited since those techniques make no attempt in explicitly reasoning about unseen parts of objects. In contrast, Image Sculpting <ref type="bibr" target="#b32">[Yenphraphai et al. 2024</ref>] leverages off-the-shelf single-view reconstruction models to enable impressive 3D-aware editing results, but also requires a computationally demanding inference-time optimization. Unlike prior approaches, our work focuses on fine-tuning large image diffusion models specifically for this task. This allows our method to exhibit remarkable robustness to challenging editing operations that could not be performed with existing baselines (see Figure <ref type="figure" target="#fig_6">8</ref>). Additionally, no inference-time optimization is needed, allowing for fast evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generative Image Editing</head><p>Drag-based methods have emerged as a prominent paradigm for interactive image editing, offering users precise control over object movement and transformation. Early methods like DragGAN <ref type="bibr" target="#b17">[Pan et al. 2023]</ref> utilized GANs for point-to-point dragging but faced challenges with generalization and editing quality. More recent methods, such as DragDiffusion <ref type="bibr" target="#b14">[Mou et al. 2023]</ref>, InstantDrag <ref type="bibr" target="#b23">[Shin et al. 2024]</ref>, and EasyDrag <ref type="bibr" target="#b8">[Hou et al. 2024]</ref>, extend this concept to diffusion models, leveraging fine-tuned or reference-guided approaches to enhance photorealism. DragonDiffusion <ref type="bibr" target="#b14">[Mou et al. 2023</ref>] stands out by avoiding fine-tuning and employing energy functions with visual cross-attention, enabling diverse editing tasks within and across images. DiffEditor <ref type="bibr" target="#b15">[Mou et al. 2024]</ref> and DiffUHaul <ref type="bibr" target="#b2">[Avrahami et al. 2024</ref>] further refine drag-style editing, addressing challenges like entanglement and enhancing consistency in dragging results. For articulated object interactions, DragAPart <ref type="bibr" target="#b10">[Li et al. 2025]</ref> focuses on part-level motion understanding, allowing edits like opening drawers or repositioning parts. In contrast, generative editing methods that do not rely on drag-based interactions offer alternative workflows for tasks like object insertion, removal, and repositioning. Ob-jectDrop <ref type="bibr" target="#b29">[Winter et al. 2024</ref>] models the effects of objects on scenes using counterfactual supervision, enabling realistic object manipulation. Meanwhile, SEELE <ref type="bibr">[Wang et al. 2024a]</ref> formulates subject repositioning as a prompt-guided inpainting task, preserving image fidelity while offering precise spatial control. Magic Fixup <ref type="bibr" target="#b0">[Alzayer et al. 2024</ref>] employs diffusion models to transform coarsely edited images into photorealistic outputs, leveraging video data to learn how objects adapt under various conditions. Similarly, Ob-jectStitch <ref type="bibr" target="#b25">[Song et al. 2023]</ref> and IMPRINT <ref type="bibr" target="#b26">[Song et al. 2024</ref>] focus on object compositing while preserving identity and harmonizing with the background, making them valuable for realistic image manipulation. However, unlike our approach, none of the methods in this paragraph benefit from a 3D-aware prior or provide controls to support 3D-aware edits like out-of-plane rotations or 3D translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Image-to-video with motion control</head><p>Image-to-video methods with motion control <ref type="bibr" target="#b4">[Bahmani et al. 2025;</ref><ref type="bibr" target="#b6">Guo et al. 2025;</ref><ref type="bibr" target="#b22">Shi et al. 2024;</ref><ref type="bibr">Wang et al. 2024b</ref>] are related to generative image editing to some extent, as any generated frame could be taken as an edited image. However, edits are limited to motions that are plausible in a video and, to our best knowledge, none of the methods provide 3D control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Our goal is 3D editing (e.g., out-of-plane rotation and translation) of a chosen object within an image. Existing 3D editing approaches that use inference-time optimization <ref type="bibr" target="#b18">[Pandey et al. 2024;</ref><ref type="bibr" target="#b32">Yenphraphai et al. 2024]</ref> suffer from excessive inference times, making them impractical in real-world applications. In contrast, feedforward approaches <ref type="bibr" target="#b13">[Michel et al. 2024]</ref> suffer from a lack of high-quality training data, limiting generality and control. We propose a feedforward 3D editing method that offers precise control and good generality. The editing workflow is illustrated in Figure <ref type="figure">2</ref>. As shown, the 3D edits we consider include out-of-plane rotations and translations</p><formula xml:id="formula_0">Input 3D Guide Output 3D Edit 3D-Fixup</formula><p>Fig. <ref type="figure">2</ref>. Inference pipeline. We assume editing instructions (possibly converted from text prompts) are in the form of 3D operations like rotation and translation. Given a mask indicating the object to be edited, we first perform image-to-3D <ref type="bibr" target="#b31">[Xu et al. 2024]</ref> to reconstruct the mesh. We then apply the user's desired 3D edit to obtain the 3D guidance. Here the 3D edit is visualized as the transformation between original mesh (pink wire-frame) and the edited mesh (cyan wire-frame). Finally, the model outputs the 3D aware editing result.</p><p>that change the perspective of the object. Formally, given a source image 𝐼 src , the user selects the object to be modified. The selection is represented via the mask 𝑀 src , which we use to obtain a rough 3D reconstruction. The user then performs a 3D edit of the rough 3D reconstruction. Upon rendering the modified 3D reconstruction we obtain the 3D guidance 𝐼 guide ∈ R 𝐻 ×𝑊 ×3 , which is used to generate the desired editing result.</p><p>To provide the necessary supervision for training the feedforward model, i.e., to obtain a source image 𝐼 src , a guidance image 𝐼 guide , and a ground truth target image 𝐼 tgt , we construct a new dataset derived from videos. For this, we develop the data processing pipeline that we describe in Section 3.1. As videos naturally capture 3D motion as well as variations in lighting and background conditions, they offer a rich source of real-world data. By integrating this dataset into the training process, our method learns to handle complex 3D transformations while ensuring photorealism and maintaining the fidelity of the edited subject.</p><p>Using this dataset, we fine-tune a pretrained diffusion model conditioned on 1) the edited guidance image 𝐼 guide , and 2) the source image 𝐼 src . Importantly, unlike prior 2D editing methods, the guidance image is obtained by 3D-transforming a full 3D reconstruction of the chosen object in the image. We describe architecture and training setup of this model in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Constructing the Dataset from Videos</head><p>Given a video, we create data pairs by sampling two frames, a source image 𝐼 src and a target image 𝐼 tgt . We use both images to compute a guidance image 𝐼 guide . Figure <ref type="figure" target="#fig_1">3</ref> provides an overview of our data processing pipeline. We discuss details next.</p><p>Flow for sampling the source and target image. We compute optical flow across all frames in a given video. If the accumulated flow across the entire video is too small, we discard the video. If the accumulated flow exceeds a threshold, we sample two frames from the video clip which we refer to as 𝐼 src and 𝐼 tgt .</p><p>Obtaining mask for the main object. We use Grounded-SAM <ref type="bibr" target="#b19">[Ren et al. 2024</ref>] to obtain object masks and filter cases with privacy, aesthetic, or insignificant 3D transformation issues. Notice that Grounded-SAM struggles to detect occluded or unusually shaped instances, which are rare. To automatically identify the main object, Given a video, we sample two frames, the source frame 𝐼 src and the target frame 𝐼 tgt , using optical flow as a cue: we discard videos where the flow indicates little motion through the entire clip. Using Image-to-3D methods, we reconstruct a mesh for the desired object for both frames. We then estimate the 3D transformation T (see Figure <ref type="figure" target="#fig_2">4</ref>) between the source frame mesh and the target frame mesh. Availability of the transformation T enables two ways to create the training data: (1) in "Transform Source", we paste the rendering of the transformed source mesh onto the target frame; (2) in "Transform Target", we paste the rendering of the target mesh onto the source frame. Data examples are shown in Figure <ref type="figure" target="#fig_3">5</ref>. We then initialize the parameters for the 3D transformation T and use an optimization to improve T: (1) We unproject the 2D correspondences on the source frame to 3D pointclouds and apply the current T to transform points to the target image; (2) we project points back to 2D and compare via an L2 loss with the 2D correspondences of the target frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paste to target frame Video</head><p>we define an "instance score" that prioritizes centrally located objects that occupy significant portions of the frame. The score is the weighted sum of inverted border score 𝑆 𝑏 and area score 𝑆 𝑎 across the video. They are calculated by</p><formula xml:id="formula_1">𝑆 𝑏 = 1 - 𝑝 inst border 𝑝 border , 𝑆 𝑎 = 𝑝 inst 𝑝 image ,<label>(1)</label></formula><p>where 𝑝 border is the number of border pixels in the whole image, 𝑝 inst border is the number of instance pixels which touch the border, 𝑝 inst is the total number of pixels of the instance, and 𝑝 image is the number of pixels of the whole image. The instance score is 𝑆 inst = 0.6 * 𝑆 𝑏 + 0.4 * 𝑆 𝑎 . We select the highest-scoring instance.</p><p>Image to 3D reconstruction. Given the source image 𝐼 src and the target image 𝐼 tgt , we perform image to 3D reconstruction using In-stantMesh <ref type="bibr" target="#b31">[Xu et al. 2024</ref>]. Specifically, given the foreground masks 𝑀 src ∈ R 𝐻 ×𝑊 and 𝑀 tgt ∈ R 𝐻 ×𝑊 , we first crop the foreground object and pad the image to a square while ensuring that the object is centered. InstantMesh then employs Zero123++ <ref type="bibr" target="#b21">[Shi et al. 2023]</ref> to generate multiview images. Subsequently, InstantMesh operates on the multiview images to compute the reconstructed meshes 𝑆 src and 𝑆 tgt for the source and target images.</p><p>Estimating 3D transformation with tracking. To obtain a coarse edit in an automated manner, we use the object meshes for the source and target image to estimate a 3D transformation. This process is illustrated in Figure <ref type="figure" target="#fig_2">4</ref>. Concretely, to estimate the 3D transformation T between the object in the source image 𝐼 src and the target image 𝐼 tgt , we first compute 𝑁 correspondences 𝑝 src and 𝑝 tgt along with their visibility maps 𝑣 src and 𝑣 tgt , using SpaTracker [Xiao Using the depth maps and correspondences, we backproject the 2D points into 3D space:</p><formula xml:id="formula_2">P src = Π -1 (𝑝 src , 𝐷 src , K),<label>(2)</label></formula><formula xml:id="formula_3">P tgt = Π -1 (𝑝 tgt , 𝐷 tgt , K),<label>(3)</label></formula><p>where Π -1 denotes the unprojection operation and K ∈ R 3×3 is the intrinsic camera matrix. The 3D points P ∈ R 3 are calculated as:</p><formula xml:id="formula_4">P = 𝐷 (𝑝) • K -1       𝑝 𝑥 𝑝 𝑦 1       ,<label>(4)</label></formula><p>where 𝐷 (𝑝) is the depth at pixel 𝑝 = (𝑝 𝑥 , 𝑝 𝑦 ).</p><p>The translation component of T is initialized by calculating the centroid offset between the two point clouds:</p><formula xml:id="formula_5">t = c tgt -c src , c = 1 𝑁 𝑁 ∑︁ 𝑖=1 P 𝑖 .<label>(5)</label></formula><p>To initialize the rotation, a coarse grid search is performed jointly over the 𝑋 , 𝑌 , and 𝑍 axes, using a step size of 10 • within the range [0 • , 360 • ]. The transformation T is optimized by minimizing the re-projection loss:</p><formula xml:id="formula_6">L reproj = 𝑁 ∑︁ 𝑖=1 ∥𝑝 tgt,𝑖 -Π(TP src,𝑖 , K)∥ 2 2 , (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where Π is the projection operation:</p><formula xml:id="formula_8">Π(P, K) = K 0 P. (7)</formula><p>The optimization iteratively adjusts T until convergence. Eq. 6 assumes T to be rigid transformations. However, for non-rigid transformations, Eq. 6 finds a close rigid approximation. During training, the model learns from non-rigid transformation as ground truth while using a rigid approximation in guidance 𝐼 guide . This discrepancy is often desirable, as it encourages the model to adapt non-rigidly, ensuring the edited object fits naturally into its new context. For example, when rotating the dog in Fig. <ref type="figure">1</ref> or the horse in Fig. <ref type="figure" target="#fig_7">9</ref>, subtle posture adjustments, such as foot placement, help the resulting scene remain plausible.</p><p>Creating the training data. With the optimized transformation T computed, we have two settings to obtain the guidance image and the editing mask. We refer to the first setting as "Transform Source" (TS): the estimated 3D transformation T is applied to the source mesh 𝑆 src . The transformed mesh is rendered and pasted onto the target image 𝐼 tgt based on the target mask 𝑀 tgt to form the guidance image 𝐼 𝑇 𝑆 guide . The editing mask 𝑀 𝑇 𝑆 guide has 1.0 for the static background, 0.5 for the rendered regions, and 0.0 for the holes created by cropped the object in the 𝐼 tgt . We refer to the second setting as "Transform Target" (TT): we transform and render the target mesh 𝑆 tgt onto the source frame 𝐼 src to obtain the guidance image 𝐼 𝑇𝑇 guide . The background is warped based on the flow computed between the source frame and the target frame following Alzayer et al. <ref type="bibr" target="#b0">[Alzayer et al. 2024]</ref>. 𝑀 𝑇𝑇 guide is similar to 𝑀 𝑇 𝑆 guide for the background (1.0) and rendered regions (0.5), while the holes (0.0) indicated the warped regions. Thus, the final training tuple for "Transform Source" and "Transform Target" are (𝐼 source , 𝐼 𝑇 𝑆 guide , 𝑀 𝑇 𝑆 guide , 𝐼 target ), and (𝐼 source , 𝐼 𝑇𝑇 guide , 𝑀 𝑇𝑇 guide , 𝐼 target ) respectively. We sample data pair randomly from TT or TS when training the model. Both the editing masks 𝑀 𝑇 𝑆 guide and 𝑀 𝑇𝑇 guide guide the model to inpaint missing regions (0.0), enhance 3D-transformed areas (0.5), and preserve intact content (1.0). Examples of the collected data are illustrated in Figure <ref type="figure" target="#fig_3">5</ref>.</p><p>Our dataset is sourced from 8 million licensed videos. We discard videos exceeding 500 frames due to high compute costs, reducing the dataset to 2 million. After filtering cases depicting humans (privacy and aesthetics issues), illustrations and drone videos (insignificant 3D transformations), and videos without detected entities, we retain 375k clips. Further motion-based flow filtering reduces this to 50k videos. Note that the proposed data pipeline is general and applicable to any video dataset. Processing each video takes ∼95.7s and requires 14.41GB of memory on a single A100 GPU. The pipeline includes flow filtering, mask extraction, image-to-3D, tracking, and 3D estimation, with videos averaging 50-500 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">3D Editing with a Diffusion Model</head><p>Our diffusion model aims to generate realistic images that complete the 3D edit specified by the guidance image 𝐼 guide . Hence, the structure of the image should be preserved as outlined in the guidance. For regions indicated by the mask 𝑀 guide , the model performs the following operations: inpainting missing regions (𝑀 guide = 0.0), modifying ambiguous regions (𝑀 guide = 0.5), and preserving content and identity in confident regions (𝑀 guide = 1.0).</p><p>An overview of our architecture is provided in Figure <ref type="figure" target="#fig_4">6</ref>. We base our architecture on <ref type="bibr">MagicFixup [Alzayer et al. 2024</ref>] and adopt two networks in our pipeline: a generator for generating the output image 𝑓 gen and an extractor for extracting details 𝑓 detail from the source image 𝐼 src . We use those networks in a diffusion process which operates as follows: the diffusion forward process progressively adds Gaussian noise to an image, yielding a sequence of intermediate states: </p><formula xml:id="formula_9">𝑥 𝑡 ∼ N ( √ 𝛼 𝑡 𝑥 𝑡 -1 , (1 -𝛼 𝑡 )I),<label>(8)</label></formula><formula xml:id="formula_10">𝑥 𝑇 = √ ᾱ𝑇 𝐼 guide + √︁ 1 -ᾱ𝑇 𝜖,<label>(10)</label></formula><p>where 𝜖 ∼ N (0, I) and ᾱ𝑇 = 𝑇 𝑠=1 𝛼 𝑠 . This initialization ensures alignment with the guidance while bridging the gap between training and inference domains. The extractor 𝑓 detail operates on the source image 𝐼 src for referencing and with the goal to preserve finegrained details and object identity. To ensure compatibility with the diffusion process, we add noise to the source image:</p><formula xml:id="formula_11">𝐼 𝑡 = √ ᾱ𝑡 𝐼 src + √ 1 -ᾱ𝑡 𝜖, 𝜖 ∼ N (0, I), ᾱ𝑡 = 𝑡 𝑠=1 𝛼 𝑠 . (<label>11</label></formula><formula xml:id="formula_12">)</formula><p>From this noisy source image, the extractor computes the feature</p><formula xml:id="formula_13">𝐹 𝑡 = [𝑓 1 𝑡 , . . . , 𝑓 𝑛 𝑡 ] = 𝑓 detail ([𝐼 𝑡 , 𝐼 src , 𝑀 guide ]; 𝑡),<label>(12)</label></formula><p>for each self-attention block. Here 𝑛 is the number of attention blocks, and [•] denotes concatenation along the channel dimension. These features are injected into the model through cross-attention layers, enabling details preservation from the source image to outputs during synthesis. For each layer 𝑖, at step 𝑡, the features 𝐹 𝑡 extracted by the extractor 𝑓 detail serve as keys 𝐾 and values 𝑉 , while the features of the generator 𝑓 gen [𝑔 1 𝑡 , . . . , 𝑔 𝑛 𝑡 ] act as queries 𝑄. Formally, we have</p><formula xml:id="formula_14">𝐴 𝑖 𝑡 = softmax( 𝑄 𝑖 𝑡 𝐾 𝑖⊤ 𝑡 √ 𝑑</formula><p>), and</p><formula xml:id="formula_15">𝐺 𝑖 𝑡 = 𝐴 𝑖 𝑡 𝑉 𝑖 𝑡 ,<label>(13)</label></formula><p>where 𝑄 𝑖 𝑡 , 𝐾 𝑖 𝑡 , and 𝑉 𝑖 𝑡 are query, key, and value projections of the respective features. This mechanism ensures that fine details from 𝐼 src are faithfully transferred to the synthesized output.</p><p>During inference, user instructions (e.g., text prompts, drags on 3D objects) are converted into 3D transformations T (out-of-plane rotations and translations). Using InstantMesh <ref type="bibr" target="#b31">[Xu et al. 2024]</ref>, we perform image-to-3D reconstruction to generate a 3D mesh of the subject. Applying T to the mesh, we obtain the guidance for editing.</p><p>The model uses this guidance 𝐼 guide along with the mask 𝑀 guide to produce the final output. Figure <ref type="figure">2</ref> illustrates this framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation details</head><p>For fair comparisons, following the state-of-the-art Magic-Fixup <ref type="bibr" target="#b0">[Alzayer et al. 2024]</ref>, we train our model starting from pretrained weights of Stable Diffusion 1.4. Training samples are drawn from data settings-TT, TS, MF-with probabilities (0.35, 0.35, 0.3), where MF is sampling from Magic Fixup's data. To encourage identity preservation, we drop the conditioning on 𝐼 src with a 0.2 probability, forcing the model to rely on 𝐼 src 's context. We train the model with a batch size of 8, using AdamW <ref type="bibr" target="#b12">[Loshchilov 2017</ref>] and a learning rate of 1e-5 on 8 NVIDIA A100 GPUs for about two days. We use a linear diffusion noise schedule, with 𝛼 1 = 0.9999, 𝛼 𝑇 = 0.98, and 𝑇 = 1000. We use DDIM for sampling with 100 steps during inference time.</p><p>The images were all cropped to 512 × 512 for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the proposed method both qualitatively and quantitatively on a set of edits. For this, we curated a set of user edits to show the use cases of the model in practical applications. We also created a test dataset which contains large 3D transformations to validate the proposed method.</p><p>Dataset. We use stock video as our dataset for training and testing. The training data consists of around 50k data points which contains common objects with motions in the scene. We randomly sample diverse scenes and objects while maintaining a reasonable scale when constructing the test set.</p><p>Baseline. To validate the effectiveness of the proposed method, we compare to seven baselines: Magic Fixup [Alzayer et al. 2024], Object 3DIT <ref type="bibr" target="#b13">[Michel et al. 2024]</ref>, Zero-1-to-3 <ref type="bibr" target="#b11">[Liu et al. 2023</ref>], In-stantDrag <ref type="bibr" target="#b23">[Shin et al. 2024]</ref>, MOFA-Video <ref type="bibr" target="#b16">[Niu et al. 2024</ref>], Blended LDM <ref type="bibr" target="#b1">[Avrahami et al. 2023]</ref>, and finally Instruct-pix2pix <ref type="bibr" target="#b5">[Brooks et al. 2023]</ref>. Please refer to the supplementary materials for details regarding the baselines.</p><p>Metrics. For quantitative evaluation, we use LPIPS <ref type="bibr" target="#b33">[Zhang et al. 2018]</ref> and FID <ref type="bibr" target="#b7">[Heusel et al. 2017]</ref> metrics. LPIPS measures the perceptual similarity to assess fidelity to the ground truth using a neural network such as AlexNet <ref type="bibr" target="#b9">[Krizhevsky et al. 2012]</ref> or VG-GNet <ref type="bibr" target="#b24">[Simonyan and Zisserman 2014]</ref>. FID (Fréchet Inception Distance) evaluates the realism of generated images by comparing their distribution to that of real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with Baselines</head><p>We compare the proposed method with several state-of-the-art image-editing methods, which operate on different types of conditions, such as the 3D transform, a drag (point correspondence), and a text prompt. The results are shown in Figure <ref type="figure">7</ref>. Similar to our approach, 3DIT <ref type="bibr" target="#b13">[Michel et al. 2024]</ref> and Zero123 <ref type="bibr" target="#b11">[Liu et al. 2023</ref>] use the 3D transform as condition. However, 3DIT fails to generate plausible results because of the domain gap between its synthetic training data and real-world images, while Zero123 struggles with identity preservation. For the dragging-based methods InstantDrag <ref type="bibr" target="#b23">[Shin et al. 2024</ref>] and MOFA-Video <ref type="bibr" target="#b16">[Niu et al. 2024]</ref>, we "Rotate the fish by -100°" Fig. <ref type="figure">7</ref>. Comparison with baselines. We compare several state of the art baselines with different kinds of conditions, such as 3D transforms, drags, inpainting masks, and text prompts. We can see that none of the baselines accurately follow the target 3D transform while preserving identity. Baselines that directly use 3D transforms suffer from a lack of good training data, and using other types of conditions makes it hard to unambiguously specify the 3D transform.</p><p>Table <ref type="table">1</ref>. Quantitative comparison to baselines. We compare to the baselines using the LPIPS and FID metrics. The result shows that the 3D editing of the proposed method is closer to the ground truth and real distribution. <ref type="bibr">Magic Fixup [Alzayer et al. 2024]</ref>  use the known correspondence between the source and transformed mesh to define an input drag. We find that drags are too ambiguous to clearly define a 3D transform and both methods struggle to interpret larger drags, such as the rotation of the goldfish or the shoes. Blended LDM <ref type="bibr" target="#b1">[Avrahami et al. 2023]</ref> takes the guidance image and the mask as inputs and adopts Blended Diffusion <ref type="bibr" target="#b3">[Avrahami et al. 2022]</ref> to refine the coarse edit, which does not preserve identity. Finally, Instruct-pix2pix <ref type="bibr" target="#b5">[Brooks et al. 2023</ref>] is instructed by a text prompt, but suffers from its ambiguity. In contrast, our proposed method can generate high-quality edits for large 3D transformations while preserving identity.</p><formula xml:id="formula_16">Model LPIPS ↓ FID (5k) ↓ FID (30k) ↓</formula><p>We also present a comparison with Magic Fixup in Figure <ref type="figure" target="#fig_7">9</ref>. The results demonstrate that our proposed method achieves more realistic images, benefiting from 3D-transformation-based guidance. For example, our method effectively handles pose changes, such as adjusting the camera's viewing direction or modifying the poses of subjects, as shown in the horse, jaguar, and cake examples. In contrast, Magic Fixup struggles with such edits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">3D Editing with Continuous Rotations</head><p>We also demonstrate that the proposed method can handle extensive 3D edits on common objects, as illustrated in Figure <ref type="figure" target="#fig_6">8</ref>. In each scenario, we progressively increase the rotation from 0 to 180 degrees along the y-axis, applying it to the reconstructed mesh to generate the 3D-transformation-based guidance image. The results show that our method successfully deals with out-of-plane 3D rotation edits, from minor adjustments to substantial transformations, highlighting the model's 3D-awareness during editing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quantitative Comparison of 3D Editing</head><p>We compute metrics to evaluate the performance of methods as shown in Tab. 1. LPIPS is calculated for each model by measuring the similarity between its outputs and the ground-truth images. The final LPIPS score is obtained as the mean value across all pairs. FID assesses the realism of the generated results by comparing the distribution of the generated images to that of real video frames. The results demonstrate that the proposed method produces outputs that are highly realistic and align well with the real data as indicated by lower FID and LPIPS values. In terms of the detail preservation, we address the challenging task of hallucinating novel views and missing parts based on 𝐼 guide . Since it relies only on single-view image to generate unseen regions, preserving identity and fine details is inherently difficult. However, LPIPS in Table <ref type="table">1</ref> shows that 3D-Fixup achieves better detail preservation than prior methods. Finally, we also compare the runtime with Image-sculpting <ref type="bibr" target="#b32">[Yenphraphai et al. 2024</ref>], an optimization-based method for image editing. The runtime is ∼ 877𝑠 per sample, while ours can achieve ∼ 2𝑠 for 50 DDIM steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study of Data Setting and Conditioning</head><p>We evaluate the importance of each data setting in Tab. 2. Using the setting with all the data yields the best performance in terms of FID and LPIPS. We also evaluate the effect of different conditioning mechanisms in Tab. 3. First, we consider different mask settings: (0.0, 0.5, 1.0) vs. (0.0, 1.0). Then we study the impact of dropout of image features for cross-attention. The results suggest that our conditioning outperforms other configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduced 3D-Fixup, a workflow that tackles the problem of 3Daware image editing. To make 3D-aware image editing efficient, we adopt a feedforward method. To train such a model, data is crucial. Since collecting data with corresponding 3D edits is time-consuming and expensive, we developed an automatic framework to collect suitable data from real-world videos. The resulting method bridges the gap between 2D image editing and 3D transformations, enabling realistic edits that preserve content identity while maintaining fidelity across various perspectives.</p><p>We demonstrated the effectiveness of our approach through extensive experiments, showcasing its ability to handle large out-ofplane rotations and translations, as well as challenging scenarios involving significant pose changes. Quantitative evaluations using LPIPS and FID metrics validate the realism and accuracy of our edits, outperforming state-of-the-art baselines such as Magic Fixup.</p><p>We found that intricate details, such as sprinkles on donuts or textures on clothing, are sometimes not preserved well, likely due to image encoder limitations. Additionally, 3D-Fixup produces suboptimal results when 𝐼 guide is of low quality. This occurs when the image-to-3D step performs poorly due to occlusion, incompleteness, or a suboptimally detected mask, which can be mitigated by outpainting masks/objects. Future work may explore extending the framework to handle more complex scenes with multiple objects and refining the underlying 3D priors to enhance generalization across diverse datasets. We demonstrate that the proposed method can handle extensive 3D edits on common objects. In each scenario, we progressively increase the rotation from 0 to 180 degrees along the y-axis, applying it to the reconstructed mesh to generate the 3D-transformation-based image guidance. The results show that our method can handle large out-of-plane 3D rotations during editing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>3D Edit 3D Guide Magic Fixup Ours Ground Truth Mask The results demonstrate that the proposed method achieves more realistic outputs by leveraging the 3D-transformationbased guidance. For instance, our method effectively handles pose changes, such as adjusting the camera's viewing direction for the cakes and jaguar, or modifying the poses of the horse and parrot.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FigFig. 3 .</head><label>3</label><figDesc>Fig. 4. Estimate T</figDesc><graphic coords="4,137.96,165.48,103.94,65.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Data pipeline: Estimation of the 3D transformation T. We estimate the 3D transformation T by leveraging correspondences between the source frame and the target frame. Given frames between the source frame and the target frame, we first perform tracking to obtain corresponding points. We then initialize the parameters for the 3D transformation T and use an optimization to improve T: (1) We unproject the 2D correspondences on the source frame to 3D pointclouds and apply the current T to transform points to the target image; (2) we project points back to 2D and compare via an L2 loss with the 2D correspondences of the target frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Examples of the training data. Given a video, we use the steps described in Figure 3 to obtain the training data, i.e., the source image 𝐼 src and the target image 𝐼 tgt . The guidance image is obtained via the developed data pipeline. The mask has three values: 0 indicates the hole created by the coarse edit and the model needs to inpaint by looking at the details of the reference; 0.5 refers to the rendering of the object; and 1.0 denotes the original background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Overview of the training pipeline. We develop a conditional diffusion model for 3D-aware image editing. It consists of two networks: 𝑓 gen and 𝑓 detail .During training, given the inputs-target frame 𝐼 tgt , 3D guidance 𝐼 guide , mask 𝑀 guide , and detail feature 𝐹 𝑡 -𝑓 gen learns the reverse diffusion process to predict the noise 𝜖 and reconstruct 𝐼 tgt . To better preserve identity and fine-grained details from the source image 𝐼 src , 𝑓 detail takes as input the source image 𝐼 src , its noisy counterpart 𝐼 𝑡 , and the mask 𝑀 guide , and extracts detail features 𝐹 𝑡 . We apply cross-attention between 𝐹 𝑡 and the intermediate features of 𝑓 gen to incorporate content and details from 𝐼 src during the reverse diffusion process.</figDesc><graphic coords="5,52.33,177.46,75.67,75.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. 3D editing with continuous rotations. We demonstrate that the proposed method can handle extensive 3D edits on common objects. In each scenario, we progressively increase the rotation from 0 to 180 degrees along the y-axis, applying it to the reconstructed mesh to generate the 3D-transformation-based image guidance. The results show that our method can handle large out-of-plane 3D rotations during editing.</figDesc><graphic coords="9,138.72,588.60,59.81,59.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Comparison with Magic Fixup.The results demonstrate that the proposed method achieves more realistic outputs by leveraging the 3D-transformationbased guidance. For instance, our method effectively handles pose changes, such as adjusting the camera's viewing direction for the cakes and jaguar, or modifying the poses of the horse and parrot.</figDesc><graphic coords="10,135.28,283.55,64.40,64.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>which gradually resemble a standard Gaussian as diffusion time 𝑡 increases towards a final timestep 𝑇 . Here 𝛼 𝑡 defines the noise schedule at diffusion time 𝑡. The model learns the reverse process: a standard Gaussian input 𝑥 𝑇 ∼ N (0, I) is gradually denoised toward intermediate states 𝑥 𝑡 before eventually arriving at the final estimated image 𝑥 0 :𝑥 𝑡 -1 = 𝑓 gen (𝑥 𝑡 , 𝐼 guide , 𝑀 guide , 𝐹 𝑡 , 𝑡; 𝜃 ).At each denoising step 𝑡, the model is conditioned on the guidance 𝐼 guide , mask 𝑀 guide , and the features 𝐹 𝑡 extracted by the extractor 𝑓 detail . We initialize the process from a noisy version of the guidance image, i.e., we use</figDesc><table><row><cell>(9)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results for training with different sets of training data.</figDesc><table><row><cell>Data setting</cell><cell>LPIPS ↓ FID ↓</cell></row><row><cell>Transform source</cell><cell>0.3874 151.6589</cell></row><row><cell>Transform source + MF</cell><cell>0.3321 145.3312</cell></row><row><cell cols="2">Transform source + Transform Target + MF 0.2397 132.1145</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation study of conditioning. We evaluate the effect of different conditioning mechanisms with two mask configurations: (0.0,0.5,1.0) v.s. (0.0,1.0), and the impact of dropout of image features.</figDesc><table><row><cell cols="4">Configurations Mask (0.0, 1.0) Without dropout Ours</cell></row><row><cell>FID ↓</cell><cell>17.5615</cell><cell>21.2870</cell><cell>13.0228</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>, Vol. 1, No. 1, Article . Publication date: May 2025.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Work supported in part by <rs type="funder">NSF</rs> grants <rs type="grantNumber">2008387</rs>, <rs type="grantNumber">2045586</rs>, <rs type="grantNumber">2106825</rs>, <rs type="grantNumber">MRI 1725729</rs>, <rs type="funder">NIFA</rs> award <rs type="grantNumber">2020-67021-32799</rs>, and the <rs type="institution">Amazon-Illinois Center on AI for Interactive Conversational Experiences</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qTCaYGG">
					<idno type="grant-number">2008387</idno>
				</org>
				<org type="funding" xml:id="_fNxqfBf">
					<idno type="grant-number">2045586</idno>
				</org>
				<org type="funding" xml:id="_dt5abuS">
					<idno type="grant-number">2106825</idno>
				</org>
				<org type="funding" xml:id="_rcDqHBe">
					<idno type="grant-number">MRI 1725729</idno>
				</org>
				<org type="funding" xml:id="_gZ5vw99">
					<idno type="grant-number">2020-67021-32799</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Hadi Alzayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuaner</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Gharbi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.13044</idno>
		<title level="m">Magic Fixup: Streamlining Photo Editing by Watching Dynamic Videos</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Blended latent diffusion</title>
		<author>
			<persName><forename type="first">Omri</forename><surname>Avrahami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Omri</forename><surname>Avrahami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rinon</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weili</forename><surname>Nie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.01594</idno>
		<title level="m">DiffUHaul: A Training-Free Method for Object Dragging in Images</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Blended diffusion for text-driven editing of natural images</title>
		<author>
			<persName><forename type="first">Omri</forename><surname>Avrahami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Fried</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Sherwin</forename><surname>Bahmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Yifan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Skorokhodov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeong Joon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
		<title level="m">Tc4d: Trajectory-conditioned text-to-4d generation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="53" to="72" />
		</imprint>
	</monogr>
	<note>European Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Instructpix2pix: Learning to follow image editing instructions</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="18392" to="18402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sparsectrl: Adding sparse controls to text-to-video diffusion models</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="330" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">EasyDrag: Efficient Point-based Manipulation on Diffusion Models</title>
		<author>
			<persName><forename type="first">Xingzhong</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haihang</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8404" to="8413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dragapart: Learning a part-level motion prior for articulated objects</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanxia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="165" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Zero-1-to-3: Zero-shot one image to 3d object</title>
		<author>
			<persName><forename type="first">Ruoshi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rundi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basile</forename><surname>Van Hoorick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="9298" to="9309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><surname>Loshchilov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object 3dit: Language-guided 3d-aware image editing</title>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Bhattad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Vanderbilt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Chong</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiechong</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.02421</idno>
		<title level="m">Dragondiffusion: Enabling drag-style manipulation on diffusion models</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Diffeditor: Boosting accuracy and flexibility on diffusion-based image editing</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiechong</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8488" to="8497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mofa-video: Controllable image animation via generative motion field adaptions in frozen image-to-video diffusion model</title>
		<author>
			<persName><forename type="first">Muyao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="111" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Drag your gan: Interactive point-based manipulation on the generative image manifold</title>
		<author>
			<persName><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Leimkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhimitra</forename><surname>Meka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2023 Conference Proceedings</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Karran</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Metheus</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Hold-Geoffroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niloy</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<title level="m">Diffusion Handles: Enabling 3D Edits for Diffusion Models by Lifting Activations to 3D. CVPR</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.14159[cs.CV]</idno>
		<title level="m">Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Sajnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeroen</forename><surname>Vanbaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kapil</forename><surname>Katyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.14403[cs.CV]</idno>
		<title level="m">GeoDiffuser: Geometry-Based Image Editing with Diffusion Models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hansheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.15110[cs.CV]</idno>
	</analytic>
	<monogr>
		<title level="m">Single Image to Consistent Multi-view Diffusion Base Model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">123</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Motioni2v: Consistent and controllable image-to-video generation with explicit motion modeling</title>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu-Yun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weikang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dasong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ka</forename><forename type="middle">Chun</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><surname>Hongwei Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2024 Conference Papers</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">InstantDrag: Improving Interactivity in Drag-based Image Editing</title>
		<author>
			<persName><forename type="first">Joonghyuk</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daehyeon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2024 Conference Papers</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Objectstitch: Object compositing with diffusion model</title>
		<author>
			<persName><forename type="first">Yizhi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soo</forename><forename type="middle">Ye</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Aliaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="18310" to="18319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imprint: Generative object compositing by learning identity-preserving representation</title>
		<author>
			<persName><forename type="first">Yizhi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soo</forename><forename type="middle">Ye</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Aliaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8048" to="8058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liping</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.01566</idno>
		<title level="m">Generating rich and controllable motions for video synthesis</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Yikai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaole</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.16861</idno>
		<title level="m">Xiangyang Xue, and Yanwei Fu. 2024a. Repositioning the Subject within Image</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shlomi</forename><surname>Fruchter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.18818</idno>
		<title level="m">ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object Removal and Insertion</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SpatialTracker: Tracking Any 2D Pixels in 3D Space</title>
		<author>
			<persName><forename type="first">Yuxi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangzhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="20406" to="20417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Instantmesh: Efficient 3d mesh generation from a single image with sparse-view large reconstruction models</title>
		<author>
			<persName><forename type="first">Jiale</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.07191</idno>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image sculpting: Precise object editing with 3d geometry control</title>
		<author>
			<persName><forename type="first">Jiraphon</forename><surname>Yenphraphai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xichen</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sainan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Panozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="4241" to="4251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
