<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Distillation for Enhancing Walmart E-commerce Search Relevance Using Large Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2025-05-11">11 May 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hongwei</forename><surname>Shang</surname></persName>
							<email>hongwei.shang@walmart.com</email>
							<idno type="ORCID">0009-0005-9856-9178</idno>
						</author>
						<author>
							<persName><forename type="first">Nguyen</forename><surname>Vo</surname></persName>
							<email>vknguyen09@gmail.com</email>
							<idno type="ORCID">0000-0001-5888-4740</idno>
						</author>
						<author>
							<persName><forename type="first">Tian</forename><surname>Zhang</surname></persName>
							<email>tian.zhang@walmart.com</email>
						</author>
						<author>
							<persName><forename type="first">Shuyi</forename><surname>Chen</surname></persName>
							<email>shuyi.chen@walmart.com</email>
						</author>
						<author>
							<persName><forename type="first">Prijith</forename><surname>Chandran</surname></persName>
							<email>prijith.chandran@walmart.com</email>
						</author>
						<author>
							<persName><forename type="first">Changsung</forename><forename type="middle">2025</forename><surname>Kang</surname></persName>
							<email>changsung.kang@walmart.com</email>
						</author>
						<author>
							<persName><forename type="first">Nitin</forename><surname>Yadav</surname></persName>
							<email>nitin.yadav@walmart.com</email>
						</author>
						<author>
							<persName><forename type="first">Ajit</forename><surname>Puthenputhussery</surname></persName>
							<email>ajit.puthenputhussery@walmart.com</email>
						</author>
						<author>
							<persName><forename type="first">Xunfan</forename><surname>Cai</surname></persName>
							<email>xunfan.cai@walmart.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Walmart Global Tech Sunnyvale</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Amazon Inc. Sunnyvale</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Walmart</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>Walmart</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<settlement>Walmart</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<address>
									<settlement>Walmart</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<address>
									<settlement>Walmart</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge Distillation for Enhancing Walmart E-commerce Search Relevance Using Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-05-11">11 May 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">522C74C2FC0E12142257093C4FA75BEE</idno>
					<idno type="DOI">10.1145/3701716.3715242</idno>
					<idno type="arXiv">arXiv:2505.07105v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-19T11:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LLM</term>
					<term>Knowledge Distillation</term>
					<term>E-commerce Search</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ensuring the products displayed in e-commerce search results are relevant to users' queries is crucial for improving the user experience. With their advanced semantic understanding, deep learning models have been widely used for relevance matching in search tasks. While large language models (LLMs) offer superior ranking capabilities, it is challenging to deploy LLMs in real-time systems due to the high-latency requirements. To leverage the ranking power of LLMs while meeting the low-latency demands of production systems, we propose a novel framework that distills a highperforming LLM into a more efficient, low-latency student model. To help the student model learn more effectively from the teacher model, we first train the teacher LLM as a classification model with soft targets. Then, we train the student model to capture the relevance margin between pairs of products for a given query using mean squared error loss. Instead of using the same training data as the teacher model, we significantly expand the student model's dataset by generating unlabeled data and labeling it with the teacher model's predictions. Experimental results show that the student model's performance continues to improve as the size of the augmented training data increases. In fact, with enough augmented data, the student model can outperform the teacher model. The student model has been successfully deployed in production at Walmart.com with significantly positive metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, there has been exceptionally rapid growth in ecommerce platforms. On e-commerce platforms such as Walmart, Amazon and Taobao, customers enter search queries, and the platform must deliver the most relevant items from millions of products in the product catalog. As the product catalog continues to grow, continuously improving search ranking systems has become essential for showing customers highly relevant items on e-commerce platforms. Product search ranking engine typically comprises two main stages: the retrieval stage where a set of relevant products candidates is retrieved from the product catalog to form the recall set, the rerank stage where the candidates obtained from the previous stage are re-ranked to form a ranked product list to be shown to customers.</p><p>The embedding-based two tower model has become an important approach in the retrieval stage of e-commerce search engines, especially the bi-encoder which uses transformer-encoded representations. Bi-encoder models learn embedding representations for search queries and products from training data. At retrieval time, semantically similar items are retrieved based on simple similarity metrics, such as the cosine similarity between the query and the item. Since the query and document embeddings are learned through separate models, this approach allows the embeddings to be precomputed offline with minimal computational overhead and low latency <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">37]</ref>. In contrast, the cross-encoder model takes the query and item text as input and directly outputs a prediction.</p><p>Although bi-encoder approaches are typically much faster, they are generally less effective than cross-encoder methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref>. By concatenating query and item as input, the cross-encoder model can benefit from the attention mechanism across all tokens in the inputs, thus capturing interactions between query and item text. As a result, the trade-off with bi-encoders involves sacrificing the model's effectiveness for large latency gain. The retrieval stage has strict latency requirements as it needs to search through millions of products in the product catalog. However, the downstream re-rank stage only ranks the products retrieved during the retrieval stage, allowing it to afford more relaxed latency requirements compared to the retrieval stage.</p><p>Our previous production model is the first ranking model with a cross-encoder (XE) feature that can better understand the relevance of items to search queries. In this work, we present our our enhanced BERT-Base (ùêµùê∏ùëÖùëá Base ) model with cross-encoder architecture, optimized through knowledge distillation from LLMs on an augmented large-scale training dataset.</p><p>In this work, our goal is to improve effectiveness of cross-encoder models used in production for the tail segment. To achieve this goal, we propose a novel knowledge distillation framework that distills the teacher LLM model (i.e. 7 billion parameters) into a student model (i.e. ùêµùê∏ùëÖùëá Base <ref type="bibr" target="#b10">[11]</ref>) with significantly augmented training data. This framework improves the student model's effectiveness while preserving its efficiency. First, we train the LLM teacher model using soft-label targets converted from human editorial labels. In addition to item's title, product type, brand, color and gender, we further incorporate item's description into the model input to improve our teacher LLM model's performance. After that, the student model is trained to mimic the margin between two documents predicted by the teacher model. Rather than distilling the knowledge on the labeled training data, we generate large-scale unlabeled data from user's log data, and label it with the teacher model's predictions such that the student model can learn from the teacher at a much larger dataset. The effectiveness of our cross-encoder based student model is demonstrated through both offline and online experiments, showing its ability to improve both the relevance and engagement metrics of the system. Our cross-encoder based student model has been deployed in Oct 2024 as the most dominant feature in the current production system, and is powering tail queries in Walmart.com's e-commerce engine.</p><p>Our main contributions are summarized as follows:</p><p>‚Ä¢ We propose a knowledge distillation framework in which the teacher model is trained with soft-labeling, enabling more effective distillation through the teacher's single output. The distillation process uses a extended Margin-MSE loss on an augmented, large-scale unlabeled dataset. Building on the work of Hofst√§tter et al. <ref type="bibr" target="#b6">[7]</ref>, where both the student and teacher models share the same training data and require true labels for distillation, we significantly expand the dataset used to train the student model. Specifically, we generate a large-scale unlabeled dataset and use the teacher model's predictions to label the data. The Margin-MSE loss is then computed on all item pairs for a given query, eliminating the need for true labels. ‚Ä¢ We conduct an ablation study that reveals the following findings: (1) using Margin-MSE loss to align the margin with the teacher model significantly improves knowledge distillation compared to using pointwise cross-entropy (CE) loss.</p><p>(2) distilling knowledge through a large-scale dataset labeled with the teacher's predictions leads to a substantial performance boost, enabling the student model to achieve results comparable to the teacher model. ( <ref type="formula">3</ref>) the student model performs on par with the teacher model, even without the 'item description' field, which has been shown to be beneficial. These results demonstrate that increasing the model's capacity is not necessary to effectively learn from the teacher model. ‚Ä¢ Our proposed model has been successfully deployed in production at Walmart.com with significantly positive metrics.</p><p>The rest of this paper is organized as follows: in Section 2, we review related work and discuss the previous production model. Section 3 outlines our model architecture and loss functions. In Section 4, we present our offline experiment setup and evaluation metrics, followed by the online test results in Section 5. Finally, Section 6 concludes with a discussion of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Search ranking has long been a popular topic within the search community. Traditional method in search ranking are based on text match between queries and items like LSI <ref type="bibr" target="#b1">[2]</ref>, BM25 <ref type="bibr" target="#b23">[24]</ref> and its variations <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>. They struggle with handling the lexical gap between queries and product descriptions. Neural network models can learn representations of queries and documents from raw text, helping to bridge the lexical gap between the query and documents vocabularies to some extent. Many large e-commerce platforms have deployed neural network models in their product search engines e.g. Walmart <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref>, Amazon <ref type="bibr" target="#b19">[20]</ref>, Facebook Marketplace <ref type="bibr" target="#b4">[5]</ref>, Taobao <ref type="bibr" target="#b39">[40]</ref>, JD.com <ref type="bibr" target="#b31">[32]</ref>, Baidu <ref type="bibr" target="#b2">[3]</ref>. Recent advancements in e-commerce search have focused on various aspects, including retrieval, ranking, and query understanding systems. Lin et al. <ref type="bibr" target="#b11">[12]</ref> addressed relevance degradation in Walmart's retrieval system with a relevance reward model, while Peng et al. <ref type="bibr" target="#b22">[23]</ref> introduced an entity-aware model to enrich query representation with engagement data. Luo et al. <ref type="bibr" target="#b15">[16]</ref> proposed a multitask learning framework for ranking optimization at Amazon, and Peng et al. <ref type="bibr" target="#b21">[22]</ref> developed a query rewriting framework to address the semantic gap in long-tail queries.</p><p>Recent advances in LLMs have revolutionized semantic search, with fine-tuned LLMs achieving accuracy close to human performance on labeled datasets <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27]</ref>. However, due to latency constraints, LLMs cannot be deployed for real-time inference in production. Knowledge distillation <ref type="bibr" target="#b5">[6]</ref> bridges this gap by transferring LLM capabilities to lighter, deployable models. Knowledge distillation methods can be mainly categorized into three types: response-based <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>, representation-based <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36]</ref>, and relation-based methods <ref type="bibr" target="#b13">[14]</ref>. Specifically, Hofst√§tter et al. <ref type="bibr" target="#b6">[7]</ref> introduced a margin focused loss (Margin-MSE) to adapt knowledge  <ref type="bibr" target="#b24">[25]</ref> introduced a novel instruction distillation method to improve the efficiency of LLMs by converting complex pairwise ranking into more efficient pointwise ranking. Building on the work of Hofst√§tter et al. <ref type="bibr" target="#b6">[7]</ref>, we expand the student model's dataset by incorporating augmented unlabeled data, which we label using the teacher model's predictions. Our knowledge distillation approach does not require manual labeling of query-item pairs for the student model.</p><p>Previous Production Model. The previous BERT-based cross-encoder model treats the classification task as a multi-class classification problem, outputting probabilities over three classes (class 0, class 1, and class 2). It uses a Walmart in-house pre-trained BERT model as the starting checkpoint, which is then fine-tuned on human-labeled query-item pairs (QIPs) (human-judged_v1 in 3). It aims to classify the relevance of these pairs. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the query and item text are concatenated into a single string and fed into the BERT encoder. After encoding, non-linear layers transform the embedding into a three-dimensional classification vector, which is then passed through a softmax layer to generate a three-class probability vector. This vector is optimized using cross-entropy loss.</p><p>The outputs of the cross-encoder model are used as features in our tail relevance model, which ranks results for the tail queries. While the launch of the cross-encoder in April 2024 marked a significant improvement over the previous model (which lacked the cross-encoder feature), there is still substantial room for further enhancement in relevance for long-tail queries. One limitation of the previous model was the insufficient training data, primarily due to the high costs associated with human annotators' labeling. Additionally, the model suffered from a lack of sufficient negativelabel examples and the presence of incorrect labels in the training data. Given a query ùëû and an item ùëë, where each item ùëë has attributes such as title, product type, brand, color, gender and description, our goal is to train a teacher model using LLM, and then distill this knowledge into a more lightweight student model using a largescale, augmented dataset. We denote the predicted relevance for query ùëû and document ùëë as ùë° (ùëû, ùëë) ‚àà R from the teacher model, and ùë† (ùëû, ùëë) ‚àà R from the student model. Our guidelines categorize each (ùëû, ùëë) pair into five ordered ratings, 4, 3, 2, 1, and 0. These ratings are then mapped to three classes (Class 2, Class 1, Class 0) based on our production requirements, as shown in Table <ref type="table" target="#tab_1">2</ref>. Examples of these three relevance classes are provided in Table <ref type="table" target="#tab_0">1</ref>.</p><p>In this work, we formulate search relevance prediction as a binary classification problem with soft targets <ref type="bibr" target="#b30">[31]</ref>. We map the three-class labels into soft targets (as shown in Table <ref type="table" target="#tab_1">2</ref>), where an excellent item is labeled as 1, a good item as 0.5, and an irrelevant item as 0. The advantage of predicting a single output is that it enhances the effectiveness of knowledge distillation. Our framework consists of two main components: (1) the cross-encoder architecture based LLM used as the teacher model, (2) a more lightweighted crossencoder architecture model serving as the student model. First, we fine-tuned the LLM as a teacher model using human editorial labels. Then, we use the teacher model to make inferences on a large-scale, augmented set of unlabeled query-item pairs obtained from search log data, generating a substantial training data for the student model. The details of these components are provided in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LLM as the Teacher Model</head><p>We use LLMs to predict the relevance of an item to a query based on textual information. For training our teacher model, we employ two 7B-parameter open-source LLMs: Llama2-7B <ref type="bibr" target="#b27">[28]</ref> and Mistral-7B-v0.1 <ref type="bibr" target="#b8">[9]</ref>. The model training is implemented using PyTorch <ref type="bibr" target="#b20">[21]</ref> and the Huggingface Transformers framework <ref type="bibr" target="#b33">[34]</ref>. Both Llama2-7B and Mistral-7B-v0.1 are fine-tuned using a cross-encoder architecture to predict the relevance between a query and an item. To improve training efficiency, we apply the softRank Adaptation (LoRA) <ref type="bibr" target="#b7">[8]</ref>, which has been shown to outperform full model fine-tuning when trained on datasets of size on the order of 10 6 <ref type="bibr" target="#b17">[18]</ref>.</p><p>Our teacher models are trained as classification models, using concatenated query and item textual features as inputs, along with soft relevance labels. The classification is based on the "last token", where the hidden state of this token is used as the representation of the query-item pair. An MLP layer is then applied to transform this representation into a single output value, which represents the predicted relevance score ùë° (ùëû, ùëë) of the teacher model. We use cross-entropy loss to train the LLM, as defined by L ùë°ùëíùëéùëê‚Ñéùëíùëü (ùëû) for each query ùëû:</p><formula xml:id="formula_0">L ùë°ùëíùëéùëê‚Ñéùëíùëü (ùëû) = ‚àëÔ∏Å ùëë ‚ààD ùëû [-ùë¶ ùëû,ùëë ‚Ä¢ log ùë° (ùëû, ùëë) -(1 -ùë¶ ùëû,ùëë ) ‚Ä¢ log(1 -ùë° (ùëû, ùëë))],</formula><p>where D ùëû is the document collection for query ùëû, and ùë¶ ùëû,ùëë ‚àà {0, 0.5, 1} is the soft label for query ùëû and document ùëë, converted from the original editorial feedback. The objective is to optimize ùë° (ùëû, ùëë) by minimizing the above loss function.</p><p>Note that the teacher model incorporates additional text data, specifically the "item description", which has been shown to improve performance in multi-billion parameter LLMs <ref type="bibr" target="#b17">[18]</ref>. According to Mehrdad et al. <ref type="bibr" target="#b17">[18]</ref>, one of the key advantages of LLMs is their ability to effectively process long item descriptions-information that is often difficult for human evaluators to review quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cross-encoder Architecture based Student Model</head><p>Our student model is built on a ùêµùê∏ùëÖùëá Base cross-encoder architecture.</p><p>Similar to the teacher model, we concatenate the query with the item's attributes, including title, product type (PT), brand, gender and color. To meet production latency requirements, we exclude the item description from the input. Each item attribute is separated by unique separator tokens, as shown below:</p><formula xml:id="formula_1">[ùê∂ùêøùëÜ ] ùëûùë¢ùëíùëü ùë¶ [ùëÜùê∏ùëÉ ] [ùëÜùê∏ùëÉ ùë° ]ùë°ùëñùë°ùëôùëí [ùëÜùê∏ùëÉ ùëù ]ùëÉùëá [ùëÜùê∏ùëÉ ùëè ]ùëèùëüùëéùëõùëë [ùëÜùê∏ùëÉ ùëê ]ùëêùëúùëôùëúùëü [ùëÜùê∏ùëÉ ùëî ]ùëîùëíùëõùëëùëíùëü</formula><p>, where Our student model training is based on a loss function similar to the margin MSE loss <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref>. Based on the observation that different architectures converge to different scoring ranges, <ref type="bibr" target="#b6">[7]</ref> proposed distilling knowledge not by directly optimizing the raw scores for each query-item pair, but by focusing on the margin between relevant and non-relevant documents for each query, using a Margin-MSE loss. The relevance of a document is determined by its true label. In their work, the training data for the student model is the same as that for the teacher model, meaning that all query-item pairs have known true labels (either relevant or non-relevant) for knowledge distillation. We extend this approach by augmenting the training data with a much larger set of unlabeled data, where the true labels are unknown. Building on the works of Hofst√§tter et al. <ref type="bibr" target="#b6">[7]</ref> and Vo et al. <ref type="bibr" target="#b30">[31]</ref> which focus on distilling knowledge from the teacher model using labeled data, we adapt the approach to handle unlabeled data. Specifically, we compute the Margin-MSE loss for all triplets (ùëû, ùëë ùëñ , ùëë ùëó ), where ùëë In contrast to Hofst√§tter et al. <ref type="bibr" target="#b6">[7]</ref>, who sample triplets (ùëû, ùëë + , ùëë -) where ùëë + is a relevant document and ùëë -is an irrelevant document based on the training data's relevance labels, our approach relies entirely on the teacher model's predictions and does not require any labeling of the query-item pairs. This extension makes knowledge distillation feasible for large-scale augmented datasets, even when they are unlabeled. </p><formula xml:id="formula_2">[</formula><formula xml:id="formula_3">L ùë†ùë°ùë¢ùëëùëíùëõùë° (ùëû) = 2 |D ùëû |(|D ùëû | -1) ‚àëÔ∏Å ùëë ùëñ ,ùëë ùëó ‚ààD ùëû , 1‚â§ùëñ&lt; ùëó ‚â§ |D ùëû | (Œî ùë° ùëû,ùëë ùëñ ,ùëë ùëó -Œî ùë† ùëû,ùëë ùëñ ,ùëë ùëó ) 2 .</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Offline Experiments 4.1 Datasets</head><p>Training Datasets. In search ranking, people usually use either search log data <ref type="bibr" target="#b34">[35]</ref> or human editorial judgment data <ref type="bibr" target="#b32">[33]</ref> for training text models. While collecting logged data from search engagements and impressions can help generate large-scale training data, we observe that for most of the tail queries, engagement data is often sparse and does not always reliably indicate relevance.</p><p>We leverage human-annotated labels following 5-scale relevance guidelines (as described in Table <ref type="table" target="#tab_1">2</ref>), to create our datasets. Our human labeled data is generated by manually assessing the top k (usually 10) items for a set of sampled queries. We refer to these as human-judged datasets. Over time, the size of the human-judged datasets increase as more data is annotated. We version our humanjudged datasets with higher version indicating addition of more annotated data. We conduct experiments using two such versions shown in Table <ref type="table" target="#tab_2">3</ref>. When training our previous production model and teacher models, we only had access to human-judged_v1 dataset.</p><p>To improve knowledge distillation from the teacher model, we also create three datasets, where the labels are generated from the teacher model's predictions. We refer to these as llm-judged datasets. These datasets are created by incrementally adding more samples, with each new version incorporating additional predictions from the teacher model. As we expand the dataset, we update the version number. This also allows us to assess the impact of scaling the training data on the student model's performance. All datasets are summarized in Table <ref type="table" target="#tab_2">3</ref>.</p><p>Test Data. We evaluate the models using a golden test dataset consisting of 2,354 queries randomly selected from the tail segment of our search traffic. For each query, we sample the top 10 products and, an additional 10 random products from the remaining results, if available. This results in a total of 32,586 QIPs in the golden dataset. Note that the golden test data was generated after all human evaluation data (used for training data) was collected, ensuring no risk of data leakage. While 32,000+ QIPs might seem small for a test set, our analysis shows that offline metrics from this golden dataset align closely with online human evaluations across multiple features. This consistency demonstrates that the offline metrics from the golden test data are a reliable indicator of model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Setup</head><p>4.2.1 Model Implementation. We use a cross-encoder based LLM model as the teacher model (with the architecture described in Section 3.2), and fine-tune two 7B pre-trained language models: Llama2-7B and Mistral-7B-v0.1. Both models are initialized from Hugging Face checkpoints and fine-tuned on the human-judged data human-judged_1. The model training is conducted on 8 NVIDIA 40GB A100 GPUs using the AdamW optimizer. Building on the work of Mehrdad et al. <ref type="bibr" target="#b17">[18]</ref> on fine-tuning LLMs with Walmart's human-annotated data, we adopt the hyperparameters identified as optimal in their study. Specifically, we set the LoRA rank to 256, as increasing it further did not yield performance gains for datasets of size around 10 6 <ref type="bibr" target="#b17">[18]</ref>. We also set the LoRA ùõº to 128 and the dropout rate to 0.05. For the learning rate, we experiment with values of 1ùëí -4 and 1ùëí -5 using a decay scheduler, and find that 1ùëí -5 produced the best results.</p><p>We use the ùêµùê∏ùëÖùëá Base model with a cross-encoder architecture as the student model (as described in Section 3.3). The student model is trained using the same learning rate of 1ùëí -5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Online</head><p>Serving. We deploy our Bert-based student model, ùëã ùê∏ùë£2, into production. The output from the ùëã ùê∏ùë£2 model for a given (query, item) pair -referred to as the ùëã ùê∏ùë£2 feature score -is used as a feature in our ranking model. To reduce latency, we implement two optimizations: 1. We precompute ùëã ùê∏ùë£2 feature scores for frequent query-item pairs as query signals through an offline pipeline. 2. We cache the ùëã ùê∏ùë£2 feature scores to further minimize real-time computation. For query-item pairs not present in the cache or query signal, the ùëã ùê∏ùë£2 model is invoked at run-time. To further enhance the runtime performance of the cross encoder model, we employ techniques like operator fusion and intermediate representations using the TensorRT framework <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Evaluation Metrics.</head><p>The models are evaluated on two tasks to measure semantic relevance: a classification task (determining whether a product is an exact match) and a ranking task. We use the following two offline metrics to assess performance:</p><p>‚Ä¢ Recall at a fixed precision: This metric evaluates performance on the classification task. It measures the percentage of relevant products retrieved at specific precision levels for our golden test dataset. In our work, we fix the precision at 90% and 95%. We denote recall at these precision levels as R@P=95% and R@P=90%, respectively. ‚Ä¢ Normalized Discounted Cumulative Gain (NDCG): This metric evaluates performance on the ranking task. Here NDCG@5 and NDCG@10 are used to measure ranking quality for the top 5 and top 10 documents, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments Results</head><p>For both teacher and student model experiments, we use the crossencoder based ùêµùê∏ùëÖùëá Base (ùëã ùê∏ùë£1, as shown in Tables <ref type="table" target="#tab_3">4</ref> and<ref type="table" target="#tab_4">5</ref>) from the previous production systems as our baseline. This choice is driven by the fact that live experiments (see Section 5) consistently compare the performance of the new models against the existing production model.</p><p>Experiments with Teacher Models. Table <ref type="table" target="#tab_3">4</ref> compares the performance of Llama2-7B and Mistral-7B to the baseline ùêµùê∏ùëÖùëá Base model on the golden test data. All three models are fine-tuned with the human-annotated dataset human-judged_1. Specifically, the ùêµùê∏ùëÖùëá Base ùëã ùê∏ùë£1 model is fine-tuned starting from a Walmart pre-trained ùêµùê∏ùëÖùëá Base checkpoint, which was trained on a masked language modeling task and a prediction task <ref type="bibr" target="#b17">[18]</ref>. In contrast, both the Llama2-7B and Mistral-7B models are fine-tuned starting from Hugging Face checkpoints. Compared to the baseline ùëã ùê∏ùë£1, both Llama2-7B and Mistral-7B achieve competitive performance on both classification and ranking tasks. Notably, Mistral-7B outperforms the baseline with +13.27% in R@P=95% and +6.52% in R@P=90%. Its NDCG@5 and NDCG@10 scores also improve by +1.33% and +1.03% respectively, showing better performance on the ranking task as well. Based on the results in Table <ref type="table" target="#tab_3">4</ref>, since Mistral-7B performs the best as a teacher model, we use its predictions to label a large-scale augmented dataset for knowledge distillation.</p><p>For the student models, we train multiple ùêµùê∏ùëÖùëá Base models on the generated large-scale datasets and conduct an ablation study to examine the impact of each factor on model performance. This process allows us to address the following research questions and share insights with machine learning practitioners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">RQ1</head><p>: What is the most effective knowledge distillation loss when both the student and teacher models use the same architecture? Previous work by Hofst√§tter et al. <ref type="bibr" target="#b6">[7]</ref> shows that different model architectures, such as cross-encoder based BERT, bi-encoder based BERT, and ColBERT, produce output scores with varying magnitudes. Their proposed Margin-MSE loss effectively adapts to these differences in score distributions.</p><p>In our case, both the student model and teacher models use the cross-encoder architecture, so we are interested in whether focusing on the margin between two items remains an effective approach for knowledge distillation. We aim to determine the most effective approach for knowledge distillation in this context. We compare our extended Margin-MSE loss (described in Section 3.3) with a pointwise cross-entropy loss, where the teacher model's prediction scores serve as the target labels. The pointwise cross-entropy loss is defined as follows:</p><formula xml:id="formula_4">L ùë†ùë°ùë¢ùëëùëíùëõùë° ‚Ä≤ (ùëû) = ‚àëÔ∏Å ùëë ‚ààD ùëû [-ùë° ùëû,ùëë ‚Ä¢ log ùë† (ùëû, ùëë) -(1 -ùë° ùëû,ùëë ) ‚Ä¢ log(1 -ùë† (ùëû, ùëë))].</formula><p>We validate our strategy with an ablation study comparing two knowledge distillation losses. As shown in Table <ref type="table" target="#tab_4">5</ref>, Model ùëã ùê∏ùë£1.2 and ùëã ùê∏ùë£1.3 were both fine-tuned on the same dataset, llm-judged_1, labeled by the Mistral-7B teacher model. The only difference between these two models is the choice of knowledge distillation loss. The results show that the Margin-MSE loss significantly outperforms the cross-entropy loss across both precision/recall and NDCG metrics. Notably, we observe substantial improvements in precision/recall, with relative gains of +14.83% at R@P=95% and +3.91% at R@P=90%. 4.3.2 RQ2: Does large-scale unlabeled training data facilitate knowledge distillation from the teacher model to the student model? Using the extended Margin-MSE as the knowledge distillation loss, we fine-tune ùêµùê∏ùëÖùëá Base models (ùëã ùê∏ùë£1.3, ùëã ùê∏ùë£1.4, ùëã ùê∏ùë£2) on LLM-labeled datasets of increasing sizes. As the dataset size grows from 50M to 110M and then to 170M, we observe consistent improvements in NDCG metrics, while performance on precision/recall metrics appears to plateau at higher data volume (see Table <ref type="table" target="#tab_4">5</ref>). Overall, as the dataset expands, the student model continues to show significant performance gains, highlighting the effectiveness of training with large-scale teacher-generated labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">RQ3:</head><p>Can a 110M-parameter student model achieve performance comparable to a 7B-parameter teacher LLM when trained on a large-scale unlabeled dataset? When compared to the Mistral-7B teacher model, ùëã ùê∏ùë£1.4 achieves identical performance on R@P=95% (+13.27% for both models) but shows a slightly lower R@P=90% (+5.92% for ùëã ùê∏ùë£1.4 vs. +6.52% for the teacher model). However, ùëã ùê∏ùë£1.4 slightly outperforms the teacher model on NDCG metrics, with +1.36% for NDCG@5 (vs. +1.33% for the teacher) and +1.04% for NDCG@10 (vs. +1.03% for the teacher). When comparing ùëã ùê∏ùë£2 to the teacher model, ùëã ùê∏ùë£2 shows notable improvements in NDCG metrics, with +1.5% for NDCG@5 (vs. +1.33% for the teacher) and +1.04% for NDCG@10 (vs. +1.03% for the teacher). ùëã ùê∏ùë£2 slightly lags behind on precision/recall metrics though, achieving +12.84% for R@P=95% (vs. +13.27% for the teacher) and +6.04% for R@P=90% (vs. +6.52% for the teacher).</p><p>Overall, despite being more than 60 times smaller than the teacher model, the student model demonstrates comparable performance, proving that it effectively learns from the teacher. This suggests that, in this context, additional model capacity may not be necessary for successful knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">RQ4</head><p>: Can the student model effectively learn from the teacher model, with fewer input features? Item descriptions have been shown to enhance performance in multi-billion parameter LLMs <ref type="bibr" target="#b17">[18]</ref>. We include the item description in the input for the teacher LLM. However, due to the long text and latency requirements in production, we exclude it from the student model. Despite removing the item description from the student model's input, the student model still achieves performance comparable to that of the teacher model. This also demonstrates the effectiveness of knowledge distillation, even when the student model has less input data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Online Experiments</head><p>We deployed the enhanced ùëã ùê∏ùë£2 feature into production for our online experiments. In these experiments, the ùëã ùê∏ùë£2 score for a query-item pair serves as a feature in our ranking model. The control model uses the existing production setup with the ùëã ùê∏ùë£1 feature, while the variation model incorporates the new ùëã ùê∏ùë£2 feature in place of ùëã ùê∏ùë£1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Manual Evaluation Results</head><p>Table <ref type="table">6</ref>: Human evaluation of top-10 rankings for tail queries by the new model NDCG@5 Lift (P-value) NDCG@10 Lift (P-value) +1.07% (0.00) +0.87% (0.01)</p><p>We evaluate the performance of the proposed ùëã ùê∏ùë£2 by having human assessors evaluate the top-10 ranking results generated by the model, with ùëã ùê∏ùë£2 as a feature, compared to the existing production model that uses ùëã ùê∏ùë£1. Queries are randomly selected from the tail segment of our search traffic. For each query, human assessors are presented with the product image, title, price and a link to the product on the Walmart website. They rate the relevance of each product on a 5-point scale, while NDCG metrics are calculated based on a mapped 3-class scale, in accordance with our production requirements. As shown in Table <ref type="table">6</ref>, the new ùëã ùê∏ùë£2 feature significantly improved relevance for tail queries, with notable lifts in both the top 5 and top 10 rankings. It is crucial to underscore the significance of this improvement given the robustness of our baseline and the maturity of our existing search ranking system. The search ranking system has already been optimized over time, making any incremental enhancement a noteworthy achievement. Therefore, it should be viewed as a substantial uplift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Interleaving Results</head><p>Interleaving <ref type="bibr" target="#b9">[10]</ref> is an online evaluation method used to compare user engagement performance between our proposed model and the previous production model at Walmart. During the 14-day interleaving period, each user is shown a mix of ranking results from both the control and variation models. The metric measured is ATC@40, the count of add-to-carts (ATC) within the top 40 positions for both the control and variation models. The results show an ATC-lift of 0.06% with a P-value of 0.00 on the top-40 ranking, indicating a significant improvement in user engagement performance. We conduct an online AB test using production tail traffic from August 26 to September 9, 2024 (two weeks). Our proposed architecture shows statistically significant improvements across multiple key engagement metrics over the previous production model. As reported in Table <ref type="table">8</ref>, the new architecture increases the average ATC rate per session by 0.8% and the ATC rate per visitor by 1.2%. Additionally, it reduces search session abandonment rate by 0.6% and lowers the average number of clicks required before an ATC event by 1.1%. These positive results lead to the successful deployment of the student model in production. It now handles the traffic for tail queries, significantly improving the customer experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Online AB Test</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Future Work</head><p>In this work, we present a knowledge distillation framework tailored for search re-ranking tasks. Our results show that the student model can be significantly improved through knowledge distillation using the Margin-MSE loss, as compared to pointwise cross-entropy loss. This suggests that learning the margin between two items is more effective than optimizing pointwise loss, even when both the student model and teacher models use a cross-encoder architecture. We also conduct an ablation study to examine the impact of augmented training data size on model performance. Our results show that increasing the training data size significantly boosts the student model's performance. However, once the data reaches a certain size, performance improvements plateau.</p><p>The student model, ùëã ùê∏ùë£2, trained using this framework, has been successfully deployed in the re-ranking stage of Walmart's e-commerce platform, resulting in notable improvements in both relevance and engagement. This gains are validated through both offline and online evaluations.</p><p>Despite these promising results, there are a few limitations in our current approach. Due to computational and time constraints, we did not experiment with different versions of human-judged data for training the teacher model. Future work could include more experiments with the teacher model, which may further enhance the performance of both the teacher and student models. Additionally, we did not incorporate query attributes (e.g., query product type) as input for the teacher model, which could provide further context and improve model performance. Furthermore, while the current ùëã ùê∏ùë£2 model focuses solely on learning relevance between queryitem pairs, we plan to explore incorporating a multi-objective loss function that combines both relevance and engagement metrics. Finally, while this current ùëã ùê∏ùë£2 model is deployed for tail queries, we aim to expand its use to head/torso query segments in the near future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture for the previous production version of the Cross encoder</figDesc><graphic coords="3,53.80,83.69,240.25,171.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our proposed knowledge distillation framework</figDesc><graphic coords="4,53.80,83.69,504.39,186.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>ùëÜùê∏ùëÉ ùë° ], [ùëÜùê∏ùëÉ ùëù ] and [ùëÜùê∏ùëÉ ùëè ], [ùëÜùê∏ùëÉ ùëê ] and [ùëÜùê∏ùëÉ ùëî ] are distinct [ùë¢ùëõùë¢ùë†ùëíùëë] tokens from the BERT vocabulary. After the encoder pass, the hidden state of [ùê∂ùêøùëÜ] token is used as the representation of the query-item pair. An MLP layer is then applied to this hidden state to generate a single prediction score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ùëñ and ùëë ùëó are documents from the document collection for query ùëû, denoted by D ùëû . Let |D ùëû | denote the number of documents in D ùëû , which results in |D ùëû |(|D ùëû | -1)/2 triplets, as shown in Equation (1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Let Œî ùë° ùëû,ùëë ùëñ ,ùëë ùëó and Œî ùë† ùëû,ùëë ùëñ ,ùëë ùëó represent the score differences for the pairs (ùëû, ùëë ùëñ ) and (ùëû, ùëë ùëó ) from the teacher and student models respectively, where Œî ùë° ùëû,ùëë ùëñ ,ùëë ùëó = ùë° (ùëû, ùëë ùëñ ) -ùë° (ùëû, ùëë ùëó ) and Œî ùë† ùëû,ùëë ùëñ ,ùëë ùëó = ùë† (ùëû, ùëë ùëñ ) -ùë† (ùëû, ùëë ùëó ). The goal of the student model is to make Œî ùë† ùëû,ùëë ùëñ ,ùëë ùëó as close as possible to Œî ùë° ùëû,ùëë ùëñ ,ùëë ùëó . The loss function for the student model for query ùëû is defined as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Example of a query and items with class 2, class 1 and class 0 respectively</figDesc><table><row><cell>Query</cell><cell cols="2">Item Class label</cell></row><row><cell>apple iphone 14 pro max</cell><cell>iphone 14 pro max</cell><cell>2</cell></row><row><cell>apple iphone 14 pro max</cell><cell>iphone 14 pro</cell><cell>1</cell></row><row><cell cols="2">apple iphone 14 pro max iphone 14 pro max case</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Rating score and relevance label mapping</figDesc><table><row><cell cols="2">Rating Description</cell><cell cols="2">Class Soft</cell></row><row><cell>Score</cell><cell></cell><cell cols="2">Label Target</cell></row><row><cell>4</cell><cell>Excellent -perfect match</cell><cell>2</cell><cell>1</cell></row><row><cell>3</cell><cell cols="2">Good -item with a mismatched attribute 1</cell><cell>0.5</cell></row><row><cell>2</cell><cell>Okay</cell><cell>0</cell><cell>0</cell></row><row><cell>1</cell><cell>bad -Irrelevant</cell><cell>0</cell><cell>0</cell></row><row><cell>0</cell><cell>Embarrassing</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">3 Architecture</cell><cell></cell><cell></cell></row><row><cell cols="2">3.1 Problem Formulation</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Statistics of Training datasets</figDesc><table><row><cell>Dataset</cell><cell cols="6">Total QIPs Unique queries Unique items Class 2 percentage Class 1 percentage Class 0 percentage</cell></row><row><cell>human-judged_v1</cell><cell>6.0M</cell><cell>709K</cell><cell>2.3M</cell><cell>64.2%</cell><cell>19.7%</cell><cell>16.1%</cell></row><row><cell>human-judged_v2</cell><cell>10M</cell><cell>1M</cell><cell>3.7M</cell><cell>63.6%</cell><cell>19.8%</cell><cell>16.6%</cell></row><row><cell>llm-judged_v1</cell><cell>50M</cell><cell>5.2M</cell><cell>15M</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>llm-judged_v2</cell><cell>110M</cell><cell>5.2M</cell><cell>22.6M</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>llm-judged_v3</cell><cell>170M</cell><cell>5.2M</cell><cell>29.4M</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Offline Experiment results for LLM teacher models</figDesc><table><row><cell>Model</cell><cell>Train Data</cell><cell>Starting Checkpoint</cell><cell cols="4">R@P=95% R@P=90% NDCG@5 NDCG@10</cell></row><row><cell cols="3">ùëã ùê∏ùë£1 (Baseline) human-judged_1 Walmart pre-trained Bert-Base</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>Llama2-7B</cell><cell>human-judged_1</cell><cell>Hugging Face checkpoint</cell><cell>+11.84%</cell><cell>+5.57%</cell><cell>+1.02%</cell><cell>+0.8%</cell></row><row><cell>Mistral-7B</cell><cell>human-judged_1</cell><cell>Hugging Face checkpoint</cell><cell>+13.27%</cell><cell>+6.52%</cell><cell>+1.33%</cell><cell>+1.03%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Offline Comparison of Bert-Base models</figDesc><table><row><cell>Model</cell><cell>Train Data</cell><cell cols="6">Loss Function Starting Checkpoint R@P=95% R@P=90% NDCG@5 NDCG@10</cell></row><row><cell cols="4">ùëã ùê∏ùë£1 (baseline) human-judged_1 multi-class CE Walmart pre-trained</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>ùëã ùê∏ùë£1.1</cell><cell cols="3">human-judged_2 soft-target CE Walmart pre-trained</cell><cell>+9.99%</cell><cell>+4.38%</cell><cell>+0.77%</cell><cell>+0.66%</cell></row><row><cell>ùëã ùê∏ùë£1.2</cell><cell>llm-judged_1</cell><cell>CE</cell><cell>ùëã ùê∏ùë£1.1</cell><cell>-2.28%</cell><cell>+2.01%</cell><cell>+1.16%</cell><cell>+0.89%</cell></row><row><cell>ùëã ùê∏ùë£1.3</cell><cell>llm-judged_1</cell><cell>Margin MSE</cell><cell>ùëã ùê∏ùë£1.1</cell><cell>+12.55%</cell><cell>+5.92%</cell><cell>+1.24%</cell><cell>+0.92%</cell></row><row><cell>ùëã ùê∏ùë£1.4</cell><cell>llm-judged_2</cell><cell>Margin MSE</cell><cell>ùëã ùê∏ùë£1.1</cell><cell>+13.27%</cell><cell>+5.92%</cell><cell>+1.36%</cell><cell>+1.04%</cell></row><row><cell>ùëã ùê∏ùë£2</cell><cell>llm-judged_3</cell><cell>Margin MSE</cell><cell>ùëã ùê∏ùë£1.1</cell><cell>+12.84%</cell><cell>+6.04%</cell><cell>+1.5%</cell><cell>+1.04%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>AB test results on e-commerce engagement metrics for the proposed architecture</figDesc><table><row><cell>Improvement</cell></row></table><note><p>Note: Bold values indicate statistically significant results (p&lt;0.05).</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to sincerely thank <rs type="person">Juexin Lin</rs>, <rs type="person">Nick Rossi</rs>, and <rs type="person">Tony Lee</rs> for providing the ANN data and for their valuable discussions. Our gratitude also goes to <rs type="person">Ciya Liao</rs> and <rs type="person">Matthew Mu</rs> for their useful guidance. We appreciate <rs type="person">Mossaab Bagdouri</rs> for his help with the manual evaluations and <rs type="person">Atul Singh</rs> for his work on implementing the query signal pipeline. Last but not least, we would like to extend our heartfelt thanks to <rs type="person">Yuan-Tai Fu</rs>, <rs type="person">Bhavin Madhani</rs>, and the entire engineering team. Without their support, the successful operation of the pipeline would not have been possible.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving text-based similar product recommendation for dynamic product advertising at yahoo</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 31st ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2883" to="2892" />
		</imprint>
	</monogr>
	<note>Gaurav Batra, and Ritesh Agrawal</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unbiased Learning to Rank Meets Reality: Lessons from Baidu&apos;s Large-Scale Search Dataset</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Deffayet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Michel</forename><surname>Renders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onno</forename><surname>Zoeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1546" to="1556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Metric-guided distillation: Distilling knowledge from the metric to ranker and retriever for generative commonsense reasoning</title>
		<author>
			<persName><forename type="first">Xingwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartuer</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yiu</surname></persName>
		</author>
		<author>
			<persName><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11708</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Que2engage: Embedding-based retrieval for relevant and engaging products at facebook marketplace</title>
		<author>
			<persName><forename type="first">Yunzhong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feier</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maolong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congcong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the ACM Web Conference 2023</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="386" to="390" />
		</imprint>
	</monogr>
	<note>Bin Kuang, and Arul Prakash</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the Knowledge in a Neural Network</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving efficient neural ranking models with crossarchitecture knowledge distillation</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hofst√§tter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Althammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schr√∂der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mete</forename><surname>Sertkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02666</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<title level="m">Lora: Low-rank adaptation of large language models. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Albert Q Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianna</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><surname>Saulnier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06825</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Mistral 7B. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Evaluating Retrieval Performance Using Clickthrough Data</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Kristina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toutanova</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of naacL-HLT</title>
		<meeting>naacL-HLT<address><addrLine>Minneapolis, Minnesota, 2</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enhancing Relevance of Embedding-based Retrieval at Walmart</title>
		<author>
			<persName><forename type="first">Juexin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satya</forename><surname>Praveen R Suram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prijith</forename><surname>Chembolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrushikesh</forename><surname>Chandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Mohapatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Magnani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 33rd ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="4694" to="4701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal pre-training with self-distillation for product understanding in e-commerce</title>
		<author>
			<persName><forename type="first">Shilei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Sixteenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1039" to="1047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Knowledge distillation via instance relationship graph</title>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunfeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunqiang</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7096" to="7104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Knowledge distillation based contextual relevance matching for e-commerce product search</title>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaokun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="63" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Chen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaochen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenwei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreyashi</forename><surname>Nag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.02215</idno>
		<title level="m">Exploring Query Understanding for Amazon Product Search</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic Retrieval at Walmart</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Magnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suthee</forename><surname>Chaidaroon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Reddy Suram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajit</forename><surname>Puthenputhussery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Kashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3495" to="3503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Navid</forename><surname>Mehrdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrushikesh</forename><surname>Mohapatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mossaab</forename><surname>Bagdouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prijith</forename><surname>Chandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Magnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xunfan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajit</forename><surname>Puthenputhussery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.00247</idno>
		<title level="m">Large Language Models for Relevance Judgment in Product Search</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">In defense of dual-encoders for neural ranking</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="15376" to="15400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic product search</title>
		<author>
			<persName><forename type="first">Priyanka</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijai</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vihan</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weitian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Shingavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Choon</forename><surname>Hui Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2876" to="2885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large language model based long-tail query rewriting in taobao search</title>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the ACM on Web Conference 2024</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Entity-aware Multi-task Learning for Query Understanding at Walmart</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vachik</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Mcnabb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Sharnagat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Magnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ciya</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sravanthi</forename><surname>Rajanala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends¬Æ in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingyong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaiqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.01555</idno>
		<title level="m">Instruction distillation makes large language models efficient zero-shot rankers</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">StatBM25: An Aggregative and Statistical Approach for Document Ranking</title>
		<author>
			<persName><forename type="first">Xing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanghong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">Xiangji</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM SIGIR International Conference on Theory of Information Retrieval</title>
		<meeting>the 2018 ACM SIGIR International Conference on Theory of Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="207" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large language models can accurately predict searcher preferences</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1930">2024. 1930-1940</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timoth√©e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozi√®re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improvements to BM25 and language models examined</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Trotman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Puurula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Burgess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Australasian Document Computing Symposium</title>
		<meeting>the 2014 Australasian Document Computing Symposium</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient inference with tensorrt</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Vanholder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GPU Technology Conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Knowledge Distillation for Efficient and Effective Relevance Search on E-commerce</title>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juexin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Sd Mohseni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsung</forename><surname>Taheri</surname></persName>
		</author>
		<author>
			<persName><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR eCom</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning Multi-Stage Multi-Grained Semantic Embeddings for E-Commerce Search</title>
		<author>
			<persName><forename type="first">Binbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixiong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sulong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weipeng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the ACM Web Conference 2023</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="411" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Han</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukuntha</forename><surname>Narayanan Sundararaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Gungor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Chalasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurchi</forename><surname>Subhra Hazra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.17152</idno>
		<title level="m">Improving Pinterest Search Relevance Using Large Language Models</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Xuyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajit</forename><surname>Puthenputhussery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsung</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Fang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.19548</idno>
		<title level="m">Meta Learning to Rank for Sparsely Supervised Queries</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ReprBERT: distilling BERT to an efficient representation-based relevance model for e-commerce</title>
		<author>
			<persName><forename type="first">Shaowei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keping</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4363" to="4371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pretrained transformers for text ranking: BERT and beyond</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on web search and data mining</title>
		<meeting>the 14th ACM International Conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1154" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multilingual taxonomic web page classification for contextual targeting at Yahoo</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliyar</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kapil</forename><surname>Asgarieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Thadani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujyothi</forename><surname>Perez-Sorrosal</surname></persName>
		</author>
		<author>
			<persName><surname>Adiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4372" to="4380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multilingual Taxonomic Web Page Categorization Through Ensemble Knowledge Distillation</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliyar</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kapil</forename><surname>Asgarieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Thadani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujyothi</forename><surname>Perez-Sorrosal</surname></persName>
		</author>
		<author>
			<persName><surname>Adiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanghao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.04170</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Multi-Objective Personalized Product Retrieval in Taobao Search. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
