<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Forecasting high-impact research topics via machine learning on evolving knowledge graphs</title>
				<funder>
					<orgName type="full">Alexander von Humboldt Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2025-01-07">7 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xuemei</forename><surname>Gu</surname></persName>
							<email>xuemei.gu@mpl.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for the Science of Light</orgName>
								<address>
									<addrLine>Staudtstrasse 2</addrLine>
									<postCode>91058</postCode>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mario</forename><surname>Krenn</surname></persName>
							<email>mario.krenn@mpl.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for the Science of Light</orgName>
								<address>
									<addrLine>Staudtstrasse 2</addrLine>
									<postCode>91058</postCode>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Forecasting high-impact research topics via machine learning on evolving knowledge graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-07">7 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">ED608FBF2983FD865ACA1A6182B1E593</idno>
					<idno type="arXiv">arXiv:2402.08640v3[cs.DL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-03-04T12:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The exponential growth in scientific publications poses a severe challenge for human researchers. It forces attention to more narrow sub-fields, which makes it challenging to discover new impactful research ideas and collaborations outside one's own field. While there are ways to predict a scientific paper's future citation counts, they need the research to be finished and the paper written, usually assessing impact long after the idea was conceived. Here we show how to predict the impact of onsets of ideas that have never been published by researchers. For that, we developed a large evolving knowledge graph built from more than 21 million scientific papers. It combines a semantic network created from the content of the papers and an impact network created from the historic citations of papers. Using machine learning, we can predict the dynamic of the evolving network into the future with high accuracy (AUC values beyond 0.9 for most experiments), and thereby the impact of new research directions. We envision that the ability to predict the impact of new ideas will be a crucial component of future artificial muses that can inspire new impactful and interesting scientific ideas.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>As we see an explosion in the number of scientific articles <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>, it becomes increasingly challenging for researchers to find new impactful research directions beyond their own expertise. Consequently, researchers might have to focus on narrow subdisciplines. A tool that can read and intelligently act upon scientific literature could be an enormous aid to individual scientists in choosing their next new and high-impact research project, which -on a global scale -could significantly accelerate science itself.</p><p>These days, a natural first choice for an AI-assistant would be powerful large-language-models (LLMs) such as GPT-4 <ref type="bibr" target="#b4">[5]</ref>, Gemini <ref type="bibr" target="#b5">[6]</ref>, LLaMA-2 <ref type="bibr" target="#b6">[7]</ref> or custom-made models <ref type="bibr" target="#b7">[8]</ref>. However, these models often struggle in scientific reasoning, and it remains unclear how they can suggest new scientific ideas or evaluate their impact in a reliable way in the near term.</p><p>An alternative and complementary approach is to build scientific semantic knowledge graphs. Here, the nodes represent scientific concepts and the edges are formed when two concepts are researched together in a scientific paper <ref type="bibr" target="#b1">[2]</ref>. While this approach extracts only small amounts of information from each paper, surprisingly non-trivial conclusions can be drawn if the underlying dataset of papers is large. An early example of this is a work in biochemistry <ref type="bibr" target="#b8">[9]</ref>. The authors use their semantic network, where nodes represent biomolecules, to find new potentially more efficient exploration strategies for the bio-chemistry community on a global scale. In these semantic networks, an edge between two concepts indicates that researchers have jointly investigated these research concepts. The edges are drawn from papers, thus they are created at a specific time when the paper was published. In this way, one creates an evolving semantic network that captures what researchers have investigated in the past. With such an evolving network, one can ask how the network might evolve in the future. In the scientific context, this question can be reformulated into what scientists will research in the future. For example, if two nodes do not share an edge, one can ask whether they will share an edge in the next three years -or, alternatively, whether scientists will investigate these two concepts jointly within three years. This question, denoted as a link-prediction problem in network theory <ref type="bibr" target="#b9">[10]</ref>, has been successfully demonstrated with high prediction quality for semantic networks in the field of quantum physics <ref type="bibr" target="#b10">[11]</ref> and artificial intelligence <ref type="bibr" target="#b3">[4]</ref>. These works focus on the question what scientists will work on, completely leaving out which of these topics will be impactful.</p><p>Impact in the scientific community is often approximated (for lack of better metrics <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>) by citations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, including exciting results that find interpretable mathematical models to describe citation evolution <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>. Beside concrete mathematical modelling, impact of scientific papers has also been predicted using advanced statistical and machine-learning methods that use meta-data such as including authors and affiliations <ref type="bibr" target="#b19">[20]</ref>, the content and the references of the paper <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. Techniques employed for the predictions of individual paper impact using a combination of characteristics include support-vector machines <ref type="bibr" target="#b22">[23]</ref>, regression <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>, dense <ref type="bibr" target="#b26">[27]</ref> or graph neural networks <ref type="bibr" target="#b27">[28]</ref>.</p><p>The prediction of a paper's impact however is only possible after the research is completed, and long after its underlying idea is created. A true scientific assistant or muse however should contribute at the earliest stage of the scientific cycle, when the idea for the next impactful research project is born. One solution is the prediction at the concept level. Specifically, we can ask the question Which scientific concepts, that have never been investigated jointly, will lead to the most impactful research?.</p><p>In this work, we answer this question by combining semantic networks and citation networks that are purely The edges are augmented with citation information, which acts as a proxy for impact in our work. A mini-knowledge graph (blue edges) is constructed from four randomly selected papers (p1-p4) <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref> from OpenAlex as an example. Here, cp4 represents the total citations of paper p4 since its publication, and cp4(y) is its annual citations from 2018 to 2022 (e.g., cp4(2018) = 4). The citation value of the edge is the sum of the all papers creating the edge.</p><p>based on the level of scientific concepts 1 . Specifically, we develop a large evolving knowledge graph using more than 21 million scientific papers, from 1709 (starting with a letter by Antoni van Leeuwenhoek <ref type="bibr" target="#b37">[38]</ref>) to April 2023. The vertices of the knowledge graph are scientific concepts and the edges between two concepts contain information about when these topics have been investigated and how often they have been cited subsequently. We then train a machine learning model on the historic evolution of the knowledge graph. We find that the neural network can predict with high accuracy which concept pairs, that have never been jointly investigated before in any scientific paper, will be highly cited in the future.</p><p>Being able to predict the potential impact of new research ideas -before the paper is written or the research is done or even started -could be a cornerstone in future scientific AI-assistants that help humans broadening their horizon of possible new research endeavours <ref type="bibr" target="#b38">[39]</ref>.</p><p>1 GitHub: Impact4Cast</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RESULTS</head><p>Creating a list of scientific concepts -At the heart of our knowledge graph are scientific concepts, as depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. We chose not to rely on existing concept lists, such as the APS or computer science ontology <ref type="bibr" target="#b39">[40]</ref>, for several reasons. Firstly, our goal is to ultimately cover all natural sciences comprehensively, and a universal list encompassing this breadth doesn't currently exist. Secondly, we want to capture the most recent concepts that might be absent from existing lists. Lastly, generating our list ensures that we have a granular understanding and control over the concepts.</p><p>To build our concept list, we started with 2,444,442 papers from four publicly available preprint servers: arXiv, bioRxiv, medRxiv, and chemRxiv. We use papers from preprint servers for two reasons: (1) It contains papers that are not published yet in journals, thus our dataset also contains state-of-the-art concepts; (2) they associate . We find many revolutionary topics in the realm of quantum physics and optics research in the last decade, including Perovskite devices <ref type="bibr" target="#b32">[33]</ref>, the emergence of complex and nonhermitian topology <ref type="bibr" target="#b33">[34]</ref>, the introduction of advanced concepts of machine learning in physics <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref> and quasi-BIC (bound state in continuum) resonances.</p><p>papers to research categories, which can be used to focus on specific scientific domains (as we do, with the field of quantum physics and optics). The data cutoff is February 2023. From these, we extracted titles and abstracts of the papers. To single out concept candidates from this extensive collection, we applied the Rapid Automatic Keyword Extraction (RAKE) algorithm based on statistical text analysis to automatically detect important keywords <ref type="bibr" target="#b40">[41]</ref>. Concepts with two words, like phase transition, were retained if they appear in at least 9 papers, while longer concepts, such as single molecule localization microscopy, needed to appear in at least 6 papers. In this way, we can increase the fraction of high-quality concepts. We further developed a suite of natural language processing tools to refine the concepts, followed by manual inspection to remove any incorrectly identified ones. Finally, we got a list which contains over 368,000 concepts. We focus here on concepts specific to the sub-field of optics and quantum physics (representing roughly 10% of the entire concepts), but our method can immediately be translated to any other domain. This refined domain-specific concept list serves as the vertices of our knowledge graph.</p><p>Creating an evolving, citation-augmented knowledge graph -Now that we have the vertices, we can create edges that contain information from the scientific literature. We get the citation information from papers in OpenAlex <ref type="bibr" target="#b41">[42]</ref>, an open-source database containing detailed information on more than 92 million publications. Edges are drawn when two concepts co-occur in the title or abstract of a scientific paper. If a paper connects two vertices, the weight of the newly formed edge is the paper's annual citation numbers from 2012 to 2023 together with the total citation number since its publication. If more than one paper creates an edge, then the edge contains the sum of the annual citations (as well as the sum of the total citations) gained by all papers. As research papers appear over time, and their citations are created in time, we effectively build an evolving, citation-augmented knowledge graph that evolves in time (see Fig. <ref type="figure" target="#fig_0">1</ref>). From these 92 million papers, 21 million contain at least two concepts of our concept list and can therefore for an edge in the knowledge graph.</p><p>The final constructed knowledge graph has 37,960 vertices with more than 26 million edges (built from 190 million concept pairs, containing multi-edges when multiple papers create the same edge) from the OpenAlex dataset, with a data cutoff at April 2023. In Fig. <ref type="figure" target="#fig_1">2</ref>, we show the fastest growing (in terms of citation) concepts and concept pairs since 2012, where we can recognize many highly influential topics in quantum physics and optics research. the past, we can formulate the prediction of impact for new concept pairs as a supervised learning task, as illustrated in Fig. <ref type="figure">3</ref>. For a vertex pair that has not had any connection in the year 2016, we predict whether three years later this pair accumulated more than a certain number of citations. Using the historical knowledge graph, we possess an ideal supervision signal for our binary classification task. During the training phase, we selected pairs of vertices that were not connected and calculated 141 features for each pair. These features include 41 network features, divided into 20 node features (such as the number of neighbors and PageRank <ref type="bibr" target="#b42">[43]</ref> over the past three years) and 21 edge features (including cosine, geometric, and Simpson similarities <ref type="bibr" target="#b43">[44]</ref>). Additionally, we incorporated 100 impact features: 58 of these are node citation features, covering total citations and yearly citations within the last three years. The other 42 features are about vertex pairs and include measures such as the citation ratio between them. Detailed feature description are available in GitHub: Impact4Cast. The network features are inspired by the winner of the Science4Cast competition <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b44">45]</ref>, and the citation features are developed empirically and could potentially be improved by careful feature importance analysis. Our neural network is a fully connected feed-forward network with four hidden layers of 600 neurons each. The exploration of more advanced architectures might improve the prediction qualities further. The neural network has to predict whether the unconnected vertex pair in 2019 will have at least IR citations (IR stands for the impact range). The impact range (IR) is a threshold representing the minimum number of citations a concept pair must accumulate within a specified time frame (e.g., three years) to be classified as "high-impact". For instance, an IR = 100 means that only concept pairs with at least 100 citations during the defined period are considered impactful. This binary threshold simplifies the problem into a classification task, making it computationally tractable while providing a clear measure of success. Predicting individual citation counts is inherently noisy due to the stochastic nature of citation dynamics. By using IR, we focus on identifying high-impact trends, avoiding fluctuations of precise citation counts. IR provides a clear and measurable target for classification. This allows us to use metrics like the Area Under the Curve (AUC) of Receiver Operating Characteristics (ROC) curves to evaluate the prediction quality <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Forecasting impact of newly created concept connections -With an evolving knowledge graph from</head><p>We perform the training for different values of the impact range IR from IR = 1 to IR = 200, and then quantify the quality with AUC of the ROC curves <ref type="bibr" target="#b45">[46]</ref>. The AUC gives a measure of classification quality and stands for the probability that a randomly chosen true example is ranked higher than a randomly chosen false example. A random classifier has AU C = 0.5. We measure the AUC for a test set (which contains unconnected pairs not in the training set) for a prediction from 2016 to 2019, and for an evaluation dataset, with 10 million random data from 2019 to 2022 (while keeping the training data of the neural network from 2016 to 2019). The evaluation dataset shows how well the neural network performs on future, never-seen datasets. This is motivated by our goal that ultimately we want to train a neural network with all available data (let's say, until January 2023) and predict what happens until the future in 2026. In Fig. <ref type="figure" target="#fig_2">4(a)</ref>, we find that the AUC scores for both the test set and the evaluation set are beyond 0.8, in most of the cases beyond 0.9, for different IR. We can conclude that the neural network can forecast a high impact of previously never-investigated concept connections to a high degree. In Fig. <ref type="figure" target="#fig_2">4</ref>(b), we sort the concept pairs of the evaluation dataset with the neural network (IR = 100), and plot their true citation counts. We further divide the 10 million evaluation dataset into 20 equal parts and plot their average citation count (represented by green bars) for each 5% segment. This clearly demonstrates good predictions at the individual concept pair level. As seen in Fig. <ref type="figure" target="#fig_2">4(c</ref>), the highest predicted concept pairs indeed get more than 3 orders of magnitude more citations than the average citation of all 10 million pairs.</p><p>Forecasting genuine impact beyond link prediction -Next, we perform an even more challenging, genuine impact prediction task that goes beyond link prediction (i.e., predicting which concept pairs will be investigated in the future by a scientific paper). Concretely, in this task training data is conditioned on unconnected vertex pairs in 2016 which are actually connected in 2019. We quantify the quality using the AUC of the ROC curve. For example, IR = 100, i.e, (&lt; 100, &gt;= 100), refers to whether the 3-year citation counts after 2016 (test) or after 2019 (eval) is at least 100. TPR (true positive rate) measures how often a test correctly identifies a true positive, while FPR (false positive rate) measures how often it correctly identifies a true negative. (b): Sorted predictions of the neural network on the evaluation set (blue curve in (a)) shows the very high quality prediction at the level of individual concept pairs. The y-axis stands for the respective fraction of the evaluation dataset (10 7 data points). The histogram is separated into 20 equal bins. No fitting is involved. In (c), we show the average citation of the first N highest predicted concept pairs. This plot shows impressively that the highest predicted concept pairs indeed have very high citation, more than 3 orders of magnitude higher than the average citation of all 10 7 pairs (0.029 citations). (d): This more challenging step shows that citation prediction goes beyond link predictions. Here we take unconnected vertex pairs, conditioned on a connection 3 years later. The neural network is tasked to classify these concept pairs in low or high citations, revealing that it is not just predicting new links, but is learning intrinsic citation features. Here IR = [5, 100], i.e, (0 -5, &gt;= 100), means whether the 3-year citation count after 2016 (test) or after 2019 (eval) is at most 5 or at least 100.</p><p>The neural network only gets citation information from 2016 and has to predict whether the newly generated concept pair will be highly impactful or not in the future. For that, our classification task asks whether the newly generated edge will receive citations within 0-5 or above 100 (Fig. <ref type="figure" target="#fig_2">4(d)</ref>) in 2019. We see that the AUC score is beyond 0.7 (for the test set) and beyond 0.67 for the evaluation set, clearly indicating that the neural network can predict impact properties that go beyond the simple link-prediction task.</p><p>Highly predicted impact pair and potential applications -We can now investigate the largest predicted pairs of concepts, by taking all unconnected vertex pairs (∼694 million pairs) until January 2023, and let the neural network (trained with all unconnected pairs in 2019 with supervision signal in 2022) sort them by impact predictions. We find that the highest predicted pair is renewable energy and cancer cell. This prediction is a very high-risk bet. For more practical, personalized suggestions, one can restrict the unconnected concept pairs to those related to specific scientists or research groups, aiming for high-impact collaboration suggestions. By examining the published works of scientists to identify their research interests, it becomes possible to identify concept pairs where one aligns with one scientist's specialty and the other with another scientist's. Thereby, one can suggest potential collaborations of high impact. As an example, by constraining the personalized research interests of scientists in experimental quantum optics and one researcher in biophysics, the highest predicted impact concepts pairs are 'microfluidic channel ' with 'Kerr resonator ', 'SARS CoV' with 'quantum enhanced sensitivity' or 'electron microscopy' with 'quantum vacuum field '. These suggestions can be further refined based on their similarity (e.g., represented by the cosine similarity) or the prominence of the concepts (indicated by the node degree), as we show in Fig. <ref type="figure" target="#fig_3">5</ref>. Here, we plot 100,000 concept pairs that have not been studied together until January 2023 and use the neural network trained on 2019 dataset to predict their impact. The points are plotted based on various properties, such as the similarity between concepts, their prominence within the network, their growth rate in the network (reflected in newly acquired neighbors), and how often the concepts have been cited previously. Plotting in this way allows us to identify rare outliers -concept pairs with high predicted impact that have unique properties, such as the bright yellow spots highlighted in the insets of Fig. <ref type="figure" target="#fig_3">5</ref>. These methods help us narrow down the enormously large number of possibilities into a small number of personalized and targeted suggestions, which could inspire new ideas. In practical application, it will be useful to update the knowledge graph regularly, and train the machine learning models on the latest knowledge graph  data, so it can better incorporate the latest trends and discoveries for its predictions. Training from 2014 -&gt; 2017 FIG. <ref type="figure">7</ref>. Predictions for varying intervals without retraining. The four ML models were trained using data from 2014 to 2017 and then used to predict outcomes 1 to 5 years into the future without retraining. For one task, the number of positive cases was insufficient for meaningful classification (we set a threshold of at least 10 positive cases out of 1 million total cases). FIG. <ref type="figure">8</ref>. Prediction of higher IR. The models are tasked with predicting whether concept pairs will receive at least 50 citations. Training is performed on data from 2014 to 2017, with evaluation conducted over intervals of 2, 3, 4, or 5 years (a 1-year interval lacks sufficient data). The blue line represents the performance of a fully connected neural network trained specifically for this task. In contrast, the red line represents a fully connected neural network (with identical architecture and training parameters) trained to solve the task for IR = 10 instead. Interestingly, the red model achieves slightly higher AUC values, indicating that predictions for higher impact ranges (IR = 50) benefit from training on more diverse data, including those from lower impact ranges.</p><p>Benchmarking different models and different time intervals -So far, we have only focused on a specific case: a training interval from 2016 to 2019 (3 years) and an evaluation interval from 2019 to 2022 (3 years). Additionally, we have primarily investigated the performance of feed-forward neural networks. Exploring other models and examining their predictive capabilities across various training and evaluation intervals could provide deeper insights into model performance. To do so, we expanded our study to include benchmarking on a small dataset of 1 million pairs. This benchmarking incorporates the previously used fully connected neural network alongside additional models, including a transformer architecture <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>, random forest <ref type="bibr" target="#b48">[49]</ref>, and XGBoost <ref type="bibr" target="#b49">[50]</ref>.</p><p>The feed-forward neural network, implemented using PyTorch <ref type="bibr" target="#b50">[51]</ref>, consisted of three hidden layers, each with 600 neurons and ReLU activations <ref type="bibr" target="#b51">[52]</ref>, resulting in ap-proximately 800,000 trainable parameters. Similarly, the transformer architecture <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> also implemented via PyTorch, was designed with 4 layers, a hidden size of 128, 4 attention heads, and a feedforward dimension of 512, resulting in approximately 800,000 trainable parameters. Positional encodings were added to the input features. Both neural network models were trained using Adam optimizer <ref type="bibr" target="#b52">[53]</ref> with a batch size of 2048 and a learning rate of 0.0001. The random forest classifier, implemented with scikit-learn <ref type="bibr" target="#b53">[54]</ref>, was trained with 300 trees, a minimum of 25 samples required to split a node, and 10 samples per leaf. The XGBoost model was trained using up to 2000 boosting rounds, a learning rate of 0.01, and a maximum tree depth of 10. Hyperparameters for all models were selected via a hyperparameter search for a single benchmark task (training: 2016-2019, evaluation: 2019-2022, with IR = 10) and kept constant for all tasks.</p><p>In all tasks, the models are provided with 141 input features of a specific concept pair and have to predict whether this pair will receive more or fewer IR citations (IR = 10 or IR = 50) in certain future years. To achieve this, the four models are trained on 2-, 3-, and 4-year intervals and evaluated on intervals ranging from 1 to 5 years. For example, if the training interval spans 2 years and the evaluation interval spans 5 years, the models are trained using data from 2015 to 2017 to predict whether unconnected concept pairs will receive IR citations. They are then evaluated using 2017 data to make predictions for 2022. After training, the models are evaluated on 1 million concept pairs, predicting the likelihood of each pair receiving more than IR citations. These predictions are ranked (from high to low likelihood) and compared against the ground truth to compute the ROC curve and the AUC score, which measures prediction quality. As shown in Fig. <ref type="figure">6</ref>, the models achieve AUC values exceeding 90% in these tasks. In a slightly modified task, we train the models on the data from 2014 to 2017 and evaluate them from 2017 to 1-5 years into the future (2018 to 2022). This test analyzes how well the prediction perform for intervals on which the models have not been trained and how difficult it is to predict further into the future. As shown in Fig. <ref type="figure">7</ref>, the quality of the predictions indeed decreases for larger intervals.</p><p>In Fig. <ref type="figure">8</ref>, we show that models trained on smaller impact ranges can predict higher impact ranges even slightly better than those trained exclusively on higher impact ranges. This might be due to more diverse training data (there are many more examples of IR = 10 than of IR = 50, because many more concept pairs achieve at least 10 citations rather than 50). It might also be explained by a systematic drift in citation patterns between the years the models were trained and the years they are applied, potentially due to the growing number of overall citations. It will be very interesting in the future to explore and understand this effect further and to develop new ML methods that could leverage this dynamic.</p><p>All of our models use the same set of input features and do not directly access the full knowledge graph. Developing techniques that can leverage more general graph properties -such as automatically learning features or generating embeddings -would be an interesting avenue to explore. A related approach was demonstrated in a previous competition <ref type="bibr" target="#b3">[4]</ref>, where the task was to predict the future state of a knowledge graph. There, the graph's edges represented co-occurrences of concepts in scientific papers. In contrast, the knowledge graph used in our study is more complex, with edges also weighted by the number of citations received by concept pairs. Exploring more end-to-end approaches that integrate more information from the entire knowledge graph could reveal whether such methods can outperform the models and hand-crafted features demonstrated in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DISCUSSION</head><p>We show how to forecast the impact of future research topics. Although we view this as a significant step towards developing truly useful AI-driven assistants, achieving this goal requires numerous further advancements. Firstly, developing methods to extract more complex information from each paper will be crucial, for instance by employing hyper-graph structures that carry more information from each paper <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref>, which has already been demonstrated to lead to exciting results in other domains <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref>. This might also allow for the forecast of new concepts <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref> and their impact. Incorporating the recent dataset <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62]</ref> into our research could also allow us to explore more complex data structures than those used in our paper. Secondly, it will be interesting to approximate impact with metrics that go beyond citations -which is a crucial topic in computational sociology and the study of the science of science <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Additionally, introducing metrics of surprise, as discussed in <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64]</ref>, could serve as a complementary metric to citation prediction for ranking suggestions. Finally, while the suggestion of impactful new ideas might be a key component of future AI assistants, it will be crucial to study its relation to the scientific interest of working researchers <ref type="bibr" target="#b64">[65]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MATERIALS AND METHODS</head><p>Datasets for knowledge graph construction -To compile a list of scientific concepts in natural science, we used metadata from four major preprint servers: arXiv, bioRxiv, medRxiv, and chemRxiv. The arXiv dataset can be directly downloaded from Kaggle, while metadata from bioRxiv, medRxiv, and chemRxiv are accessible through their APIs. The full methodology and codes are available on the GitHub: Impact4Cast. Our comprehensive dataset encompasses approximately 2.44 million papers, including 78,084 from arXiv's physics.optics and quant-ph categories, which were specifically utilized for identifying domain concepts.</p><p>For edge generation, we used the OpenAlex database snapshot, available for download in OpenAlex bucket. More details can be found at the OpenAlex documentation site. The complete dataset occupies around 330 GB, expanding to approximately 1.6 TB when decompressed. Our interest was specifically in scientific journal papers that include publication time, title, abstract, and citation information. By focusing on these criteria, we managed to reduce the dataset to a more manageable gzip-compressed size of 68 GB, comprising around 92 million scientific papers. From these 92 million papers, 21 million contain at least two concepts of our final concept list and can therefore for an edge in the knowledge graph.</p><p>Details on concept and edge generation -From the preprint dataset of ∼2.44 million papers, we analyzed each article's title and abstract using the RAKE algo-rithm, enhanced with additional stopwords, to extract potential concept candidates. These candidates were stored for subsequent analysis. We filtered out concepts to retain only those with two words that appeared in nine or more articles, and those with three or more words that appeared in six or more articles. This step significantly reduced the noise from the RAKE-generated concepts, yielding a refined list of 726,439 relevant concepts. To further enhance the quality of the identified concepts, we developed a suite of automatic tools designed to identify and eliminate common, domain-independent errors often associated with RAKE. In addition, we conduct a manual review to identify and eliminate any inaccuracies in the concepts. The entire process, which included eliminating non-conceptual phrases, verbs, ordinal numbers, conjunctions, and adverbials, resulted in a full list of 368,825 concepts.</p><p>We then specifically focused on articles within the physics.optics and quant-ph categories from arXiv to extract domain-specific concepts. Iterating this entire list of concepts to these domain-specific articles, we identified 87,741 relevant concepts. Employing our specially designed automated filtering tool for initial refinement and then conducting a thorough manual review to remove inaccuracies, we narrowed the list down to 37,960 high-quality, domain-specific concepts.</p><p>As an example, we show the extraction of concepts for the four papers used in Fig. <ref type="figure" target="#fig_0">1</ref>  We created concept pairs, or edges, from the Ope-nAlex dataset, by detecting when domain-specific concepts co-occurred in paper titles or abstracts. This yielded 193,977,096 concept pairs (including multi-edges) across about 21 million papers. Each edge receives a time-stamp based on its paper's publication date, converted to the number of days since January 1, 1990. The final full knowledge graph comprises 26,010,946 unique edges after merging multiple edges between the same concept pairs. The citation information for an edge includes the paper's yearly citations from 2012 to 2023, alongside its total citation since publication. The OpenAlex dataset excludes yearly citations older than ten years, hence the focus on this specific ten-year time frame due to the absence of data prior to 2012. For edges formed by multiple papers, the edge weight combines the annual and total citations from all contributing papers.</p><p>Consider the edge formed by the concepts 'single molecule localization microscopy' and 'neural net', generated from paper p1 <ref type="bibr" target="#b28">[29]</ref> published on January 7, 2020. FIG. <ref type="figure" target="#fig_0">11</ref>. ROC curve explanation. The ROC curve are plotted for the evaluation data in Fig. <ref type="figure" target="#fig_2">4</ref>, which illustrates the performance of binary classifiers discussed in our paper. On the left, for IR = 10, the area under the curve (AUC) is 0.94. The x-axis represents the False Positive Rate (FPR), while the y-axis represents the True Positive Rate (TPR). The curve demonstrates how the classifier's performance changes with variations in the classification threshold, which determines whether a case is classified as Class 0 or Class 1. For instance, at a threshold that gives a classifier with FPR=0.1, the TPR is 0.83, and at FPR=0.3, the TPR is 0.96. Therefore, the ROC curve is more informative than just a single pair of FPR and TPR. On the right, the ROC curves for IR=10, 50, and 100 are shown.</p><p>The time-stamp for this edge is derived from the days elapsed since January 1, 1990. The citation metrics for this edge includes the total and yearly citations from each contributing paper. Paper p1 with 38 citations (c p1 =38), contributes yearly citations represented as c p1 (y i ) for i=2023,2022,..., 2012, with actual values {5, 8, 16, 9} for 2023 to 2020, and zeros for previous years, culminating in a citation sequence {5, 8, 16, 9, 0, 0, 0, 0, 0, 0, 0, 0}. Similarly, paper p2 <ref type="bibr" target="#b29">[30]</ref>, published on March 20, 2020, with 43 citations (c p2 =43), adds its yearly citations {6, 16, 14, 7} for the same period (2023 to 2020). The aggregated citation data for this edge, combining c p1 and c p2 , yields a total of 81 citations, with an annual citation sequence of {11, 24, 30, 16, 0, 0, 0, 0, 0, 0, 0, 0}.</p><p>Training -Our neural network consists of six fully connected layers, which include four hidden layers with 600 neurons each. The network inputs are 141 features for each unconnected concept pair (v i , v j ), denoted as p i,j = (p 1 i,j , p 2 i,j , ..., p 141 i,j ), where each p i,j ∈ R. For instance, p 1 i,j and p 2 i,j represent the vertex degree of concepts v i and v j for the current year, y. Detailed feature description and feature generation code are available in GitHub: Impact4Cast.</p><p>In our training process for the year 2016 to predict impact in 2019, we prepared a dataset comprising approximately 689 million unconnected concept pairs. The goal was to evaluate these pairs to determine whether their 3-year citation counts would have at least IR citations (IR is impact range) or not. From this extensive collection, we selected all positive samples (the 3-year citation counts are at least IR). An equivalent number of negative samples were then randomly chosen to match across the years y, y-1, and y-2, and the cosine similarity coefficient for unconnected pairs (u, v) in year y (i.e., y=2016), with AUC scores of 0.8880, 0.8795, 0.8720, and 0.8683, respectively. In contrast, the lowest predictive three features are the average total citation count up to year y for vertex v, and the total citation count for the pair (u, v) up to years y -1 and y -2, with AUC scores of 0.5219, 0.5234, and 0.5285. Using all 141 features together leads to a significant improvement in the AUC score to 0.948, showing that the combination of all features works better.</p><p>the size of the positive set. The refined dataset was subsequently divided, allocating 85% for training and 15% for testing purposes. For the evaluation dataset in 2019, which aims to predict the impact in 2022, we randomly selected 10 million unconnected pairs. Our neural network was trained using the Adam optimizer with a learning rate of 3 × 10 -5 and a mini-batch size of 1000. In every training batch, we randomly chose an equal number of positive and negative samples from the training set. This approach was also applied to our 2019 training process for predictions into 2022, where the trained neural network is used for future forecasting. An example loss curve is shown in Fig. <ref type="figure" target="#fig_6">9</ref>. An example for the number of positive and negative cases of the evaluation dataset (for the experiments in Fig. <ref type="figure" target="#fig_2">4</ref>) is shown in Fig. <ref type="figure" target="#fig_0">10</ref>. In Fig. <ref type="figure" target="#fig_0">11</ref> we explain more details of the ROC curve, specifically its relation to false positive rate (FPR) and true positive rate (TPR).</p><p>The full dynamic knowledge graph, along with the data required for feature preparation and evaluation, was processed on an Intel Xeon Gold 6130 CPU with 1 TiB of RAM. However, it is not strictly necessary to have 1 TiB of RAM for this process with the relatively small concept list of 37,960; the high memory capacity was utilized for efficiency and to handle additional operations concurrently. The final domain knowledge graph in this work occupies approximately 23.12 GB of storage. It is worth noting that knowledge graphs built from larger concept lists will require more memory, as the data size and complexity increase.</p><p>The neural network training was conducted on a standard single GPU (Nvidia Quadro RTX 6000), with each training run for different impact ranges taking approximately 1.5 hours. For benchmarking, all models -except the transformer model -were run on a standard CPU (Intel Xeon Gold 6130) with memory usage below 15 GB. Each benchmarking task took roughly one hour to complete. The transformer model, however, was run on a single GPU (Nvidia Quadro RTX 6000), taking approximately 3 hours per task.</p><p>Individual feature's predictive ability -In Fig. <ref type="figure" target="#fig_2">4</ref> (a), we observe an AUC score of 0.948 for the 2019 evaluation dataset with the neural network that uses all 141 features, trained on 2016 dataset and impact range IR = 100. To explore the predictive ability of individual features, we trained separate neural networks on each feature using the same 2016 dataset, and then applied the 2019 evaluation dataset to these models. This resulted in 141 individual predictions, each from a network trained on a single feature. The features were ranked by their impact predictions, shown in the Fig. <ref type="figure" target="#fig_7">12</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIG. 1 .</head><label>1</label><figDesc>FIG.1. Generation of the knowledge graph with time and citation information. Vertices are formed by scientific concepts, which are extracted from scientific papers (titles and abstracts) from prominent academic preprint servers. Edges are formed when concepts are investigated jointly in a scientific publication. There are 21,165,421 out of 92,764,635 papers from OpenAlex which form at least one edge. The edges are augmented with citation information, which acts as a proxy for impact in our work. A mini-knowledge graph (blue edges) is constructed from four randomly selected papers (p1-p4)<ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref> from OpenAlex as an example. Here, cp4 represents the total citations of paper p4 since its publication, and cp4(y) is its annual citations from 2018 to 2022 (e.g., cp4(2018) = 4). The citation value of the edge is the sum of the all papers creating the edge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIG. 2 .</head><label>2</label><figDesc>FIG.2. Fastest growing citations of concepts and concept pairs: Evolution of citations over three years for the topfastest growing, previously uncited concepts (a) and concept pairs (b). We find many revolutionary topics in the realm of quantum physics and optics research in the last decade, including Perovskite devices<ref type="bibr" target="#b32">[33]</ref>, the emergence of complex and nonhermitian topology<ref type="bibr" target="#b33">[34]</ref>, the introduction of advanced concepts of machine learning in physics<ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref> and quasi-BIC (bound state in continuum) resonances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIG. 4 .</head><label>4</label><figDesc>FIG. 4. Evaluating the machine-learning-based impact forecast. (a): Classification of unconnected pairs, whether they will exceed a certain threshold three years later. Training data contains unconnected vertex pairs from 2016 and the supervision signal according to their impact range IR 3 years later. The test dataset also includes pairs from 2016, but excludes those in the training set. A more challenging evaluation set contains unconnected pairs from 2019, with outcomes verified in 2022, importantly noting that the neural network was only trained on data from 2016 to 2019, not 2019 to 2022.We quantify the quality using the AUC of the ROC curve. For example, IR = 100, i.e, (&lt; 100, &gt;= 100), refers to whether the 3-year citation counts after 2016 (test) or after 2019 (eval) is at least 100. TPR (true positive rate) measures how often a test correctly identifies a true positive, while FPR (false positive rate) measures how often it correctly identifies a true negative. (b): Sorted predictions of the neural network on the evaluation set (blue curve in (a)) shows the very high quality prediction at the level of individual concept pairs. The y-axis stands for the respective fraction of the evaluation dataset (10 7 data points). The histogram is separated into 20 equal bins. No fitting is involved. In (c), we show the average citation of the first N highest predicted concept pairs. This plot shows impressively that the highest predicted concept pairs indeed have very high citation, more than 3 orders of magnitude higher than the average citation of all 10 7 pairs (0.029 citations). (d): This more challenging step shows that citation prediction goes beyond link predictions. Here we take unconnected vertex pairs, conditioned on a connection 3 years later. The neural network is tasked to classify these concept pairs in low or high citations, revealing that it is not just predicting new links, but is learning intrinsic citation features. Here IR = [5, 100], i.e, (0 -5, &gt;= 100), means whether the 3-year citation count after 2016 (test) or after 2019 (eval) is at most 5 or at least 100.</figDesc><graphic coords="5,125.41,222.53,159.81,154.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIG. 5 .</head><label>5</label><figDesc>FIG. 5. Network features vs. predicted impact. A randomly selected set of 100,000 unconnected pairs until January 2023 is used. The color represents the neural network's prediction of each concept pair's impact. (a): The y-axis shows cosine similarity, indicating the semantic similarity between concepts; lower values represent concept pairs that are semantically distinct. The x-axis is the average vertex degree of the two concepts in the knowledge graph, reflecting their overall prominence. Concepts with low similarity and low degree yet predicted to have high impact could be surprising and offer interesting suggestions. (b): The x-axis represents the average number of new neighbors each concept gained over the last three years. Concept pairs with low similarity and few new neighbors but high impact predictions might highlight potentially overlooked but intriguing ideas. (c): The x-axis denotes citation density (average citations per paper mentioning the concepts). Pairs with low similarity and citation density but high predicted impact could again indicate overlooked potential ideas. (d): Citation counts for concept 1 (x-axis) and concept 2 (y-axis) over last three years are plotted on a logarithmic scale. We can easily identify concept pairs predicted to have high impact in the future, even though they have individually received few citations in the past.</figDesc><graphic coords="6,79.41,52.07,457.29,317.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5 (</head><label>5</label><figDesc>FIG.6. Benchmarking fully connected NNs, Transformers, Random Forest, and XGBoost on 27 variations of the prediction task, with 2-4 year training and 1-5 year evaluation intervals, across two different impact ranges (IR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIG. 9 .</head><label>9</label><figDesc>FIG.9. Loss curves for one typical example. The training and test loss curves correspond to a fully connected neural network trained on data from 2017 to 2020, with an impact range (IR) of 10.</figDesc><graphic coords="10,317.01,52.07,245.09,147.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FIG. 12 .</head><label>12</label><figDesc>FIG. 12. Neural network performance across individual features.The highest-performing four features are the Simpson similarity coefficient for the unconnected pair (u, v) across the years y, y-1, and y-2, and the cosine similarity coefficient for unconnected pairs (u, v) in year y (i.e., y=2016), with AUC scores of 0.8880, 0.8795, 0.8720, and 0.8683, respectively. In contrast, the lowest predictive three features are the average total citation count up to year y for vertex v, and the total citation count for the pair (u, v) up to years y -1 and y -2, with AUC scores of 0.5219, 0.5234, and 0.5285. Using all 141 features together leads to a significant improvement in the AUC score to 0.948, showing that the combination of all features works better.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>The authors thank <rs type="person">Burak Gurlek</rs> for interesting discussions at the start of this project, and the organizers of OpenAlex, arXiv, bioRxiv, chemRxiv, and medRxiv for making scientific resources freely and readily accessible. X.G acknowledges the support from the <rs type="funder">Alexander von Humboldt Foundation</rs>. Author Contributions X.G. and M.K. designed research; X.G. performed research and analyzed data; and X.G. and M.K. wrote the manuscript.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Competing Interests The authors declare that they have no competing financial interests. Data and Code availability: Data is accessible on Zenodo at https://doi.org/10.5281/zenodo.10692137 <ref type="bibr" target="#b65">[66]</ref>. Benchmark data used in our work is also available on Zenodo at https://doi.org/10.5281/zenodo.14527306 <ref type="bibr" target="#b66">[67]</ref>. Codes for this work are available on GitHub at https://github.com/artificial-scientist-lab/Impact4Cast.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Bergstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Börner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Milojević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Radicchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sinatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Uzzi</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aao0185</idno>
	</analytic>
	<monogr>
		<title level="j">Science of science</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="page">185</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Science</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
		<title level="m">The science of science</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bornmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haunschild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mutz</surname></persName>
		</author>
		<idno type="DOI">10.1057/s41599-021-00903-w</idno>
	</analytic>
	<monogr>
		<title level="j">Humanities and Social Sciences Communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Krenn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Buffoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eppel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gritsevskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Moutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sanjabi</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-023-00735-0</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">1326</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">OpenAI, Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Gemini: a family of highly capable multimodal models</title>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11805</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
	</analytic>
	<monogr>
		<title level="m">Open foundation and finetuned chat models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning to generate novel scientific directions with contextualized literature-based discovery</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hope</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14259</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Choosing experiments to accelerate collective discovery</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rzhetsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.150975711</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page">14569</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A survey of link prediction in complex networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Berzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Cubero</surname></persName>
		</author>
		<idno type="DOI">10.1145/3012704</idno>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predicting research trends with semantic and neural networks with an application in quantum physics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Krenn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeilinger</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1914370116</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page">1910</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Challenge the impact factor</title>
		<idno type="DOI">10.1038/s41551-017-0103</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">103</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The many facets of impact</title>
		<idno type="DOI">10.1038/s42254-024-00696-2</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Physics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">71</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The formula: The universal laws of success</title>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Hachette UK</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The evolution of citation graphs in artificial intelligence research</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cebrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rahwan</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-019-0024-5</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">79</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quantifying longterm scientific impact</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1237825</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">342</biblScope>
			<biblScope unit="page">127</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Defining and identifying sleeping beauties in science</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ferrara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Radicchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Flammini</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1424329112</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page">7426</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Quantifying the evolution of individual scientific impact</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sinatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Deville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aaf5239</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">354</biblScope>
			<biblScope unit="page">5239</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large teams develop and small teams disrupt science and technology</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-019-0941-9</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">566</biblScope>
			<biblScope unit="page">378</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning on knowledge graph dynamics provides an early warning of impactful research</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Weis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jacobson</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41587-021-00907-6</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Biotechnology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">1300</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An overview on evaluating and predicting scholarly article impact</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.3390/info8030073</idno>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">73</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A review of scientific impact prediction: tasks, features and methods</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11192-022-04547-8</idno>
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page">543</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using content-based and bibliometric features for machine learning models to predict citation counts in the biomedical literature</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aliferis</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11192-010-0160-5</idno>
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">257</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Citation impact prediction for scientific papers using stepwise regression analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11192-014-1279-6</idno>
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page">1233</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Predicting the long-term citation impact of recent publications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stegehuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Litvak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Waltman</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.joi.2015.06.005</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of informetrics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">642</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to predict citationbased impact measures</title>
		<author>
			<persName><forename type="first">L</forename><surname>Weihs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 ACM/IEEE joint conference on digital libraries (JCDL</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Predicting the citation counts of individual papers via a bp neural network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.joi.2020.101039</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Informetrics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">101039</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.01572</idno>
		<title level="m">H2cgl: Modeling dynamics of citation network for impact prediction</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Accurate and rapid background estimation in singlemolecule localization microscopy using the deep neural network bgnet</title>
		<author>
			<persName><forename type="first">L</forename><surname>Möckl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Moerner</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1916219117</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Machine learning for cluster analysis of localization microscopy data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Burn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Griffié</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Owen</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-020-15293-x</idno>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">1493</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Constraints on cosmic strings using data from the first advanced ligo observing run</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Abbott</surname></persName>
			<affiliation>
				<orgName type="collaboration">LIGO Scientific Collaboration ; Virgo Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Abbott</surname></persName>
			<affiliation>
				<orgName type="collaboration">LIGO Scientific Collaboration ; Virgo Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Abbott</surname></persName>
			<affiliation>
				<orgName type="collaboration">LIGO Scientific Collaboration ; Virgo Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Acernese</surname></persName>
			<affiliation>
				<orgName type="collaboration">LIGO Scientific Collaboration ; Virgo Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ackley</surname></persName>
			<affiliation>
				<orgName type="collaboration">LIGO Scientific Collaboration ; Virgo Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Adams</surname></persName>
			<affiliation>
				<orgName type="collaboration">LIGO Scientific Collaboration ; Virgo Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Adams</surname></persName>
			<affiliation>
				<orgName type="collaboration">LIGO Scientific Collaboration ; Virgo Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Addesso</surname></persName>
			<affiliation>
				<orgName type="collaboration">LIGO Scientific Collaboration ; Virgo Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">X</forename><surname>Adhikari</surname></persName>
			<affiliation>
				<orgName type="collaboration">LIGO Scientific Collaboration ; Virgo Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Adya</surname></persName>
			<affiliation>
				<orgName type="collaboration">LIGO Scientific Collaboration ; Virgo Collaboration</orgName>
			</affiliation>
		</author>
		<idno type="DOI">10.1103/PhysRevD.97.102002</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. D</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page">102002</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning phase transitions from dynamics</title>
		<author>
			<persName><forename type="first">E</forename><surname>Van Nieuwenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bairey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Refael</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevB.98.060301</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. B</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">60301</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Challenges for commercializing perovskite solar cells</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Saidaminov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Seok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Mcgehee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Sargent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aat8235</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">361</biblScope>
			<biblScope unit="page">8235</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exceptional topology of non-hermitian systems</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Bergholtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Budich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Kunst</surname></persName>
		</author>
		<idno type="DOI">10.1103/RevModPhys.93.015005</idno>
	</analytic>
	<monogr>
		<title level="j">Rev. Mod. Phys</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page">15005</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Machine learning and the physical sciences</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carleo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cirac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Daudet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vogt-Maranto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zdeborová</surname></persName>
		</author>
		<idno type="DOI">10.1103/RevModPhys.91.045002</idno>
	</analytic>
	<monogr>
		<title level="j">Rev. Mod. Phys</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page">45002</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Artificial intelligence and machine learning for quantum technologies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Krenn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Landgraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Foesel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Marquardt</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevA.107.010101</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">10101</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scientific discovery in the age of artificial intelligence</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chandak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Katwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deac</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-023-06221-2</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">620</biblScope>
			<biblScope unit="page">47</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">microscopical observations on the blood vessels and membranes of the intestines. in a letter to the royal society from mr. anthony van leeuwenhoek, frs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Leeuwenhoek</surname></persName>
		</author>
		<author>
			<persName><surname>Ii</surname></persName>
		</author>
		<idno type="DOI">10.1098/rstl.1708.0007</idno>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society of London</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">53</biblScope>
			<date type="published" when="1709">1709</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On scientific understanding with artificial intelligence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Krenn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pollice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aldeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cervera-Lierta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Friederich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dos Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Häse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jinich</surname></persName>
		</author>
		<author>
			<persName><surname>Nigam</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42254-022-00518-3</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Physics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">761</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The cso classifier: Ontology-driven detection of research topics in scholarly articles</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Salatino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Thanapalasingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Motta</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-30760-8_26</idno>
	</analytic>
	<monogr>
		<title level="m">Digital Libraries for Open Knowledge: 23rd International Conference on Theory and Practice of Digital Libraries</title>
		<meeting><address><addrLine>Oslo, Norway</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-09-09">2019. September 9-12, 2019. 2019</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="296" to="311" />
		</imprint>
	</monogr>
	<note>TPDL</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automatic keyword extraction from individual documents</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cowley</surname></persName>
		</author>
		<idno type="DOI">10.1002/9780470689646.ch1</idno>
	</analytic>
	<monogr>
		<title level="m">Text mining: applications and theory</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Priem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Piwowar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Orr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01833</idno>
		<title level="m">Openalex: A fullyopen index of scholarly works, authors, venues, institutions, and concepts</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking : Bringing order to the web</title>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Stanford InfoLab</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
		<title level="m">Network Science</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Predicting research trends in artificial intelligence with gradient boosting decision trees and time-aware graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Big Data (Big Data</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5809" to="5814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Roc graphs: Notes and practical considerations for researchers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00116251</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">81</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32 (NeurIPS)</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.5555/3104322.3104425</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">2825</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The physics of higher-order interactions in complex systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Battiston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Amico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bianconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ferraz De Arruda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Franceschiello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Iacopini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kéfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Latora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Moreno</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41567-021-01371-4</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Physics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">1093</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Prediction of robust scientific facts from literature</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Belikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rzhetsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Evans</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-022-00474-8</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">445</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rzhetsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
		<idno type="DOI">10.1177/0003122415601618</idno>
	</analytic>
	<monogr>
		<title level="m">Tradition and innovation in scientists&apos; research strategies</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">875</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Accelerating science with human-aware artificial intelligence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sourati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-023-01648-z</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1682</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">How are topics born? understanding the research dynamics preceding the emergence of new areas</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Salatino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Motta</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj-cs.119</idno>
	</analytic>
	<monogr>
		<title level="j">PeerJ Computer Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">119</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Augur: forecasting the emergence of new research topics, in Proceedings of the 18th ACM</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Salatino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Motta</surname></persName>
		</author>
		<idno type="DOI">10.1145/3197026.3197052</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Sciscinet: A largescale open data lake for the science of science research</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-023-02198-9</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">315</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A dataset of publication records for nobel laureates</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-019-0033-6</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Surprise! measuring novelty as expectation violation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Evans</surname></persName>
		</author>
		<idno type="DOI">10.31235/osf.io/2t46f</idno>
	</analytic>
	<monogr>
		<title level="j">SocArXiv</title>
		<imprint>
			<date type="published" when="2021">2t46f (2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Surprising combinations of research contents and contexts are related to impact and emerge with scientific outsiders from distant disciplines</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Evans</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-023-36741-4</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">1641</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Interesting scientific idea generation using knowledge graphs and llms</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krenn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.17044</idno>
	</analytic>
	<monogr>
		<title level="m">Evaluations with 100 research group leaders</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Impact4cast: Forecasting high-impact research topics via machine learning on evolving knowledge graphs [data set</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.10692137</idno>
		<ptr target="https://doi.org/10.5281/zenodo.10692137" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Benchmark dataset for impact4cast: Forecasting high-impact research topics via machine learning on evolving knowledge graphs [data set]. zenodo</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.14527306</idno>
		<ptr target="https://doi.org/10.5281/zenodo.14527306" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
