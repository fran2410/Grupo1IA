<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From large language models to multimodal AI: A scoping review on the potential of generative AI in medicine</title>
				<funder>
					<orgName type="full">Deutsche Forschungsgemeinschaft (DFG)</orgName>
				</funder>
				<funder ref="#_6NvvqTq">
					<orgName type="full">Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
				</funder>
				<funder ref="#_EfpEyDp">
					<orgName type="full">Free State of Bavaria</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-13">13 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Lukas</forename><surname>Buess</surname></persName>
							<email>lukas.buess@fau.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Recognition Lab</orgName>
								<orgName type="institution">Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthias</forename><surname>Keicher</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Maier</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Recognition Lab</orgName>
								<orgName type="institution">Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Soroosh</forename><surname>Tayebi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Recognition Lab</orgName>
								<orgName type="institution">Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">From large language models to multimodal AI: A scoping review on the potential of generative AI in medicine</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-13">13 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">9E75BF1F65CC3E13AEC210ACB167DAD1</idno>
					<idno type="arXiv">arXiv:2502.09242v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.3-SNAPSHOT" ident="GROBID" when="2025-05-13T17:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">0.8.2-3-g65968aec5</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Large language models</term>
					<term>Generative AI</term>
					<term>Multimodal AI</term>
					<term>Scoping review Study Modalities Application BiomedCLIP [82] Medical images</term>
					<term>Descriptions Classification</term>
					<term>Retrieval</term>
					<term>Visual QA BioViL [83] X-ray</term>
					<term>Reports Classification</term>
					<term>Grounding BioViL-T [84] X-ray</term>
					<term>Reports Classification</term>
					<term>Grounding</term>
					<term>Reporting CheXzero [74] X-ray</term>
					<term>Reports Classification</term>
					<term>Retrieval ConVIRT [85] X-ray</term>
					<term>Reports Classification</term>
					<term>Retrieval CPLIP [86] Histopathology images</term>
					<term>Descriptions Classification CT-CLIP [14] CT</term>
					<term>Reports</term>
					<term>Labels Classification</term>
					<term>Retrieval CT Foundation [87] CT</term>
					<term>Reports Classification</term>
					<term>Retrieval CXR-RePaiR [75] X-ray</term>
					<term>Reports Reporting ETP [88] ECG</term>
					<term>Reports Classification FairCLIP [89] SLO fundus images</term>
					<term>Clinical notes Classification FiVE [90] Histopathology images</term>
					<term>Descriptions Classification FlexR [91] X-ray</term>
					<term>Reports Classification GLoRIA [92] X-ray</term>
					<term>Reports Classification</term>
					<term>Retrieval</term>
					<term>Segmentation KAD [93] X-ray</term>
					<term>Reports Classification MaCo [94] X-ray</term>
					<term>Reports Classification MCPL [95] X-ray</term>
					<term>Reports Classification MedImageInsight [96] Medical images</term>
					<term>Descriptions Classification</term>
					<term>Retrieval</term>
					<term>Reporting Med-MLLM [97] CT</term>
					<term>X-ray</term>
					<term>Descriptions Classification</term>
					<term>Reporting Merlin [77] CT</term>
					<term>EHR</term>
					<term>Reports Classification</term>
					<term>Retrieval</term>
					<term>Reporting</term>
					<term>Segmentation MedViLL [98] X-ray</term>
					<term>Reports Classification</term>
					<term>Retrieval</term>
					<term>Reporting</term>
					<term>Visual QA MoleculeSTM [99] Molecule structure</term>
					<term>Descriptions Retrieval MolLM [100] Molecule structures</term>
					<term>Descriptions Retrieval</term>
					<term>Molecule description PLIP [101] Histopathology images</term>
					<term>Descriptions Classification</term>
					<term>Retrieval Prov-GigaPath [102] Histopathology images</term>
					<term>Reports Classification UniMed-CLIP [103] Medical images</term>
					<term>Captions Classification Xplainer [104] X-ray</term>
					<term>Reports Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative artificial intelligence (AI) models, such as diffusion models and Ope-nAI's ChatGPT, are transforming medicine by enhancing diagnostic accuracy and automating clinical workflows. The field has advanced rapidly, evolving from textonly large language models for tasks such as clinical documentation and decision support to multimodal AI systems capable of integrating diverse data modalities, including imaging, text, and structured data, within a single model. The diverse landscape of these technologies, along with rising interest, highlights the need for a comprehensive review of their applications and potential. This scoping review explores the evolution of multimodal AI, highlighting its methods, applications, datasets, and evaluation in clinical settings. Adhering to PRISMA-ScR guidelines, we systematically queried PubMed, IEEE Xplore, and Web of Science, prioritizing recent studies published up to the end of 2024. After rigorous screening, 144 papers were included, revealing key trends and challenges in this dynamic field. Our findings underscore a shift from unimodal to multimodal approaches, driving innovations in diagnostic support, medical report generation, drug discovery, and conversational AI. However, critical challenges remain, including the integration of heterogeneous data types, improving model interpretability, addressing ethical concerns, and validating AI systems in real-world clinical settings. This review summarizes the current state of the art, identifies critical gaps, and provides insights to guide the development of scalable, trustworthy, and clinically impactful multimodal AI solutions in healthcare.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative artificial intelligence (AI), exemplified by models like ChatGPT, has drawn widespread attention for its ability to process and generate human-like text, substantially advancing various domains. In healthcare, these models have rapidly transformed traditional approaches by offering capabilities beyond conventional data analysis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. For instance, large language models (LLMs) have been applied in tasks such as summarizing medical records <ref type="bibr" target="#b2">[3]</ref>, assisting in diagnostic reasoning <ref type="bibr" target="#b3">[4]</ref>, and conducting bioinformatics research <ref type="bibr" target="#b4">[5]</ref>. These advancements highlight the ability of LLMs to process and interpret complex clinical language, improving efficiency and accuracy across tasks such as radiology reporting. Recent studies further demonstrate their impact, showing that AI-generated draft radiology reports can reduce reporting time by about 25% while maintaining diagnostic accuracy <ref type="bibr" target="#b5">[6]</ref>, thus addressing workload challenges in clinical practice <ref type="bibr" target="#b6">[7]</ref>.</p><p>However, healthcare data extends far beyond clinical texts, encompassing diverse modalities such as medical images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, laboratory results <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, and genomic data <ref type="bibr" target="#b11">[12]</ref>. To address this diversity, multimodal AI systems have emerged, integrating these data types within a single model. This integration paves the way for comprehensive decision support systems that more closely mimic human clinical reasoning. Recent advancements in multimodal AI represent a significant shift, expanding generative AI applications beyond language-focused tasks to more complex data integration scenarios <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. By unifying text, images, and other clinical data, these systems hold potential for improved diagnostic accuracy and broader applications, from predictive analytics to complex interventional support <ref type="bibr" target="#b15">[16]</ref>. Several recent review articles have provided valuable overviews of multimodal AI and LLMs. Comprehensive surveys of multimodal large language models (MLLMs) in the broader computer vision domain were presented by Yin et al. <ref type="bibr" target="#b16">[17]</ref> and Wang et al. <ref type="bibr" target="#b17">[18]</ref>, highlighting recent advancements, providing a summary of architectural developments, and identifying key trends in model evolution. A broader perspective on multimodal approaches in healthcare was provided by Kline et al. <ref type="bibr" target="#b18">[19]</ref> and Acosta et al. <ref type="bibr" target="#b0">[1]</ref>. He et al. <ref type="bibr" target="#b19">[20]</ref> present a comprehensive collection of foundation models, spanning from image-only architectures to advanced multimodal models.</p><p>While previous reviews provide essential insights, the dynamic and rapidly evolving nature of this field necessitates an up-to-date and focused exploration of recent developments in LLM-based multimodal AI for medicine. This review aims to fill this gap by providing a comprehensive overview of the evolution from text-only LLMs to multimodal AI systems in medicine, with a particular emphasis on recent advancements. Unlike prior reviews, we also discuss evaluation methods specifically tailored to the challenges and requirements of medical generative AI, ensuring real-world clinical utility and reliability.</p><p>To guide this review, we formulated the following research questions:</p><p>• What methods are commonly used in the development of generative AI for healthcare applications? • What datasets support the development of generative AI in medical contexts?</p><p>• Which evaluation metrics are employed to assess the utility of generative AI models in medical contexts?</p><p>In the following sections, we first outline the methodology employed for literature collection and selection, detailing the search strategy, inclusion criteria, and data extraction processes used to ensure a comprehensive review. We then present our findings, emphasizing the shift from text-only LLMs to multimodal AI systems in medicine, with a particular focus on their applications, datasets, model architectures, and evaluation metrics. Our results reveal a significant shift towards multimodal models, which are driving innovation across various areas of healthcare. However, persistent challenges remain, particularly in the evaluation of these models, including the assessment of their reliability, clinical relevance, and generalizability. Finally, we provide an outlook on the future of generative AI in medicine, offering insights to guide further research and development in this rapidly evolving field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Our scoping review followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews (PRISMA-ScR) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, which provides a standardized framework for methodological transparency in scoping reviews. This section details the data collection methods used in our review. The complete PRISMA-ScR checklist is available in Supplementary Table <ref type="table">S</ref>.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Search strategy</head><p>The literature search consisted of a systematic database search structured into two subsearches to capture the development and application of text-only LLMs and multimodal models in medicine. The first subsearch targeted text-only LLMs using the keyword groups "medical" and "language model". The second subsearch focused on multimodal models, using three groups of keywords: "medical", "language model", and "multimodal". The full search queries, including the specific combinations used, are provided in Supplementary Table S.2. Additionally, a manual search was performed to identify recent preprints, datasets, and other resources not captured by the database search, which continued through the end of 2024 to ensure the inclusion of the most current and impactful studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Inclusion and exclusion criteria</head><p>The selection process began with structured database queries, followed by duplicate removal, title and abstract screening, and subsequent full-text reviews for potentially relevant papers. We excluded articles that were non-medical or lacked methodological novelty. To ensure balanced representation across application areas, we aimed for proportional inclusion from prevalent fields, such as X-ray report generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Synthesis of results</head><p>The selected papers were categorized through a two-step process. First, they were grouped by topics, including text-only LLMs, multimodal models, datasets, and evaluation metrics. Within each topic, papers were further categorized based on their application areas. This dual-layer categorization provides a structured overview of developments in generative AI for medicine, illustrating the progression from textonly LLMs to multimodal models. Key publications are summarized through narrative descriptions and tables, offering insights into methodological approaches, application domains, datasets, and evaluation frameworks to provide a comprehensive understanding of current trends and challenges. Tables 1 (text-only LLMs), 2 (text-only datasets), 3 (contrastive learning methods), 4 (MLLMs), 5 (multimodal datasets), and 6 (evaluation metrics) summarize the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Included studies</head><p>A total of 4,384 papers were retrieved from three databases. After removing duplicates, 2,656 articles were excluded during the initial screening based on their titles and abstracts, following the predefined inclusion and exclusion criteria. The remaining articles underwent a full-text review, during which both relevance and topic diversity were considered to avoid overrepresentation of similar studies. This step led to the exclusion of an additional 249 papers. Ultimately, 60 papers from the database search were included in the review. Additionally, 83 papers were identified through manual searches to capture the most current and relevant studies not covered in the database queries. Figure <ref type="figure" target="#fig_1">2</ref> provides an overview of the full screening process. In total, 144 papers were included in this review. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Language models in medicine</head><p>Mono-modal LLMs, which process textual data exclusively, have laid the foundation for the development of multimodal systems, demonstrating remarkable capabilities in understanding and generating human-like text. In the medical domain, LLMs demonstrated high effectiveness in processing and analyzing complex clinical data, enabling advancements in applications such as clinical documentation, medical literature summarization, and diagnostic support <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>. Their success is based on the transformer architecture, introduced in the landmark paper "Attention Is All You Need" <ref type="bibr" target="#b24">[25]</ref>, which employs self-attention mechanisms to effectively capture contextual relationships and long-range dependencies in text. This architecture has enabled LLMs to scale effectively, making them capable of processing medical texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">LLM methods</head><p>LLMs tailored to medical applications (Table <ref type="table">1</ref>) leverage various approaches to adapt general-purpose models for specialized medical tasks. A prevalent method is supervised finetuning (SFT), where general LLMs are finetuned on domain-specific datasets, such as biomedical literature and clinical notes, to enhance their understanding of medical concepts and vocabulary. This approach has been instrumental in models like BioBERT and BioMistral, which adapt general-purpose language models for biomedical applications <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>In contrast to SFT, prompt engineering techniques have emerged as a lightweight alternative for guiding pretrained models without additional training, relying on carefully designed input prompts to achieve strong task performance in medical text understanding and generation <ref type="bibr" target="#b27">[28]</ref>.</p><p>Advanced alignment techniques such as reinforcement learning from human feedback (RLHF) have been developed to further refine the outputs of LLMs for medical applications. RLHF leverages reward models trained on expert feedback to align model responses with clinical expectations. However, due to the cost of obtaining expert feedback in the healthcare domain, reinforcement learning from AI feedback (RLAIF) has emerged as an alternative <ref type="bibr" target="#b28">[29]</ref>. This technique replaces human feedback with evaluations from auxiliary AI models, reducing reliance on scarce human resources while maintaining alignment capabilities. A notable example is HuatuoGPT <ref type="bibr" target="#b29">[30]</ref>, which uses RLAIF for clinical alignment.</p><p>Another recent development in model adaption is chain-of-thought (CoT) prompting, a technique where models generate intermediate reasoning steps before producing a final answer. By breaking down complex tasks into substeps, CoT enhances model explainability and task performance, which is especially valuable in the medical domain as it not only improves accuracy but also increases trust in the model's reasoning process. For example, HuatuoGPT-o1 <ref type="bibr" target="#b30">[31]</ref> applies CoT prompting to improve medical response clarity and ensure step-by-step diagnostic reasoning.</p><p>An additional adaptation technique is retrieval augmented generation (RAG) <ref type="bibr" target="#b31">[32]</ref>, which equips LLMs with mechanisms to query external knowledge bases during inference. This approach enables models to access up-to-date information, such as medical guidelines or recent research findings, without requiring retraining. For instance, Almanac <ref type="bibr" target="#b32">[33]</ref>, ChatDoctor <ref type="bibr" target="#b3">[4]</ref>, and RadioRAG <ref type="bibr" target="#b33">[34]</ref> combine generative capabilities with retrieval systems. However, maintaining the retrieval database and ensuring its comprehensiveness pose ongoing challenges <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">LLM applications</head><p>LLMs have revolutionized various applications in biomedical language processing, demonstrating utility across a range of tasks. In named entity recognition (NER), they</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study Downstream task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clinical text</head><p>Almanac <ref type="bibr" target="#b32">[33]</ref> QA BioALBERT <ref type="bibr" target="#b35">[36]</ref> NER BioBERT <ref type="bibr" target="#b25">[26]</ref> NER, QA BioGPT <ref type="bibr" target="#b36">[37]</ref> Classification, QA BioMistral <ref type="bibr" target="#b26">[27]</ref> QA ChatDoctor <ref type="bibr" target="#b3">[4]</ref> Dialogue ChestXRayBERT <ref type="bibr" target="#b2">[3]</ref> Summarization DRG-LLaMA <ref type="bibr" target="#b37">[38]</ref> Classification GatorTron <ref type="bibr" target="#b38">[39]</ref> QA HuatuoGPT <ref type="bibr" target="#b29">[30]</ref> Dialogue HuatuoGPT-o1 <ref type="bibr" target="#b29">[30]</ref> Dialogue Johnson et al. <ref type="bibr" target="#b39">[40]</ref> Deidentification Kresevic et al. <ref type="bibr" target="#b40">[41]</ref> Summarization Mahendran and McInnes <ref type="bibr" target="#b41">[42]</ref> NER MAPLEZ <ref type="bibr" target="#b42">[43]</ref> Classification Med-BERT <ref type="bibr" target="#b43">[44]</ref> NER MedAlpaca <ref type="bibr" target="#b44">[45]</ref> QA MEDITRON-70B <ref type="bibr" target="#b45">[46]</ref> QA MED-PaLM <ref type="bibr" target="#b46">[47]</ref> QA MMed-Llama 3 <ref type="bibr" target="#b47">[48]</ref> QA Mu et al. <ref type="bibr" target="#b48">[49]</ref> Classification NYUTron <ref type="bibr" target="#b23">[24]</ref> Clinical outcome prediction PMC-LLaMA <ref type="bibr" target="#b49">[50]</ref> QA PodGPT <ref type="bibr" target="#b50">[51]</ref> QA RadBERT <ref type="bibr" target="#b51">[52]</ref> Classification, Summarization Schmidt et al. <ref type="bibr" target="#b52">[53]</ref> Error detection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bioinformatics</head><p>AlphaFold <ref type="bibr" target="#b4">[5]</ref> Structure prediction BioPhi <ref type="bibr" target="#b53">[54]</ref> Antibody design CADD v1.7 <ref type="bibr" target="#b54">[55]</ref> Scoring DNABERT <ref type="bibr" target="#b55">[56]</ref> Structure analysis Geneformer <ref type="bibr" target="#b56">[57]</ref> Classification Hie et al. <ref type="bibr" target="#b57">[58]</ref> Antibody design MSA Transformer <ref type="bibr" target="#b58">[59]</ref> Structure analysis ProGen <ref type="bibr" target="#b59">[60]</ref> Structure prediction ProtGPT2 <ref type="bibr" target="#b60">[61]</ref> Protein design ProtTrans <ref type="bibr" target="#b61">[62]</ref> Structure analysis scBERT <ref type="bibr" target="#b62">[63]</ref> Classification ToxinPred 3.0 <ref type="bibr" target="#b63">[64]</ref> Classification</p><p>Table 1: Summary of LLM methods, categorized by their application to clinical text and bioinformatics tasks. The table includes method names and target applications. (Abbreviations: NER -named entity recognition, QA -question answering)</p><p>enable the extraction of critical medical entities, such as diseases, drugs, and symptoms from unstructured text. This capability supports clinical data annotation, which is crucial for automated clinical decision support systems <ref type="bibr" target="#b35">[36]</ref>. Dialogue systems represent another application of LLMs in medicine. Models like ChatDoctor <ref type="bibr" target="#b3">[4]</ref> and HuatuoGPT <ref type="bibr" target="#b29">[30]</ref> facilitate patient interactions, simulate doctorpatient consultations, and assist in providing medical information and guidance. These systems aim to reduce barriers to medical access by providing instant responses.</p><p>In summarization tasks, medical LLMs condense lengthy electronic health records (EHRs) into concise summaries. This application significantly reduces the documentation burden on healthcare providers and aids decision-making by presenting critical patient information in a structured format <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b64">65]</ref>.</p><p>Deidentification and privacy-preserving applications are critical areas where LLMs contribute to medical data management by safeguarding patient confidentiality in sensitive clinical texts. LLMs can automate the removal of protected health information from medical documents by anonymizing identifiers such as names and dates while preserving data utility <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Text classification tasks also benefit from LLM advancements, with applications such as predicting patient outcomes and categorizing medical literature <ref type="bibr" target="#b23">[24]</ref>.</p><p>In bioinformatics, LLMs have expanded beyond language processing to analyze biological sequences like DNA, RNA, and proteins. Models such as DNABERT <ref type="bibr" target="#b55">[56]</ref> have advanced gene annotation, while AlphaFold <ref type="bibr" target="#b4">[5]</ref> has achieved groundbreaking success in protein structure prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">LLM datasets</head><p>The development of medical LLMs relies on diverse and specialized datasets that capture the complexity of medical language, context, and tasks. These datasets fall into categories such as clinical text, domain-specific literature, conversational data, and bioinformatics resources, each serving distinct purposes in the development of medical LLMs. These datasets enable general-purpose LLMs to align with the medical domain, which is critical for achieving reliable and accurate outputs in clinical settings.</p><p>Clinical text datasets play a central role in training medical LLMs (see Table <ref type="table">2</ref>). For instance, EHR datasets like MIMIC-IV <ref type="bibr" target="#b65">[66]</ref> provide a rich source of structured and unstructured clinical data, commonly used for tasks such as summarization and NER, which are both essential for automating documentation and decision-making processes in healthcare. The eICU-CRD dataset <ref type="bibr" target="#b66">[67]</ref>, another EHR resource, focuses on intensive care unit patient data, further broadening the scope of potential applications.</p><p>To introduce domain-specific knowledge into LLMs, datasets like GAP-Replay <ref type="bibr" target="#b45">[46]</ref> and MedC-K <ref type="bibr" target="#b49">[50]</ref>, composed of biomedical literature and textbooks, are essential. These datasets are designed to equip models with the specialized terminology and reasoning patterns found in biomedical research and education.</p><p>For conversational AI in medicine, dialogue datasets are crucial. MedDialog <ref type="bibr" target="#b67">[68]</ref> provides examples of doctor-patient interactions, enabling LLMs to learn medical dialogues, including patient concerns, physician responses, and diagnostic reasoning. These datasets are essential for developing medical conversational assistance systems capable of simulating clinical dialogues and supporting in patient education, diagnostic reasoning, and post-treatment follow-ups.</p><p>Bioinformatics datasets extend the scope of LLM applications beyond clinical text, supporting tasks in genomics and molecular biology. Resources like AlphaFold DB <ref type="bibr" target="#b4">[5]</ref> and UniProtKB <ref type="bibr" target="#b68">[69]</ref> provide structured data for protein structure and sequence analysis, making them valuable for drug discovery and molecular research. Similarly, genomic datasets such as GENCODE <ref type="bibr" target="#b11">[12]</ref> and GenBank <ref type="bibr" target="#b69">[70]</ref> offer data for tasks like gene prediction, helping models to better understand complex biological patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Size Application</head><p>Clinical text eICU-CRD <ref type="bibr" target="#b66">[67]</ref> 200K instances EHR GAP-Replay <ref type="bibr" target="#b45">[46]</ref> 48.1B tokens Literature MedDialog-EN <ref type="bibr" target="#b67">[68]</ref> 250K dialogues Dialogue MedC-K <ref type="bibr" target="#b49">[50]</ref> 4.8M papers, 30K textbooks Literature MedC-I <ref type="bibr" target="#b49">[50]</ref> 202M tokens Dialogue, QA Medical Meadow <ref type="bibr" target="#b44">[45]</ref> 160K instances QA MIMIC-IV <ref type="bibr" target="#b65">[66]</ref> 299K patients EHR MMedC <ref type="bibr" target="#b47">[48]</ref> 25.5B tokens Multilingual literature MultiMedQA <ref type="bibr" target="#b46">[47]</ref> 213K instances QA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bioinformatics</head><p>AlphaFold DB <ref type="bibr" target="#b4">[5]</ref> 200M entries Protein Design CPTAC Data Portal <ref type="bibr" target="#b70">[71]</ref> NA Genomics, Protein Design GenBank <ref type="bibr" target="#b69">[70]</ref> NA sequences Genomics GENCODE <ref type="bibr" target="#b11">[12]</ref> NA Genomics UniProtKB <ref type="bibr" target="#b68">[69]</ref> 227M sequences Protein Design</p><p>Table 2: Summary of datasets used for training medical LLMs, categorized into clinical text and bioinformatics data. The table includes dataset names, sizes, and primary application areas. (Abbreviations: NA -not available, NER -named entity recognition, QA -question answering, EHR -electronic health record)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Multimodal language models in medicine</head><p>By showcasing the potential of LLMs in processing clinical text, these models have established a strong foundation for integrating additional modalities, leading to the development of multimodal language models specifically designed for healthcare. Multimodal models combine diverse data types, such as text and medical images, to tackle complex medical tasks, including report generation <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b72">73]</ref>, image-text retrieval <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b74">75]</ref>, and medical consultation <ref type="bibr" target="#b13">[14]</ref>. By building on advancements in LLMs, multimodal language models improve the integration and contextual understanding of multimodal medical data. This section provides an overview of recent architectures and methods addressing the unique challenges posed by multimodal medical data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Architectures</head><p>Before presenting the literature, we briefly outline the two primary architectures in multimodal AI, i.e., the contrastive language-image pretraining (CLIP) and MLLMs (see Figure <ref type="figure">4</ref>). These architectures serve as foundational frameworks for integrating multiple data modalities in medical AI. Although CLIP <ref type="bibr" target="#b75">[76]</ref> is not inherently generative, its ability to align images and text within a shared embedding space makes it a crucial component in multimodal AI systems. CLIP <ref type="bibr" target="#b75">[76]</ref> is designed to align different modalities, such as image and text, in a shared embedding space. Although originally developed for image-text pairs, its framework can be extended to other modalities, making it a versatile tool for various multimodal learning tasks. By jointly training on paired modalities data, it excels in tasks like zero-shot image classification <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b76">77]</ref>, where new classes can be recognized without additional training. This makes CLIP particularly useful for situations where annotated medical data is limited.</p><formula xml:id="formula_0">$ $ … $ # $ " $ ! &amp; ! &amp; " &amp; # … &amp; $ %! &amp; '" … … … … %# &amp; '" … %# &amp; '! %# &amp; '$ %$ &amp; '" %$ &amp; '# … %$ &amp; '! %$ &amp; '$ %" &amp; '" %" &amp; '# … %" &amp; '! %" &amp; '$ %! &amp; '$ %! &amp; '# … %# &amp; '# … %! &amp; '!</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modality Encoder</head><note type="other">Text Encoder</note><p>On the other hand, MLLMs, such as LLaVA <ref type="bibr" target="#b77">[78]</ref>, extend the capabilities of LLMs by integrating non-textual data directly into their embeddings. This integration allows for a more holistic understanding of complex datasets, combining linguistic context with multimodal features like images or clinical measurements. These models excel in tasks such as radiology report generation <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b72">73]</ref>, question answering (QA) about medical images <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b79">80]</ref>, and decision support in diagnosis <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b80">81]</ref>.</p><p>By leveraging complementary strengths, these architectures address the diverse challenges posed by multimodal medical data. CLIP is effective for aligning different data modalities, while MLLMs excel in diagnostic reasoning, together forming a powerful combination for improving multimodal AI in medicine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multimodal LLM methods</head><p>Modality alignment is a fundamental step for most MLLMs. Many approaches leverage CLIP-based methods (Table <ref type="table">3</ref>), which primarily focus on learning a shared latent space where modalities can be jointly represented for downstream tasks.</p><p>For instance, BiomedCLIP <ref type="bibr" target="#b81">[82]</ref> uses contrastive learning to align medical images with paired reports, achieving state-of-the-art results in retrieval tasks. Building on this framework, CheXzero <ref type="bibr" target="#b73">[74]</ref> adapts CLIP for zero-shot classification of X-ray images, while CT-CLIP <ref type="bibr" target="#b13">[14]</ref> extends this approach to computed tomography (CT) scans. Similarly, UniMed-CLIP <ref type="bibr" target="#b102">[103]</ref> enhances this paradigm by using classification datasets augmented by LLM-generated captions to train a foundation model capable of handling various medical image modalities.</p><p>More recent efforts have focused on large-scale pretrained models developed by industry leaders, aiming to generalize across diverse medical imaging tasks. Models like CT Foundation <ref type="bibr" target="#b86">[87]</ref> and MedImageInsight <ref type="bibr" target="#b95">[96]</ref>, accessible via application programming interfaces (APIs), exemplify this trend by offering robust pretrained embeddings that address data scarcity in medical imaging and support downstream applications.</p><p>While many CLIP-based methods focus on aligning text with medical images, recent approaches have extended this to other modalities. For example, ETP <ref type="bibr" target="#b87">[88]</ref> aligns electrocardiogram (ECG) signals <ref type="bibr" target="#b104">[105,</ref><ref type="bibr" target="#b105">106]</ref> with clinical reports, while MolLM <ref type="bibr" target="#b99">[100]</ref> pairs chemical structures with textual descriptions to support drug discovery.</p><p>Table <ref type="table">3</ref>: Summary of multimodal CLIP-based methods. The table includes method names, the modalities utilized (e.g., text and medical images), and the primary tasks addressed, such as image-text retrieval, report generation, and disease classification. (Abbreviations: QA -question answering) Table <ref type="table">4</ref>: Summary of multimodal MLLM-based methods. The table includes method names, the modalities utilized (e.g., text and medical images), and the primary tasks addressed, such as report generation, visual QA, and disease classification. (Abbreviations: QA -question answering) LLM-based methods, in contrast to CLIP approaches, directly integrate multimodal inputs into the language model's embeddings, enabling more complex reasoning and generative tasks. These approaches rely on modality-specific encoders to process non-textual data, converting them into feature embeddings compatible with the LLM's text-based representation space (Table <ref type="table">4</ref>). For instance, SkinGPT-4 <ref type="bibr" target="#b128">[129]</ref> and RaDialog <ref type="bibr" target="#b72">[73]</ref> integrate features from two-dimensional (2D) images, while models like Merlin <ref type="bibr" target="#b76">[77]</ref> and CT-CHAT <ref type="bibr" target="#b13">[14]</ref> extend this capability to volumetric three-dimensional (3D) CT data. Some models, such as MAIRA-2 <ref type="bibr" target="#b71">[72]</ref> and AutoRG-Brain <ref type="bibr" target="#b107">[108]</ref>, further ground text predictions by incorporating bounding boxes and segmentation masks, enabling interactive, region-based report generation for enhanced explainability <ref type="bibr" target="#b126">[127]</ref>.</p><p>Current advancements also focus on text-guided segmentation and synthetic medical image generation. Text-guided segmentation models like LViT create segmentation masks from textual prompts, enabling tasks such as tumor detection and organ identification <ref type="bibr" target="#b115">[116]</ref>. Beyond segmentation, synthetic image generation has emerged as another multimodal approach for data augmentation and model training. Methods such as GenerateCT <ref type="bibr" target="#b113">[114]</ref> for CT volumes and RoentGen <ref type="bibr" target="#b127">[128]</ref> for X-rays use text-conditioned diffusion models to produce realistic medical images <ref type="bibr" target="#b132">[133]</ref>.</p><p>Generalist models, such as BiomedGPT <ref type="bibr" target="#b12">[13]</ref> and MedVersa <ref type="bibr" target="#b79">[80]</ref>, unify multiple modalities and tasks through shared representations or mixture-of-experts strategies. These models employ specialized modules to process different modalities while a central language model coordinates their outputs, enabling tasks such as classification, segmentation, retrieval, and visual QA. This approach highlights the scalability and versatility of generalist models in addressing complex multimodal challenges in medicine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Multimodal LLM applications</head><p>MLLMs have been increasingly applied across diverse medical tasks, showcasing their potential to transform clinical workflows and decision support systems. This section highlights key applications where MLLMs contribute to improving healthcare.</p><p>A key advancement in multimodal AI is generalist models capable of handling diverse medical data types and tasks. Models such as BiomedGPT <ref type="bibr" target="#b12">[13]</ref> and RadFM <ref type="bibr" target="#b80">[81]</ref> support a wide range of imaging modalities and anatomical regions, enabling comprehensive diagnostic assistance across multiple specialties.</p><p>Radiology report generation remains one of the most important applications of MLLMs in healthcare, providing detailed textual descriptions directly from medical images. Systems such as MAIRA-2 <ref type="bibr" target="#b71">[72]</ref> and RaDialog <ref type="bibr" target="#b72">[73]</ref> have demonstrated their ability to generate comprehensive reports from X-rays, while CT-CHAT <ref type="bibr" target="#b13">[14]</ref> and AutoRG-Brain <ref type="bibr" target="#b107">[108]</ref> extend this capability to CT and magnetic resonance imaging (MRI) scans, respectively. These tools assist radiologists by automating preliminary reporting and standardizing documentation, potentially reducing reporting delays.</p><p>Visual QA systems support clinicians in querying medical images using natural language prompts, supporting real-time decision-making and diagnostic interpretation. For instance, models like LLaVA-Med <ref type="bibr" target="#b14">[15]</ref> and Med-Flamingo <ref type="bibr" target="#b118">[119]</ref> provide concise, contextually relevant answers to clinical queries, assisting radiologists and physicians in complex cases.</p><p>Synthetic medical image generation has become increasingly important for data augmentation and simulating rare pathological conditions. Models like GenerateCT <ref type="bibr" target="#b113">[114]</ref> and RoentGen <ref type="bibr" target="#b127">[128]</ref> generate realistic CT and X-ray images from textual prompts, enhancing dataset diversity.</p><p>Semantic scene modeling is another emerging application where models create structured representations of complex environments, such as the operating room. For example, ORacle <ref type="bibr" target="#b15">[16]</ref> generates semantic scene graphs to assist with surgical planning and intraoperative navigation by representing tools, anatomy, and procedural stages in a comprehensive framework.</p><p>Finally, systems like ReXplain <ref type="bibr" target="#b125">[126]</ref> aim to bridge communication gaps between clinicians and patients. By transforming radiology reports into patient-friendly video summaries, these models provide an accessible way to convey complex clinical information, further highlighting multimodal AI's potential to improve patient care.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Multimodal LLM datasets</head><p>Multimodal datasets integrating images, text, and other clinical information (Table <ref type="table">5</ref>) are essential for tasks such as radiology report generation, visual QA, and cross-modal retrieval. These datasets not only enable effective model training but are also crucial for ensuring fairness and generalization in medical AI systems. A range of multimodal datasets has been curated to support various medical imaging and diagnostic tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Modalities Size Application</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D-image-text</head><p>CheXpert <ref type="bibr" target="#b133">[134]</ref> X-ray, Reports, Labels 224K triplets Chest X-ray CheXinstruct <ref type="bibr" target="#b110">[111]</ref> X-ray, Instructions 8.5M instruction triplets Chest X-ray Harvard-FairVLMed <ref type="bibr" target="#b88">[89]</ref> SLO fundus images, Demographics, Notes 10K samples Ophthalmology MedTrinity-25M <ref type="bibr" target="#b134">[135]</ref> Medical images, Captions, Bounding-boxes 25M pairs Medical imaging MedVidQA <ref type="bibr" target="#b135">[136]</ref> Medical videos, Labels, QA-pairs 6K videos, 6K labels, 3K QA Medical videos MIMIC-CXR <ref type="bibr" target="#b7">[8]</ref> X-ray, Reports 377K images, 227K reports Chest X-ray MS-CXR <ref type="bibr" target="#b82">[83]</ref> X-ray, Descriptions, Bounding-boxes 1K image-sentence pairs, Bounding-boxes Chest X-ray OmniMedVQA <ref type="bibr" target="#b136">[137]</ref> Medical images, QA 118K images, 127K QA-pairs Medical imaging OpenPath <ref type="bibr" target="#b100">[101]</ref> Histopathology images, Captions 208K pairs Digital pathology PadChest <ref type="bibr" target="#b137">[138]</ref> X-ray, Reports 160K images, 109K texts Chest X-ray PathVQA <ref type="bibr" target="#b138">[139]</ref> Medical images, QA 5K images, 33K QA Medical imaging PMC-15M <ref type="bibr" target="#b81">[82]</ref> Medical images, Captions 15M image-text pairs Medical imaging PubMedVision <ref type="bibr" target="#b139">[140]</ref> Medical images, QA 1.3M QA pairs Medical imaging Quilt-1M <ref type="bibr" target="#b140">[141]</ref> Histopathology images, Captions 1M pairs Digital pathology Rad-ReStruct <ref type="bibr" target="#b141">[142]</ref> X-ray, Structured reports 3720 images, 3597 Reports Chest X-ray SLAKE [143] Medical images, QA 642 images, 14K QA pairs Medical imaging UniMed [103] Medical images, Captions 5.3M image-text pairs Medical imaging VQA-RAD [144] Radiology images, Captions 315 images, 3.5K QA pairs Radiology 3D-volume-text AMOS-MM [145] [146] CT, Reports, QA 2K image-report pairs, 7K QA Chest, abdomen, pelvis CT BrainMD [132] MRI, Reports, EHR 2.5K cases Brain MRI BIMCV-R [147] CT, Reports 8K image-report pairs CT CT-RATE [14] CT, Reports, Labels 25K triplets Chest CT INSPECT [9] CT, Reports, EHR, labels 23K image-report pairs, EHRs Chest CT M3D-Data [117] CT, Captions, QA, Masks 120K images, 42K captions, 509K QA, 149K masks CT RadGenome-Brain MRI [108] MRI, Reports, Masks 3.4K image-region-report triplets Brain MRI RadGenome-Chest CT [148] CT, Reports, Masks, QA 25K image-report pairs, 665K masks, 1.3M QA Chest CT Others Duke Breast Cancer MRI [149] Genomic, MRI images, Clinical data 922 subjects Breast cancer PTB-XL [150] ECG signals, Reports, Labels 21K triplets ECG PubChemSTM [99] Chemical structures, Descriptions 280K chemical structure-text pairs Drug design SwissProtCLAP [151] Protein Sequence, Text 441K sequence-text pairs Protein design</p><p>Table 5: Summary of multimodal datasets used for medical AI, grouped by modality categories. The table lists dataset names, the types of modalities (e.g., text and medical images), dataset sizes, and key applications such as image-text retrieval, report generation, and disease classification. (Abbreviations: QA -question answering.)</p><p>A substantial proportion of multimodal datasets focus on pairing vision and text data, as this combination is central to tasks where both visual context and descriptive language are critical for diagnostic interpretation. Notable public datasets like MIMIC-CXR <ref type="bibr" target="#b7">[8]</ref> and CheXpert <ref type="bibr" target="#b133">[134]</ref> provide rich resources for training 2D vision-language models in radiology. These datasets include not only radiology reports but also diseasespecific labels, enabling more comprehensive evaluations. For benchmarking report generation, ReXGradient <ref type="bibr" target="#b149">[152]</ref>, a private benchmark dataset of 10,000 studies collected across 67 medical sites in the United States, offers diverse coverage and serves as a reliable standard for radiology-specific performance evaluation.</p><p>Expanding beyond radiology, datasets like Quilt-1M <ref type="bibr" target="#b140">[141]</ref> have introduced multimodal resources covering additional domains such as digital pathology <ref type="bibr" target="#b121">[122,</ref><ref type="bibr" target="#b150">153]</ref>.</p><p>Recent advancements have also led to datasets tailored for 3D imaging modalities such as CT <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr">145,</ref><ref type="bibr" target="#b144">147]</ref> and MRI <ref type="bibr" target="#b107">[108]</ref>. Notably, RadMD <ref type="bibr" target="#b80">[81]</ref> integrates both 2D and 3D imaging modalities, supporting a broader range of applications.</p><p>In addition to image-text pairs, a few datasets now include task-specific annotations to support specialized applications. For instance, RadGenome-Brain MRI <ref type="bibr" target="#b107">[108]</ref> and RadGenome-Chest CT <ref type="bibr" target="#b145">[148]</ref> provide segmentation masks, while datasets like MedTrinity-25M <ref type="bibr" target="#b134">[135]</ref> offer bounding box annotations. These annotations are critical for grounding text predictions to specific regions of interest, enhancing both explainability and diagnostic accuracy in multimodal models.</p><p>The data formats of multimodal datasets also vary significantly based on their intended use cases. While datasets like OpenPath <ref type="bibr" target="#b100">[101]</ref> present images from publicly available sources in formats such as JPEG, datasets like MIMIC-CXR <ref type="bibr" target="#b7">[8]</ref> and CT-RATE <ref type="bibr" target="#b13">[14]</ref> preserve clinical formats such as Digital Imaging and Communications in Medicine (DICOM) and Neuroimaging Informatics Technology Initiative (NIfTI). These formats are essential for maintaining complete clinical information and enabling compatibility with healthcare systems.</p><p>Beyond traditional imaging and text combinations, datasets have also begun exploring additional modalities for specialized biomedical tasks. For example, Swis-sProtCLAP <ref type="bibr" target="#b148">[151]</ref> integrates protein sequence data to support protein design frameworks, highlighting the potential of multimodal datasets to extend AI applications beyond diagnostic imaging into molecular and genomic research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation metrics for generative AI in medicine</head><p>Evaluating generative AI in medicine is essential to ensure models produce accurate, clinically relevant, and reliable outputs <ref type="bibr" target="#b151">[154]</ref>. This section explores evaluation metrics for both text generation, such as radiology report generation, and image generation, emphasizing the importance of clinical validity and utility. As general-purpose metrics often fall short in capturing medical accuracy, domain-specific approaches are required.</p><p>As report generation is a key application of generative AI in medicine, research has focused on developing reliable evaluation strategies. While standard lexical metrics such as BLEU <ref type="bibr" target="#b154">[157]</ref>, ROUGE <ref type="bibr" target="#b155">[158]</ref>, and METEOR <ref type="bibr" target="#b156">[159]</ref> are commonly used, they often fail to reflect clinical accuracy, as high scores can be achieved despite factually incorrect outputs. To address this, specialized clinical metrics tailored for report generation have emerged (Table <ref type="table">6</ref>).</p><p>For instance, NER-based metrics like RaTEScore <ref type="bibr" target="#b152">[155]</ref> evaluate key medical entities extracted from both predicted and ground truth reports, offering a more targeted assessment of clinical relevance. RadFact <ref type="bibr" target="#b71">[72]</ref> further incorporates grounding by assessing factual correctness against reference image annotations. The GREEN metric described in <ref type="bibr" target="#b151">[154]</ref> goes beyond standard evaluations by integrating error detection with explanations. It provides a clinically grounded score alongside human-interpretable feedback on significant errors, making it a promising tool for both model validation and iterative improvement. ReXrank <ref type="bibr" target="#b149">[152]</ref>, a benchmark for chest X-ray report generation, combines lexical and clinical metrics for more task-specific assessment.</p><p>Additionally, clinical efficacy can be measured using standard classification metrics, such as precision, recall, sensitivity, specificity, and F1-score, particularly when evaluation datasets include labeled disease categories <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>. A text classifier can be trained</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clinical Metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lexical Metrics</head><p>Generate the medical report for the given image region.</p><p>The liver is of normal size and shape, with proportional lobes. No lesions are visible on both the left and right lobes of the liver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The correct report looks like this:</head><p>The liver is of normal size and shape, with proportional lobes. Low-density lesions are visible on both the left and right lobes of the liver.</p><p>ROUGE (0.96) BLEU (0.96) GREEN (0.5) RadGraph (0.89) RaTEScore (0.68) METEOR (0.96) BertScore (0.99)</p><p>Fig. <ref type="figure">4</ref>: Evaluation of generative AI in medicine: Lexical metrics from the general domain cannot completely capture the clinical correctness as they mainly cover text similarity. Clinically-relevant metrics like GREEN <ref type="bibr" target="#b151">[154]</ref>, RaTEScore <ref type="bibr" target="#b152">[155]</ref>, or Rad-Graph <ref type="bibr" target="#b153">[156]</ref> also evaluate the clinical correctness. on the generated reports to predict labels, enabling a more structured evaluation of diagnostic accuracy.</p><p>Evaluating image generation in medical AI requires considerations beyond standard image quality metrics like Fréchet Inception Distance <ref type="bibr" target="#b161">[164]</ref> and mean squared error. Since synthetic medical images are often used for data augmentation or diagnostic training, their clinical utility must be assessed alongside visual quality. One effective strategy involves generating condition-specific medical images and training a classifier on the synthetic data to evaluate its generalization performance on real clinical cases <ref type="bibr" target="#b113">[114]</ref>. This ensures that the generated images are not only visually realistic but also contribute to model performance on downstream tasks, such as disease classification and segmentation.</p><p>Despite advancements in specialized evaluation metrics for both text and image generation, challenges remain regarding their generalizability across clinical sites and datasets. Frameworks like ReXamine-Global <ref type="bibr" target="#b162">[165]</ref> address this by evaluating the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric Type Application</head><p>CheXbert <ref type="bibr" target="#b157">[160]</ref> Classification Chest X-ray report labeling CRAFT-MD <ref type="bibr" target="#b158">[161]</ref> Generative Conversation evaluation FineRadScore <ref type="bibr" target="#b159">[162]</ref> Generative Report evaluation GREEN <ref type="bibr" target="#b151">[154]</ref> Generative Report evaluation Ong Ly et al. <ref type="bibr" target="#b160">[163]</ref> Calibration Model generalization RadCliQ <ref type="bibr" target="#b153">[156]</ref> Composite metric Report evaluation RadFact <ref type="bibr" target="#b71">[72]</ref> Grounding Grounded report evaluation RadGraph-F1 <ref type="bibr" target="#b153">[156]</ref> NER similarity Report evaluation RaTEScore <ref type="bibr" target="#b152">[155]</ref> NER similarity Report evaluation</p><p>Table 6: Evaluation metrics for medical report generation. This table summarizes key metrics used to evaluate generative AI systems in medical report generation, categorized by type and primary application. (Abbreviations: NER -named entity recognition) robustness of metrics across diverse institutions and data distributions. For text generation, a combination of lexical metrics and clinically grounded assessments is essential to ensure factual correctness and clinical relevance. Similarly, for image generation, both visual quality and downstream clinical utility, such as diagnostic performance on real clinical cases, should be jointly evaluated. Ultimately, a multi-dimensional evaluation approach that considers both data diversity and task-specific requirements is crucial for the safe and effective deployment of generative AI in healthcare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>In this scoping review, we systematically explored the evolution of generative AI in medicine, focusing on LLMs, multimodal LLMs, and their evaluation metrics. Using the PRISMA-ScR framework <ref type="bibr" target="#b20">[21]</ref>, we collected 144 papers published between January 2020 and December 2024 from PubMed, IEEE Xplore, and Web of Science, complemented by a manual search to ensure comprehensive coverage. Our findings highlight the shift from unimodal LLMs focused on textual tasks to more complex multimodal systems capable of integrating medical images, clinical notes, and structured data. These models have shown promise in enhancing diagnostic support, automating clinical workflows, and reducing the workload of healthcare professionals.</p><p>LLMs have advanced biomedical language processing, improving tasks like medical report summarization, named entity recognition, and conversational AI. Adaptation techniques such as supervised finetuning, reinforcement learning, and RAG have further specialized language models for clinical tasks. However, reliance on static datasets like MIMIC-IV <ref type="bibr" target="#b65">[66]</ref> limits the ability to capture evolving medical knowledge. Moreover, privacy issues persist due to the need for extensive data deidentification, and dataset biases can affect fairness by overrepresenting specific populations <ref type="bibr" target="#b163">[166,</ref><ref type="bibr" target="#b164">167]</ref>.</p><p>Multimodal LLMs extend LLM capabilities by integrating multiple data types, such as medical images and text, to address tasks like report generation, cross-modal retrieval, and clinical question answering. Despite these advancements, data heterogeneity remains a challenge, as clinical datasets often vary significantly in format, quality, and completeness across institutions. Additionally, most widely used datasets, such as MIMIC-CXR and CT-RATE <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>, focus heavily on radiology, limiting the generalizability of models to other medical domains.</p><p>Evaluating generative AI models in medicine requires specialized metrics that go beyond standard language evaluation metrics. While lexical metrics like BLEU <ref type="bibr" target="#b154">[157]</ref> and ROUGE <ref type="bibr" target="#b155">[158]</ref> are commonly used, they often fail to capture clinical relevance and factual accuracy. To address this, domain-specific metrics such as RadGraph <ref type="bibr" target="#b153">[156]</ref>, RaTEScore <ref type="bibr" target="#b152">[155]</ref>, and GREEN <ref type="bibr" target="#b151">[154]</ref> have been developed to assess the clinical validity of generated medical reports. However, challenges remain in standardizing evaluation practices across diverse medical tasks and datasets. Most evaluations are limited to radiology, with less attention given to other specialties. The limited availability of well-annotated multimodal datasets with fine-grained clinical labels further complicates performance benchmarking. Additionally, only a few benchmarking frameworks, such as ReXrank <ref type="bibr" target="#b149">[152]</ref>, offer the ability to neutrally evaluate models on non-public datasets, limiting comparative performance assessments across different models and data sources. Expanding such benchmarks and ensuring their applicability to a broader range of clinical tasks is essential for developing trustworthy generative models in medicine.</p><p>While this scoping review provides a comprehensive overview of generative AI advancements in medicine, it has certain limitations. Despite the systematic search strategy using the PRISMA-ScR framework, the literature search may not have captured all relevant studies due to the rapidly evolving nature of the field. To mitigate this, a manual search was conducted alongside the database queries to ensure the inclusion of recent and high-impact publications. Moreover, while efforts were made to cover multiple clinical specialties, there remains an overrepresentation of radiologyfocused datasets and models, reflecting a broader trend in the literature. We aimed to balance the inclusion of topics and application areas by diversifying the datasets and models included in our analysis, but certain domains such as pathology and genomics remain less represented due to the current availability of multimodal datasets in these fields.</p><p>To further advance the development and responsible deployment of generative AI in medicine, several areas need attention <ref type="bibr" target="#b165">[168]</ref><ref type="bibr" target="#b166">[169]</ref><ref type="bibr" target="#b167">[170]</ref>. First, evaluation frameworks need to evolve beyond lexical metrics by incorporating clinically grounded assessments and domain-specific error analysis. Second, expanding the diversity of training datasets is critical. The current overrepresentation of western institutions and radiology-focused datasets risks introducing biases that limit global applicability <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b133">134]</ref>. Future datasets should encompass a wider range of medical specialties, imaging modalities, and patient demographics, with careful attention to privacy protection and data fairness. Third, improving model explainability remains a priority <ref type="bibr" target="#b168">[171,</ref><ref type="bibr" target="#b169">172]</ref>. Techniques such as region-specific grounding can help build clinician trust. Finally, the emergence of generalist models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b79">80]</ref> capable of handling multiple modalities and tasks within a unified architecture represents an important step forward, but broader coverage across medical specialties and improved datasets remain essential for widespread adoption.</p><p>This scoping review provides a structured analysis of the evolution from unimodal LLMs to multimodal generative AI models in medicine, highlighting their potential for improving diagnostic support, clinical documentation, and decision-making. However, challenges related to data diversity, clinical relevance, model interpretability, and the standardization of evaluation metrics remain critical barriers to widespread adoption. Addressing these challenges through interdisciplinary collaboration, improved datasets, and clinically grounded evaluation strategies will be essential to ensure the responsible deployment of generative AI in healthcare. benchmark and method for structured radiology reporting. In: International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 409-419 (2023). Springer Section Item</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PRISMA-ScR checklist item Section</head><p>Information sources 7 Describe all information sources in the search (e.g., databases with dates of coverage and contact with authors to identify additional sources), as well as the date the most recent search was executed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2</head><p>Search 8 Present the full electronic search strategy for at least 1 database, including any limits used, such that it could be repeated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2</head><p>Selection of sources of evidence 9</p><p>State the process for selecting sources of evidence (i.e., screening and eligibility) included in the scoping review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3</head><p>Data charting process 10 Describe the methods of charting data from the included sources of evidence (e.g., calibrated forms or forms that have been tested by the team before their use, and whether data charting was done independently or in duplicate) and any processes for obtaining and confirming data from investigators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.4</head><p>Data items 11</p><p>List and define all variables for which data were sought and any assumptions and simplifications made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3</head><p>Critical appraisal of individual sources of evidence 12 If done, provide a rationale for conducting a critical appraisal of included sources of evidence; describe the methods used and how this information was used in any data synthesis (if appropriate).</p><p>N/A Synthesis of results 13 Describe the methods of handling and summarizing the data that were charted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.5</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selection of sources of evidence 14</head><p>Give numbers of sources of evidence screened, assessed for eligibility, and included in the review, with reasons for exclusions at each stage, ideally using a flow diagram.</p><p>3 Section Item PRISMA-ScR checklist item Section Characteristics of sources of evidence 15 For each source of evidence, present characteristics for which data were charted and provide the citations 4, 5, 6 Critical appraisal within sources of evidence 16 If done, present data on critical appraisal of included sources of evidence (see item 12). N/A Results of individual sources of evidence 17 For each included source of evidence, present the relevant data that were charted that relate to the review questions and objectives. 4, 5, 6 Synthesis of results 18 Summarize and/or present the charting results as they relate to the review questions and objectives 4, 5, 6 Discussion Summary of evidence 19 Summarize the main results (including an overview of concepts, themes, and types of evidence available), link to the review questions and objectives, and consider the relevance to key groups. 7 Limitations 20 Discuss the limitations of the scoping review process 7 Conclusions 21 Provide a general interpretation of the results with respect to the review questions and objectives, as well as potential implications and/or next steps. 7 Funding Funding 19</p><p>Describe sources of funding for the included sources of evidence, as well as sources of funding for the scoping review. Describe the role of the funders of the scoping review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional information</head><p>Table <ref type="table">S</ref>.1: PRISMA-ScR checklist <ref type="bibr" target="#b20">[21]</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Multimodal AI pipeline in healthcare: (A) Diverse medical data modalities (e.g., images, genomics, and clinical notes) are collected and processed, (B) transformed into unified representations by AI models, (C) used to generate insights such as reports, conversational assistance, and treatment plans, and (D) refined through iterative feedback to continuously optimize data collection and AI performance.</figDesc><graphic coords="2,187,54,445,69,51,51,51,61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: PRISMA flow diagram illustrating the study selection process for the scoping review. The diagram shows the number of records identified through database searches and manual searches, the removal of duplicates, the screening of titles and abstracts, the review of full-text articles, and the final inclusion of studies in the review.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Multimodal architectures: (A) CLIP-based models, which align embeddings of different modalities in a shared latent space, and (B) LLM-based models, which directly integrate different modality inputs through feature extraction and projection into LLM's ebbedding space.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements This work was partially funded via the EVUK programme ("<rs type="projectName">Next-generation AI for Integrated Diagnostics</rs>") of the <rs type="funder">Free State of Bavaria</rs>, the <rs type="funder">Deutsche Forschungsgemeinschaft (DFG)</rs>, and <rs type="funder">Friedrich-Alexander-Universität Erlangen-Nürnberg</rs> within the <rs type="programName">funding program</rs> Open Access Publication Funding.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_EfpEyDp">
					<orgName type="project" subtype="full">Next-generation AI for Integrated Diagnostics</orgName>
				</org>
				<org type="funding" xml:id="_6NvvqTq">
					<orgName type="program" subtype="full">funding program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modalities Downstream task</head><p>Alsharid et al. <ref type="bibr" target="#b106">[107]</ref> US video, Transcriptions, Gaze data Captioning AutoRG-Brain <ref type="bibr" target="#b107">[108]</ref> MRI, Reports, Masks Reporting, Grounding BiomedGPT <ref type="bibr" target="#b12">[13]</ref> Medical images, Literature, EHR Reporting, Summarization, Visual QA BioMed-VITAL <ref type="bibr" target="#b108">[109]</ref> Medical images, Instructions Visual QA ChatCAD <ref type="bibr" target="#b109">[110]</ref> X-ray, Reports Reporting CheXagent <ref type="bibr" target="#b110">[111]</ref> X-ray, Reports Classification, Reporting, Grounding COMG <ref type="bibr" target="#b111">[112]</ref> X-ray, Reports, Masks Reporting CT-CHAT <ref type="bibr" target="#b13">[14]</ref> CT, Reports Reporting, Visual QA FFA-GPT <ref type="bibr" target="#b112">[113]</ref> Fundus fluorescein angiography, Reports Reporting, Visual QA GenerateCT <ref type="bibr" target="#b113">[114]</ref> CT, Reports Image generation Huh et al. <ref type="bibr" target="#b114">[115]</ref> X-ray, Reports Reporting LLaVA-Med <ref type="bibr" target="#b14">[15]</ref> Medical images, Captions Visual QA LViT <ref type="bibr" target="#b115">[116]</ref> CT, X-ray, Masks, Text annotations Segmentation M3D-LaMed <ref type="bibr" target="#b116">[117]</ref> CT, Reports, Masks Reporting, Visual QA, Segmentation MAIRA-2 <ref type="bibr" target="#b71">[72]</ref> X-ray, Reports, Masks Reporting, Grounding MAIRA-Seg <ref type="bibr" target="#b117">[118]</ref> X-ray, Reports, Masks Reporting Med-Flamingo <ref type="bibr" target="#b118">[119]</ref> Medical images, Captions Visual QA Med-PaLM M <ref type="bibr" target="#b78">[79]</ref> Medical images, Reports, Genomics Classification, Reporting, Visual QA, Summarization MedVersa <ref type="bibr" target="#b79">[80]</ref> CT, X-ray, Dermatology images, Reports Classification, Reporting, Visual QA, Segmentation MMBERT <ref type="bibr" target="#b119">[120]</ref> Radiology images, Captions Visual QA MVG <ref type="bibr" target="#b120">[121]</ref> Medical images, Text Disease simulation ORacle <ref type="bibr" target="#b15">[16]</ref> Multi-view images, SSG, Descriptions OR scene graph prediction PathChat <ref type="bibr" target="#b121">[122]</ref> Histopathology images, QA-pairs Visual QA PathLDM <ref type="bibr" target="#b122">[123]</ref> Histopathology images, Reports Image generation QUILT-LLaVA <ref type="bibr" target="#b123">[124]</ref> Histopathology images, QA-pairs Visual QA R2GenGPT <ref type="bibr" target="#b124">[125]</ref> X-ray, Reports Reporting RaDialog <ref type="bibr" target="#b72">[73]</ref> X-ray, Reports Reporting, Dialogue RadFM <ref type="bibr" target="#b80">[81]</ref> Medical images, Reports, Descriptions Reporting, Visual QA ReXplain <ref type="bibr" target="#b125">[126]</ref> Video, Reports, Masks Video report generation RGRG <ref type="bibr" target="#b126">[127]</ref> X-ray, Reports, Bounding-boxes Reporting RoentGen <ref type="bibr" target="#b127">[128]</ref> X-ray, Reports Image generation SkinGPT-4 <ref type="bibr" target="#b128">[129]</ref> Dermatology images, Clinical notes Visual QA, Dialogue Surgical-VQLA++ <ref type="bibr" target="#b129">[130]</ref> Surgical images, QA-pairs Visual QA Universal Model <ref type="bibr" target="#b130">[131]</ref> CT, Masks, Descriptions Segmentation Vote-MI <ref type="bibr" target="#b131">[132]</ref> MRI, Reports Visual QA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional information</head><p>Author </p><note type="other">2</note><p>Eligibility criteria 6 Specify characteristics of the sources of evidence used as eligibility criteria (e.g., years considered, language, and publication status), and provide a rationale. IEEE Xplore 893 ("All Metadata":"medic*" OR "All Metadata":"healthcare" OR "All Metadata":"clinic*" OR "All Metadata":"diagnosis*" OR "biomedical") AND ("All Metadata":"language model*" OR "All Metadata":"LLM") NOT ("All Metadata":"ChatGPT") NOT ("All Metadata":"Review") NOT ("All Metadata":"Study") 240</p><p>("All Metadata":"medic*" OR "All Metadata":"healthcare" OR "All Metadata":"clinic*" OR "All Metadata":"diagnosis*" OR "biomedical") AND ("All Metadata":"language" OR "All Metadata":"language model*" OR "LLM") AND ("All Metadata":"multimodal" OR "All Metadata":"multimodal" OR "generalist" OR "CLIP*") NOT ("All Metadata":"ChatGPT") NOT ("All Metadata":"Review") NOT ("All Metadata":"Study")</p><p>Web of Science 1,111 TS=("medic*" OR "healthcare" OR "clinic*" OR "diagnosis*" OR "biomedical") AND TS=("language model*" OR "LLM") NOT TS=("ChatGPT") AND DT=("Article") 425 TS=("medic*" OR "healthcare" OR "clinic*" OR "diagnosis*" OR "biomedical") AND TS=("language" OR "language model*" OR "LLM") AND TS=("multimodal" OR "multi-modal" OR "generalist" OR "CLIP*") NOT TS=("ChatGPT") AND DT=("Article") </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multimodal biomedical ai</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Falcone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Topol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1773" to="1784" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large language models streamline automated machine learning for clinical studies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tayebi Arasteh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lotfinia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Kather</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Truhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nebelung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1603</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Chestxraybert: A pretrained language model for chest radiology report summarization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="845" to="855" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cureus</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with alphafold</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Žídek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="issue">7873</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dogra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adithan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2412.12042" />
		<title level="m">The Impact of AI Assistance on Radiology Reporting: A Pilot Study Using Simulated AI Draft Reports</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adapted large language models can outperform medical experts in clinical text summarization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Van Veen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Uden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Blankemeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Delbrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bluethgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Polacin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seehofnerová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1134" to="1142" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Horng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">317</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Inspect: a multimodal dataset for pulmonary embolism diagnosis and prognosis</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fries</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.10798</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The treasure trove hidden in plain sight: The utility of gpt-4 in chest radiograph evaluation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tayebi Arasteh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Siepmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huppertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lotfinia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Puladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Truhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nebelung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">233441</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multimodal deep learning for integrating chest radiographs and clinical parameters: a case for transformers</title>
		<author>
			<persName><forename type="first">F</forename><surname>Khader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Müller-Franzes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tayebi Arasteh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Haarburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stegmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bressem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nebelung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">309</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">230806</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Frankish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diekhans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jungreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lagarde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Loveland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sisu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="916" to="923" />
			<date type="published" when="2021">2021. 2021</date>
			<publisher>Gencode</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A generalist vision-language foundation model for diverse biomedical tasks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adhikarla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Hamamci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Almas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Simsek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Esirgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Dasdelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wittmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simsar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simsar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.17834</idno>
		<title level="m">A foundation model utilizing chest ct volumes and radiology reports for supervised-level zero-shot detection of abnormalities</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Llava-med: Training a large language-and-vision assistant for biomedicine in one day</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Oracle: Large vision-language models for knowledge-guided holistic or domain modeling</title>
		<author>
			<persName><forename type="first">E</forename><surname>Özsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="455" to="465" />
			<date type="published" when="2024">2024</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.13549</idno>
		<title level="m">A survey on multimodal large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.01319</idno>
		<title level="m">A comprehensive review of multimodal large language models: Performance and challenges across different tasks</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal machine learning in precision health: A scoping review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hutch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">171</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.03264</idno>
		<title level="m">Foundation model for advancing healthcare: Challenges, opportunities, and future directions</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Prisma extension for scoping reviews (prisma-scr): checklist and explanation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tricco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lillie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zarin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Colquhoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Levac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Horsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Weeks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of internal medicine</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="467" to="473" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The prisma 2020 statement: an updated guideline for reporting systematic reviews</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Mckenzie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Bossuyt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Boutron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Mulrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shamseer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tetzlaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Akl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Brennan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bmj</title>
		<imprint>
			<biblScope unit="volume">372</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rayyan-a web and mobile app for systematic reviews</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ouzzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hammady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fedorowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elmagarmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systematic reviews</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Health system-scale language models are all-purpose prediction engines</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Nejatian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nasir-Moin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abidin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Eaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Riina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laufer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Punjabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">619</biblScope>
			<biblScope unit="issue">7969</biblScope>
			<biblScope unit="page" from="357" to="362" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Biobert: a pretrained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Biomistral: A collection of open-source pretrained large language models for medical domains</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Labrak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bazoge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Gourraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rouvier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dufour</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.10373</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Prompt engineering in consistency and reliability with the evidence-based guideline for llms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Rlaif: Scaling reinforcement learning from human feedback with ai feedback</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Phatale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mansoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Carbune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rastogi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.15075</idno>
		<title level="m">Huatuogpt, towards taming language model to be a doctor</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.18925</idno>
		<title level="m">Huatuogpt-o1, towards medical complex reasoning with llms</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9459" to="9474" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Almanac-retrieval-augmented language models for clinical medicine</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zakka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ashley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NEJM AI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2300068</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">RadioRAG: Factual Large Language Models for Enhanced Diagnostics in Radiology Using Online Retrieval Augmented Generation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Arasteh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lotfinia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bressem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Siepmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ferber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Kather</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nebelung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Truhn</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2407.15621" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Augmented non-hallucinating large language models as medical information curators</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Kather</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">100</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bioalbert: A simple and effective pre-trained language model for biomedical named entity recognition</title>
		<author>
			<persName><forename type="first">U</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Razzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Biogpt: generative pre-trained transformer for biomedical text generation and mining</title>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">409</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Drg-llama: tuning llama model to predict diagnosis-related group for hospitalized patients</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dantona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A large language model for electronic health records</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pournejatian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Parisien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Compas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Flores</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ digital medicine</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">194</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deidentification of free-text medical records using pre-trained bidirectional transformers</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bulgarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Health, Inference, and Learning</title>
		<meeting>the ACM Conference on Health, Inference, and Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Optimization of hepatological clinical guidelines interpretation by large language models: a retrieval augmented generation-based framework</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kresevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Giuffrè</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ajcevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Accardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Crocè</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Shung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">102</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Extracting adverse drug events from clinical notes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Mcinnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMIA Summits on Translational Science Proceedings</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page">420</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Enhancing chest x-ray datasets with privacy-preserving large language models and multi-type annotations: a data-driven approach for improved classification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Lanfredi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page">103383</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Med-bert: A pretraining framework for medical records named entity recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5600" to="5608" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oberhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Löser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Truhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Bressem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.08247</idno>
		<title level="m">Medalpaca-an open-source collection of medical conversational ai models and training data</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romanou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Matoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohtashami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.16079</idno>
		<title level="m">Meditron-70b: Scaling medical pretraining for large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Large language models encode clinical knowledge</title>
		<author>
			<persName><forename type="first">K</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tanwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cole-Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pfohl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">620</biblScope>
			<biblScope unit="issue">7972</biblScope>
			<biblScope unit="page" from="172" to="180" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Towards building multilingual language model for medicine</title>
		<author>
			<persName><forename type="first">P</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">8384</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A bert model generates diagnostically relevant semantic embeddings from pathology synopses with active learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Tizhoosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Tayebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications medicine</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pmc-llama: toward building open-source language models for medicine</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="page">45</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Medpodgpt: A multilingual audioaugmented large language model for medical research and education</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Searls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Claus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">H</forename><surname>Jasodanand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Lauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Veerapaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Au</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">medRxiv</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gentili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-N</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Radbert: adapting transformer-based language models to radiology</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">210258</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Generative large language models for detection of speech recognition errors in radiology reports</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Seah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology: Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">230205</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Biophi: A platform for antibody design, humanization, and humanness evaluation based on natural antibody repertoires and deep learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Prihoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fayadat-Dilman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Svozil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Bitton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MAbs</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">2020203</biblScope>
			<date type="published" when="2022">2022</date>
			<pubPlace>Taylor &amp; Francis</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cadd v1. 7: using protein language models, regulatory cnns and other nucleotide-level scores to improve genome-wide variant predictions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schubach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nazaretyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Röner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kircher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="1143" to="1154" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Dnabert: pre-trained bidirectional encoder representations from transformers model for dna-language in genome</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Davuluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="2112" to="2120" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Transfer learning enables predictions in network biology</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Theodoris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">R</forename><surname>Al Sayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mantineo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Brydon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">618</biblScope>
			<biblScope unit="issue">7965</biblScope>
			<biblScope unit="page" from="616" to="624" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Efficient evolution of human antibodies from general protein language models</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Hie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Shanker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">U</forename><surname>Bruun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Weidenbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biotechnology</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="275" to="283" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Msa transformer</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verkuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Rives, A.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8844" to="8856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Large language models generate functional protein sequences across diverse families</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Holton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Olmos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biotechnology</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1099" to="1106" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Protgpt2 is a deep unsupervised language model for protein design</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ferruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Höcker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4348</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Prottrans: Toward understanding the language of life through self-supervised learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rehawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Feher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Angerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="7112" to="7127" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">scbert as a large-scale pretrained deep language model for cell type annotation of single-cell rna-seq data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="852" to="866" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Toxinpred 3.0: An improved method for predicting the toxicity of peptides</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Rathore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tijare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Raghava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page">108926</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Transformer-based structuring of freetext radiology report databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Biesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Layer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Block</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Attenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sprinkart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Radiology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4228" to="4236" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Mimic-iv, a freely accessible electronic health record dataset</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bulgarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gayles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shammout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Horng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Raffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Badawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The eicu collaborative research database, a freely available multi-center database for critical care research</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Meddialog: Large-scale medical dialogue datasets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9241" to="9250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Uniprot: the universal protein knowledgebase in 2023</title>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="523" to="531" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cavanaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Karsch-Mizrachi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ostell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Sayers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="36" to="42" />
			<date type="published" when="2012">2012</date>
			<publisher>Genbank</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The cptac data portal: a resource for cancer proteomics research</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Thangudu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Mcgarvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ketchum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of proteome research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2707" to="2713" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bannur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bouzid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bond-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pérez-García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Salvatelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meissen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.04449</idno>
		<title level="m">Maira-2: Grounded radiology report generation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Özsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keicher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.18681</idno>
		<title level="m">Radialog: A large vision-language model for radiology report generation and conversational assistance</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Expertlevel detection of pathologies from unannotated chest x-ray images via selfsupervised learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Talius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1399" to="1406" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Retrieval-based chest x-ray report generation using a pre-trained contrastive language-image model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Health</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="209" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Blankemeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Veen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J S</forename><surname>Gardezi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paschali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Delbrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Truyts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.06512</idno>
		<title level="m">Merlin: A vision language foundation model for 3d computed tomography</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Visual instruction tuning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Towards generalist biomedical ai</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Driess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schaekermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ktena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NEJM AI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">2300138</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adithan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Topol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.07988</idno>
		<title level="m">A generalist learner for multifaceted medical image interpretation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.02463</idno>
		<title level="m">Towards generalist foundation model for radiology</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">A multimodal biomedical foundation model trained from fifteen million image-text pairs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Preston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Valluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NEJM AI</title>
		<imprint>
			<biblScope unit="page">2400640</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Making the most of text semantics to improve biomedical vision-language processing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Boecking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bannur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hyland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wetscherek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alvarez-Valle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Learning to exploit temporal structure for biomedical vision-language processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bannur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hyland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perez-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boecking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bouzid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="15016" to="15027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Contrastive learning of medical visual representations from paired images and text</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Cplip: Zero-shot learning for histopathology with comprehensive vision-language alignment</title>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename><surname>Ganapathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Dharejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Werghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="11450" to="11459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sellergren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohlberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ktena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kiraly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hormozdiari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaroensri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.03162</idno>
		<title level="m">Advancing multimodal medical capabilities of gemini</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Etp: Learning transferable ecg representations via ecg-text pre-training</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arcucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8230" to="8234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Fairclip: Harnessing fairness in vision-language learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kouhana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Elze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="12289" to="12301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Generalizable whole slide image classification with fine-grained visual-semantic interaction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="11398" to="11407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Flexr: few-shot classification with language embeddings for structured reporting of chest x-rays</title>
		<author>
			<persName><forename type="first">M</forename><surname>Keicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zaripova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Czempiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khakzar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging with Deep Learning</title>
		<imprint>
			<biblScope unit="page" from="1493" to="1508" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Gloria: A multimodal global-local representation learning framework for label-efficient medical image recognition</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3942" to="3951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Knowledge-enhanced visuallanguage pre-training on chest radiology images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4542</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Enhancing representation in radiography-reports foundation model: A granular alignment algorithm using masked contrastive learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7620</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Mcpl: Multi-modal collaborative prompt learning for medical vision-language model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Medimageinsight: An open-source embedding model for general domain medical imaging</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santamaria-Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Guyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sangani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.06542</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A medical multimodal large language model for future pandemics</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">226</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Multi-modal understanding and generation for medical images and text via vision-language pre-training</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6070" to="6080" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Multi-modal molecule structure-text model for text-based retrieval and editing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1447" to="1457" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Mollm: a unified language model for integrating biomedical text with 2d and 3d molecular representations</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Gerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">Supplement 1</biblScope>
			<biblScope unit="page" from="357" to="368" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">A visuallanguage foundation model for pathology image analysis using medical twitter</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yuksekgonul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Montine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2307" to="2316" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A whole-slide foundation model for digital pathology from real-world data</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Unimed-clip: Towards a unified image-text pretraining paradigm for diverse medical imaging modalities</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Khattak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kunhimon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.10372</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Xplainer: From x-ray observations to explainable zero-shot diagnosis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Özsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jiraskova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Braren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Joint spatio-temporal features constrained self-supervised electrocardiogram representation learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Engineering Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="209" to="220" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Ganbased patient information hiding for an ecg authentication system</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Eom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Engineering Letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="197" to="207" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Gaze-assisted automatic captioning of fetal ultrasound videos using threeway multi-modal deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alsharid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Drukker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Papageorghiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">102630</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.16684</idno>
		<title level="m">Autorg-brain: Grounded report generation for brain mri</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.13173</idno>
		<title level="m">Biomedical visual instruction tuning with clinician preference alignment</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Chatcad: Interactive computer-aided diagnosis on medical image using large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.07257</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Delbrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paschali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Blankemeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Veen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M J</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Youssef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Reis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.12208</idno>
		<title level="m">Chexagent: Towards a foundation model for chest x-ray interpretation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Complex organ mask guided radiology report generation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="7995" to="8004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Ffa-gpt: an automated pipeline for fundus fluorescein angiography interpretation and question-answer</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">111</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Hamamci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sekuboyina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simsar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tezcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Simsek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Esirgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Almas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Dasdelen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.16037</idno>
		<title level="m">Generatect: Textconditional generation of 3d chest ct volumes</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Improving medical speech-to-text accuracy with vision-language pre-training model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.00091</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Lvit: language meets vision transformer in medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2404.00578</idno>
		<title level="m">M3d: Advancing 3d medical image analysis with multi-modal large language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Salvatelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srivastav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bouzid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bannur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bond-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Ranjit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Falck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.11362</idno>
		<title level="m">Maira-seg: Enhancing radiology report generation with segmentation-aware multimodal large language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Med-flamingo: a multimodal medical few-shot learner</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zakka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Health (ML4H)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="353" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Mmbert: Multimodal bert pretraining for improved medical vqa</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Khare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bagal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Devi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">D</forename><surname>Priyakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1033" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">Medical video generation for disease progression simulation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-D</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.11943</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">A multimodal generative ai copilot for human pathology</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ikemura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pouli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">634</biblScope>
			<biblScope unit="issue">8033</biblScope>
			<biblScope unit="page" from="466" to="473" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Pathldm: Text conditioned latent diffusion model for histopathology</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yellapragada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prasanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kurc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="5182" to="5191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Quiltllava: Visual instruction tuning by extracting localized narratives from opensource histopathology videos</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Seyfioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">O</forename><surname>Ikezogwo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ghezloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="13183" to="13192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">R2gengpt: Radiology report generation with frozen llms</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Meta-Radiology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">100033</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vairavamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Ter-Oganesyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shilo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.00441</idno>
		<title level="m">Rexplain: Translating radiology into patient-friendly video reports</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Interactive and explainable region-guided radiology report generation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tanida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kaissis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7433" to="7442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">A vision-language foundation model for the generation of realistic chest x-ray images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bluethgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chambon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Delbrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sluijs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Po Lacin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Zambrano Chaves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Chaudhari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Pre-trained multimodal large language model enhances dermatological diagnosis using skingpt-4</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Afvari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5649</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Surgical-vqla++: Adversarial contrastive learning for calibrated robust visual questionlocalized answering in robotic surgery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Seenivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page">102602</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Clip-driven universal model for organ segmentation and tumor detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="21152" to="21164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Enhancing vision-language models for medical imaging: bridging the 3d gap with innovative slice selection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">I</forename><surname>Sair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Loizou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Imami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models for 3d medical image generation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Khader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Müller-Franzes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tayebi Arasteh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Haarburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schulze-Hagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Engelhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baeßler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Foersch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7303</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ciurea-Ilcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chute</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haghgoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shpanskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.02900</idno>
		<title level="m">Medtrinity-25m: A large-scale multimodal dataset with multigranular annotations for medicine</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">A dataset for medical instructional video classification and question answering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Attal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">158</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Omnimedvqa: A new large-scale comprehensive evaluation benchmark for medical lvlm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="22170" to="22183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Padchest: A large chest x-ray image dataset with multi-label annotated reports</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bustos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pertusa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">La</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iglesia-Vaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page">101797</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10286</idno>
		<title level="m">Pathvqa: 30000+ questions for medical visual question answering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.19280</idno>
		<title level="m">Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Quilt-1m: One million image-text pairs for histopathology</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ikezogwo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seyfioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ghezloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sheikh Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Slake: A semanticallylabeled knowledge-enhanced dataset for medical visual question answering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Özsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-M</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="1650" to="1654" />
		</imprint>
	</monogr>
	<note>Rad-restruct: A novel vqa</note>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">A dataset of clinically generated visual questions and answers about radiology images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Amos: A large-scale abdominal multi-organ benchmark for versatile medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhanng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="36722" to="36732" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Bimcv-r: A landmark dataset for 3d ct text-image retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arcucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="124" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">Radgenomechest ct: A grounded vision-language dataset for chest ct analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.16754</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">A machine learning approach to radiogenomics of breast cancer: a study of 922 subjects and 529 dce-mri features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Harowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Ghate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British journal of cancer</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="508" to="516" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Strodthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-D</forename><surname>Bousseljot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kreiseler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">I</forename><surname>Lunze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaeffter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ptb-xl, a large publicly available electrocardiography dataset</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gitter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04611</idno>
		<title level="m">A text-guided protein design framework</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b149">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.15122</idno>
		<title level="m">Rexrank: A public leaderboard for ai-powered radiology report generation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">In-context learning enables multimodal large language models to classify cancer pathology images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ferber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wölflein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Wiest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ligero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ghaffari Laleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">S</forename><surname>El Nahhas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Müller-Franzes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jäger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Truhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10104</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ostmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Blankemeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bluethgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Michalson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Chaudhari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.03595</idno>
		<title level="m">Green: Generative radiology report evaluation and error notation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Ratescore: A metric for radiology report generation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">medRxiv</title>
		<imprint>
			<biblScope unit="page" from="2024" to="2026" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K U N</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S H</forename><surname>Abad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Evaluating progress in automatic chest x-ray radiology report generation. Patterns</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Acl Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation And/or Summarization</title>
		<meeting>the Acl Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation And/or Summarization</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Smit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09167</idno>
		<title level="m">Chexbert: combining automatic labelers and expert annotations for accurate radiology report labeling using bert</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">An evaluation framework for clinical use of large language models in patient interaction tasks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Johri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Schlessinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wongvibulsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Van Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Daneshjou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41591-024-03328-5</idno>
		<ptr target="https://doi.org/10.1038/s41591-024-03328-5" />
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.20613</idno>
		<title level="m">Fineradscore: A radiology report line-by-line evaluation technique generating corrections with severity scores</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Shortcut learning in medical ai hinders generalization: method for estimating ai model generalization without external data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ong Ly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Unnikrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tadic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duhamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Moayedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brudno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">124</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Buensalido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kavnoudias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Abi-Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Ghawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luna</surname></persName>
		</author>
		<title level="m">Rexamine-global: A framework for uncovering inconsistencies in radiology report generation metrics</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="185" to="198" />
		</imprint>
	</monogr>
	<note>Biocomputing 2025: Proceedings of the Pacific Symposium</note>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Ethical considerations of using chatgpt in health care</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Internet Research</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">48009</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Ethics of large language models in medicine and medical research</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Purkayastha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Gichoya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet Digital Health</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="333" to="335" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Paschali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Blankemeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Youssef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bluethgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gatidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chaudhari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.18730</idno>
		<title level="m">Foundation models in radiology: What, how, when, why and why not</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b166">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bluethgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Veen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zakka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Link</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fanous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Daneshjou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Frauenfelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gatidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chaudhari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.01233</idno>
		<title level="m">Best practices for large language models in radiology</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Evaluation and mitigation of the limitations of large language models in clinical decision-making</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jungmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hubrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Knauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vielhauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Makowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Braren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kaissis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2613" to="2622" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Lightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cobbe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.20050</idno>
		<title level="m">Let&apos;s verify step by step</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Tackling prediction uncertainty in machine learning for healthcare</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Gee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="711" to="718" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
