<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EMBODIEDBENCH: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-13">13 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rui</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hanyang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Junyu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Qian</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kangrui</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qineng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Teja</forename><forename type="middle">Venkat</forename><surname>Koripella</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marziyeh</forename><surname>Movahedi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Manling</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">EMBODIEDBENCH: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-13">13 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">58F5852D9F1824040181E27AB5B4063D</idno>
					<idno type="arXiv">arXiv:2502.09560v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.3-SNAPSHOT" ident="GROBID" when="2025-05-13T17:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">0.8.2-3-g65968aec5</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EMBODIEDBENCH, an extensive benchmark designed to evaluate visiondriven embodied agents. EMBODIEDBENCH features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning. Through extensive experiments, we evaluated 13 leading proprietary and open-source MLLMs within EMBODIEDBENCH. Our findings reveal that: MLLMs excel at highlevel tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only 28.9% on average. EMBODIEDBENCH provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLMbased embodied agents. Our code is available at <ref type="url" target="https://embodiedbench.github.io">https://embodiedbench.github.io</ref>.</p><p>To address these questions, we introduce EMBODIED-BENCH, a comprehensive benchmark comprising 1,128 testing instances across four environments. EMBODIEDBENCH is designed with two key features that set it apart from existing benchmarks: 1. Diverse tasks with hierarchical action levels. Among the four environments, EB-ALFRED and EB-Habitat focus on high-level task decomposition and planning (e.g., "put a book on the desk"), while EB-Navigation and EB-Manipulation demand planning with low-level actions</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Developing embodied agents capable of solving complex tasks in real world remains a significant challenge (Durante * Equal contribution 1 University of Illinois Urbana-Champaign 2 Northwestern University. 3 University of Toronto. 4 Toyota Technological Institute at Chicago. † Work done during internship at UIUC. Correspondence to: Rui Yang &lt;ry21@illinois.edu&gt;. <ref type="bibr">et al., 2024)</ref>. Recent advancements in foundation models-including Large Language Models (LLMs) <ref type="bibr" target="#b9">(Brown et al., 2020;</ref><ref type="bibr" target="#b0">Achiam et al., 2023;</ref><ref type="bibr" target="#b76">Touvron et al., 2023;</ref><ref type="bibr">Yang et al., 2024a)</ref> and Multimodal Large Language Models (MLLMs) <ref type="bibr">(OpenAI, 2024a;</ref><ref type="bibr" target="#b56">Reid et al., 2024;</ref><ref type="bibr">Liu et al., 2024a;</ref><ref type="bibr" target="#b78">Wang et al., 2024;</ref><ref type="bibr">Chen et al., 2023c;</ref><ref type="bibr">2025</ref>)-have unlocked unprecedented potential toward this goal. These models, trained on extensive internet-scale datasets, demonstrate exceptional proficiency in understanding human knowledge and performing human-like reasoning. Based on these capabilities, researchers can now design intelligent agents that use off-the-shelf foundation models to solve complex tasks through interaction with environments <ref type="bibr">(Huang et al., 2022a;</ref><ref type="bibr">b;</ref><ref type="bibr">2023c;</ref><ref type="bibr" target="#b2">Ahn et al., 2022;</ref><ref type="bibr" target="#b66">Song et al., 2023;</ref><ref type="bibr">Singh et al., 2023;</ref><ref type="bibr" target="#b30">Liang et al., 2023;</ref><ref type="bibr" target="#b52">Qian et al., 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head><p>Given the multitude of proposed algorithms, there is a pressing need for standardized and automated evaluation frameworks to enable comprehensive assessment and comparison. To address this need, several initiatives have been exploring LLM-based embodied agent evaluation <ref type="bibr">(Liu et al., 2023b;</ref><ref type="bibr">Choi et al., 2024;</ref><ref type="bibr">Li et al., 2024b)</ref>. While these efforts significantly contribute to understanding LLM-based agent design, the evaluation of MLLM embodied agents remains underexplored, posing a challenge for creating more versatile agents. VisualAgentBench <ref type="bibr">(Liu et al., 2024e)</ref> represents the first benchmark for evaluating MLLM agents, covering embodied tasks such as household and Minecraft. However, its limited scope, focusing exclusively on high-level planning, leaves critical questions unanswered, such as the role of vision in embodied tasks and the performance of MLLM agents in low-level tasks like navigation and manipulation.</p><p>Instruction: Pick up the star and place it into the silver container.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tasks with various action levels</head><p>Instruction: Put the books on the desk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-level Household</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EB-ALFRED EB-Habitat</head><p>Low-level Navigation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EB-Navigation</head><p>Low-level Manipulation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EB-Manipulation</head><p>Instruction: Navigate to the laptop and stay close.</p><p>Instruction: Find an orange on the TV stand and move it to the sink.</p><p>Capability-oriented fine-grained evaluation</p><p>Visual Appearance Spatial Awareness Long Horizon Stack the right cylinder on top of the left moon. Obtain a round red fruit and put it in the right counter. Place a cold apple slice on the table. 7 Complex Instruction Common Sense Base Capability I'm feeling thirsty and need a small container to hold water ... Please navigate to that object and stay near it. (cup) While you're tidying up, find a spot on the back of the toilet to put two rolls of toilet paper. It's a great place for convenient access ... Place a plate with a spoon on a counter. 5 8 17</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vision-Driven Embodied Agents Hierarchical Action Representation</head><p>High-level: Low-level: [X, Y, Z, Roll, Pitch, Yaw, Gripper]   [{"action": "find a HandTowel"}, {"action": "pick up the HandTowel"},…] (e.g., translational/rotational control) and require precise perception and spatial reasoning. 2. Capability-oriented evaluation. Unlike previous benchmarks that primarily emphasize overall accuracy <ref type="bibr">(Liu et al., 2023b;</ref><ref type="bibr">Choi et al., 2024;</ref><ref type="bibr">Liu et al., 2024e)</ref> or module-specific performance <ref type="bibr">(Li et al., 2024b)</ref>, EMBODIEDBENCH introduces a fine-grained evaluation framework that assesses six critical capabilities of embodied agents, including basic task solving, commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-horizon planning.</p><p>To facilitate the evaluation of MLLMs as embodied agents, we design a unified agent framework that integrates egocentric visual perception, few-shot in-context examples, interaction history, and environment feedback for decisionmaking. This powerful framework can unlock the full potential of current off-the-shelf MLLMs and tackle both highlevel and low-level tasks effectively. Based on EMBOD-IEDBENCH and our agent pipeline, we evaluate 13 leading closed-source MLLMs (e.g., GPT-4o, Gemini, Claude-3.5) and 7B-90B open-source models (e.g., Llama-3.2 Vision(Meta, 2024), InternVL 2.5 <ref type="bibr" target="#b18">(Chen et al., 2025)</ref>, Qwen2-VL <ref type="bibr" target="#b78">(Wang et al., 2024)</ref>). Our evaluation yields three key findings: (1) While MLLMs excel at high-level tasks, they struggle with low-level manipulation. (2) Long-horizon planning emerges as the most challenging subset. (3) Vision input is crucial for low-level tasks, with performance degrading by 40%-70% when removed, whereas its impact on high-level tasks is minimal. Additionally, our ablation studies provide practical insights into MLLM agent design, particularly regarding image resolution, multi-step image input, and visual in-context learning.</p><p>Our contributions are threefold: (1) proposing a comprehensive benchmark suite for evaluating MLLM-based embodied agents with different action levels and fine-grained capability-oriented subsets, (2) the development of an efficient MLLM agent framework, (3) conducting extensive evaluations and ablation studies of leading MLLMs, providing valuable insights for vision-driven agent design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In embodied agent research, LLMs are primarily used to support high-level planning <ref type="bibr" target="#b2">(Ahn et al., 2022;</ref><ref type="bibr">Huang et al., 2022a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b96">Yao et al., 2023;</ref><ref type="bibr">Huang et al., 2023d;</ref><ref type="bibr" target="#b54">Rana et al., 2023;</ref><ref type="bibr">Chen et al., 2023a;</ref><ref type="bibr">Gao et al., 2024b)</ref>. MLLMs are then integrated for perception-related tasks <ref type="bibr">(Chen et al., 2023b;</ref><ref type="bibr">Wang et al., 2023d;</ref><ref type="bibr">Gao et al., 2024b)</ref>. Beyond perception, MLLMs also contribute to decision-making, either by directly generating actions in an end-to-end manner <ref type="bibr" target="#b64">(Shridhar et al., 2022;</ref><ref type="bibr" target="#b8">Driess et al., 2023;</ref><ref type="bibr">Du et al., 2023;</ref><ref type="bibr" target="#b46">Mu et al., 2024)</ref> or by producing code to develop policy or value functions <ref type="bibr" target="#b30">(Liang et al., 2023;</ref><ref type="bibr">Huang et al., 2023c)</ref>.</p><p>As this field rapidly evolves, a variety of simulators <ref type="bibr" target="#b25">(Kolve et al., 2017;</ref><ref type="bibr">Shridhar et al., 2020a;</ref><ref type="bibr" target="#b85">Xiang et al., 2020;</ref><ref type="bibr" target="#b26">Li et al., 2021;</ref><ref type="bibr" target="#b69">2023)</ref> and evaluation benchmarks <ref type="bibr">(Shridhar et al., 2020b;</ref><ref type="bibr">a;</ref><ref type="bibr">James et al., 2020;</ref><ref type="bibr" target="#b101">Zheng et al., 2022;</ref><ref type="bibr" target="#b71">Szot et al., 2023;</ref><ref type="bibr">Liu et al., 2023b;</ref><ref type="bibr">2024e;</ref><ref type="bibr">Choi et al., 2024;</ref><ref type="bibr">Li et al., 2024b;</ref><ref type="bibr">Zhang et al., 2024a;</ref><ref type="bibr" target="#b20">Cheng et al., 2025)</ref> have emerged. Table <ref type="table">1</ref> provides a comprehensive comparison with existing works, highlighting how EMBODIEDBENCH sets itself apart from prior works in several aspects. More related works are listed in Appendix A.</p><p>Table <ref type="table">1</ref>. Comparison with related benchmarks. EMBODIEDBENCH is a multi-domain benchmark including household, manipulation, and navigation tasks. "Fine-grained" indicates a multi-dimensional evaluation approach rather than an overall accuracy. 1 AgentBench and VisualAgentBench include domains such as household, games, and web. 2 VLABench is originally used for evaluating VLA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benchmark</head><p>Category Action Level #Env. #Test Tasks Multimodal Fine-grained LLM/VLM Support ALFWorld <ref type="bibr">(Shridhar et al., 2020b)</ref> Household High 1 274 × × × Alfred <ref type="bibr">(Shridhar et al., 2020a)</ref> Household High 1 3062 ✓ × × VLMbench <ref type="bibr" target="#b101">(Zheng et al., 2022)</ref> Manipulation Low 1 4760 ✓ × × Behavior-1K <ref type="bibr" target="#b27">(Li et al., 2023)</ref> Household High 1 1000 ✓ × × Language Rearrangement <ref type="bibr" target="#b71">(Szot et al., 2023)</ref> Household High 1 1000 ✓ ✓ × GOAT-bench <ref type="bibr" target="#b22">(Khanna et al., 2024)</ref> Navigation Low 1 3919 ✓ × × AgentBench <ref type="bibr">(Liu et al., 2023b)</ref> Multi-domain 1 High 8 1091 × × ✓ Lota-bench <ref type="bibr">(Choi et al., 2024)</ref> Household High 2 308 × × ✓ VisualAgentBench <ref type="bibr">(Liu et al., 2024e)</ref> Multi-domain 1 High 5 746 ✓ × ✓ Embodied Agent Interface <ref type="bibr">(Li et al., 2024b)</ref> Household High 2 438 × ✓ ✓ VLABench (Zhang et al., 2024a) Manipulation Low 2 1 100 ✓ ✓ ✓ EMBODIEDBENCH (ours) Multi-domain High &amp; Low 4 1128 ✓ ✓ ✓</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Formulation</head><p>Definition of Action Levels. In embodied agent research, actions can be systematically classified into hierarchical levels based on their executability in robotic systems <ref type="bibr">(Ma et al., 2024b;</ref><ref type="bibr" target="#b5">Belkhale et al., 2024)</ref>. Lowlevel actions correspond to atomic commands directly executable by robots, defined as operations that specify translational or rotational displacements. For instance, a robotic arm's action is often parameterized as a 7-dimensional vector: a = [X, Y, Z, Roll, Pitch, Yaw, Gripper], where (X, Y, Z) denote incremental translational displacements, (Roll, Pitch, Yaw) represent rotational deltas in Euler angles, and Gripper encodes the binary open/closed state of the end-effector. Similarly, commands like "move forward 0.1 m" qualify as low-level actions, as they map unambiguously to kinematic transformations. In contrast, high-level actions can be decomposed into sequences of low-level primitives. Formally, a high-level action is defined as</p><formula xml:id="formula_0">a h = [a 1 , a 2 , . . . , a n ],</formula><p>where each a i is a low-level executable primitive. For example, executing "find a HandTowel" might involve iterating through lowlevel behaviors: rotating certain degrees, scanning for the target, and moving towards it.</p><p>Vision-driven Agents. Vision-driven agents are autonomous systems that make sequential decisions based on visual perception and language instructions. This problem can be formally modeled as a Partially Observable Markov Decision Process (POMDP) augmented with language instructions, defined by the tuple (S, A, Ω, T , O, L, R). Here, S is the complete state space unobservable to the agent; A is the space of high-level or low-level actions for the agents; Ω is the visual perception space, where each observation I t ∈ Ω corresponds to an image frame at time t; T is the transition dynamics; O relates the underlying states to the agent's visual observations; L is the language instruction that specifies the desired goal; R evaluates task completion given the language instruction L:</p><formula xml:id="formula_1">r t = 1 if s t |= L (instruction achieved) 0 otherwise . At timestep t,</formula><p>the agent maintains a history h t = (a 0 , I 1 , ..., a t-1 , I t ) and selects actions through a policy π(a t |L, h t ). The objective is to maximize the probability of task success:</p><formula xml:id="formula_2">max π E [r τ ],</formula><p>where τ is the terminal timestep, which occurs when the task succeeds s τ |= L or the maximum horizon is reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EmbodiedBench</head><p>To thoroughly assess MLLMs as embodied agents across various action levels and capabilities, we introduce EM-BODIEDBENCH, a benchmark comprising four environments: EB-ALFRED, EB-Habitat, EB-Navigation, and EB-Manipulation. To evaluate six core embodied agents' capabilities, we developed new datasets and enhanced existing simulators to support comprehensive assessments. Below is an overview of the four benchmark tasks, with further details available in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">High-level and Low-level Tasks</head><p>EB-ALFRED. We develop EB-ALFRED based on the AL-FRED dataset <ref type="bibr">(Shridhar et al., 2020a)</ref> and the AI2-THOR simulator <ref type="bibr" target="#b25">(Kolve et al., 2017)</ref>. Our simulator is based on Lota-Bench's implementation <ref type="bibr">(Choi et al., 2024)</ref> for 8 highlevel skill types: "pick up", "open", "close", "turn on", "turn off", "slice", "put down", and "find", each customizable with specific objects, for example, "find an apple". The simulator provides an egocentric view as observation, along with textual feedback on action validity and possible failure reasons. Despite its strengths, Lota-Bench's simulator has several limitations, which we outline in Appendix B.1. To enhance the simulation, we introduced key improvements, such as support for multiple instances of the same object type, allowing us to cover all task types in ALFRED. Additionally, we streamlined the action space by merging "put down" actions into a single action, since only one object can be held at a time. Due to the varying number of objects in ALFRED, the action space of EB-ALFRED is dynamic,</p><p>Embodied Environments EB-ALFRED EB-Habitat EB-Navigation EB-Manipulation Visual Perception Skill Sets ["Move forward by 0.25", "Move leftward by 0.25", "Rotate to right by 90 degrees","Rotate to left by 90 degrees..."]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Information</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task-Specific Information</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Updated Visual Perception Interaction History</head><p>Output Action History Environment Feedback 1. "Last action executed successfully." 2. "At this moment, you have completed executing 2 steps. Last action is invalid. The current reward obtained is 0.0 ..." {"action": " <ref type="bibr">[57,</ref><ref type="bibr">61,</ref><ref type="bibr">30,</ref><ref type="bibr">0,</ref><ref type="bibr">60,</ref><ref type="bibr">25</ref>, 1]"}, {"action": "[57, 61, 27, 0, 60, 25, 0]"}, {"action": ...}, ...]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In-context Demonstrations with Reasoning</head><p>Example 1: Human Instruction: Stack the red star and the olive star in sequence. Input: {'object 1': <ref type="bibr">[42, 24, 19], 'object 2': [54, 53, 19]</ref>,...} Output: {"visual_state_description": "From left to right, I can see a purple star at <ref type="bibr">[42,</ref><ref type="bibr">24,</ref><ref type="bibr">19]</ref>, an olive star at <ref type="bibr">[54,</ref><ref type="bibr">53,</ref><ref type="bibr">19</ref>]...", "reasoning_and_reflection": "I need to understand the instruction first. To stack the objects in sequence, the red star should be at the bottom and the olive star should be placed on top. The plan involves moving the gripper to...", "executable_plan": " <ref type="bibr">[[54, 51, 28, 3, 66, 37, 1]</ref>, <ref type="bibr">[54,</ref><ref type="bibr">54,</ref><ref type="bibr">20,</ref><ref type="bibr">3,</ref><ref type="bibr">66,</ref><ref type="bibr">37,</ref><ref type="bibr">0]</ref>...]" Example 2: ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction History</head><p>Output Action History Environment Feedback "Stack the lime moon and the gray moon in sequence."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Instruction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optional Information</head><p>Detection Box Object Position</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLLM-based Task Planner</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual State Description</head><p>From left to right, I can see a lime moon at <ref type="bibr">[82,</ref><ref type="bibr">17,</ref><ref type="bibr">19]</ref>, a blue moon at <ref type="bibr">[57,</ref><ref type="bibr">61,</ref><ref type="bibr">27]</ref>, and a green moon at <ref type="bibr">[52,</ref><ref type="bibr">65,</ref><ref type="bibr">19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Plan Executable Plan Reasoning</head><p>The task is to ... The lime moon at <ref type="bibr">[82,</ref><ref type="bibr">17,</ref><ref type="bibr">19</ref>] should be at the bottom, and the gray moon at <ref type="bibr">[57,</ref><ref type="bibr">61,</ref><ref type="bibr">27]</ref> should be placed on top.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reflection</head><p>The previous actions indicate some invalid paths, so I need to ensure the gripper moves correctly ... 1. Move to the gray moon's position. ... 5. Open the gripper to release the gray moon onto the lime moon.</p><p>[{"action": "[57, 61, 30, 0, 60, 25, 0]"}, {"action": " <ref type="bibr">[82,</ref><ref type="bibr">17,</ref><ref type="bibr">30,</ref><ref type="bibr">0,</ref><ref type="bibr">60,</ref><ref type="bibr">25</ref>, 0]"}, {"action": " <ref type="bibr">[82,</ref><ref type="bibr">17,</ref><ref type="bibr">26,</ref><ref type="bibr">0,</ref><ref type="bibr">60,</ref><ref type="bibr">25,</ref><ref type="bibr">1]</ref>"}] 1 2 3 4 5</p><p>Figure <ref type="figure">2</ref>. The vision-driven agent pipeline used in EMBODIEDBENCH. This pipeline serves as a robust framework for processing multimodal inputs, reflection and reasoning, and generating executable plans. For detailed descriptions, refer to Section 4.3.</p><p>ranging from 171 to 298 actions. Furthermore, we manually corrected simulator errors and refined instruction quality, ensuring more accurate action execution and improved task solvability. These enhancements make EB-ALFRED a highquality benchmark for evaluating embodied agents.</p><p>EB-Habitat. EB-Habitat is built upon the Language Rearrangement benchmark <ref type="bibr" target="#b71">(Szot et al., 2023)</ref>, featuring 282 diverse language instruction templates. It leverages the Habitat 2.0 simulator <ref type="bibr" target="#b70">(Szot et al., 2021)</ref> and focuses on planning and executing 70 high-level skills to achieve user-defined goals. These skills fall into five categories: "navigation", "pick", "place", "open", and "close", with each skill parameterized by a set of objects. Unlike ALFRED, which permits navigation to any object, EB-Habitat restricts navigation to receptacle-type objects, requiring robots to visit multiple locations to find desired items. With its wide variety of language instructions and unique navigation constraints, EB-Habitat serves as a valuable complement to EB-ALFRED.</p><p>EB-Navigation. EB-Navigation is an evaluation suite based on AI2-THOR <ref type="bibr" target="#b25">(Kolve et al., 2017)</ref>, designed to assess embodied agents' navigation abilities. Each unique navigation task is primarily defined by: (1) initial Robot Pose, (2) target object information, and (3) language instruction that specifies which target object to locate, such as "navigate to the laptop". The robot can only rely on visual observations and textual feedback, without direct positioning data, to navigate to the target object. Success is defined as reaching within a specified distance of the target. The action space includes EB-Manipulation. EB-Manipulation extends VLMBench <ref type="bibr" target="#b101">(Zheng et al., 2022)</ref> to evaluate MLLM-based embodied agents in low-level object manipulation. The agent controls a robotic arm using a 7-dimensional action vector, specifying movement parameters. Direct low-level manipulation is challenging for MLLMs. To overcome this challenge, we implemented enhancements, as illustrated in Figure <ref type="figure">2:</ref> (1) action space discretization <ref type="bibr" target="#b97">(Yin et al., 2024)</ref>, which divides the position components (x, y, z) into 100 bins and the orientation components (roll, pitch, yaw) into 120 bins, enabling valid actions to take forms like <ref type="bibr">[x, y, z, roll, pitch, yaw, gripper]</ref> = <ref type="bibr">[57,</ref><ref type="bibr">61,</ref><ref type="bibr">20,</ref><ref type="bibr">10,</ref><ref type="bibr">60,</ref><ref type="bibr">25,</ref><ref type="bibr">1]</ref>; and (2) additional information like YOLO <ref type="bibr" target="#b55">(Redmon, 2016)</ref> detection boxes with index markers <ref type="bibr">(Yang et al., 2023a)</ref> and object pose estimation for indexed objects, reducing the need for precise 3D location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Capability-oriented Data Collection</head><p>We aim to collect capability-oriented data for our four environments. To accomplish this, we have identified six capability categories, as outlined in</p><p>Table 5: (1) The Base subset High-level Planning Trajectory Low-level Planning Trajectory Instruction: Stack the right moon on top of the left star.</p><p>Step 1</p><p>Step 2</p><p>Step 3</p><p>Step 4</p><p>Step 5</p><p>Step 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Replanning Success</head><p>Step 1</p><p>Step 7</p><p>Step 11</p><p>Step 14</p><p>Step 16</p><p>Step 17</p><p>Step 20</p><p>Instruction: Place a sauce pan with a spatula on the kitchen counter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Planning Success</head><p>Fail Planning evaluates basic task-solving skills necessary for planning action sequences across tasks of low to medium difficulty.</p><p>(2) The Common Sense subset focuses on the use of common sense knowledge to indirectly refer to objects, such as describing a refrigerator as "a receptacle that can keep food fresh for several days." This subset evaluates the ability of embodied agents to reason using common sense. (3) The Complex Instruction subset includes relatively longer contexts, which can be relevant or irrelevant, to obscure the instruction. This measures an agent's ability to discern user intent from a long context. (4) The Spatial Awareness subset refers to objects by their location relative to other objects. (5) The Visual Appearance subset involves referring to objects based on their visual attributes, such as color or shape. ( <ref type="formula">6</ref>) The Long Horizon subset comprises tasks requiring extended action sequences, typically more than 15 steps in EB-ALFRED. These subsets cover a broad range of scenarios, enabling a fine-grained evaluation of embodied agents' capabilities.</p><p>To construct a diverse dataset, we employ different data collection strategies. For EB-ALFRED and EB-Manipulation, data was gathered through a combination of manual annotation and instruction augmentation using <ref type="bibr">GPT-4o (OpenAI, 2024a)</ref>. For EB-Habitat, we reorganized and adapted an existing dataset from <ref type="bibr" target="#b71">(Szot et al., 2023)</ref>, aligning it with our specific objectives. Differently, data for EB-Navigation was generated entirely through automated Python programs. In summary, EB-ALFRED and EB-Habitat each include 300 test instances, with 50 instances for 6 subsets. Due to design challenges, EB-Navigation omits the spatial awareness subset and EB-Manipulation excludes the long-horizon subset. EB-Navigation consists of 300 test cases distributed across 5 subsets (60 instances each), while EB-Manipulation contains a total of 228 instances, with 48 instances for each subset except visual appearance, which includes 36 instances. Detailed data collection is provided in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Vision-driven Agent Design</head><p>To evaluate MLLMs as agents in EMBODIEDBENCH, we design a unified embodied agent pipeline, illustrated in Figure <ref type="figure">2</ref>. This pipeline provides a robust framework for processing multimodal inputs, reasoning through interactions, and gen-erating structured, executable plans composed of sequential actions. Two planning examples are provided in Figure <ref type="figure" target="#fig_2">3</ref>, with additional examples available in Appendix H. Below, we outline the key components of our agent design.</p><p>Agent Input: The agent processes a variety of inputs, including language instructions, visual perceptions, in-context demonstrations, interaction history, and task-specific information. For visual perception, the agent can utilize either the current step image or a sequence of historical images within a sliding window. However, we observe that current MLLMs struggle to understand multiple historical images effectively, so we primarily rely on the current step image for efficiency. Task-specific information varies by task type. For high-level tasks and EB-Navigation, the agent requires valid skill sets, while EB-Manipulation tasks include descriptions of the action format. Additionally, EB-Manipulation incorporates detection boxes with visual markers and object positions to help MLLMs accurately identify 3D locations. Further examples of input prompts are provided in Appendix G.</p><p>Task Planner: At each planning step, the agent: (1) generates a textual description of the current visual input; (2) reflects on past actions and environmental feedback; (3) reasons about how to achieve the goal using available information; (4) formulates a language-based plan; and (5) converts it into an executable plan in the required format. All outputs are structured in JSON. Unlike prior work planning one action per timestep <ref type="bibr">(Liu et al., 2024e)</ref>, we support multi-step planning, allowing the agent to dynamically decide the number of actions needed. It offers two advantages:</p><p>(1) better alignment with in-context examples for sequential decision-making, and (2) reduced plan redundancy, especially in low-level tasks where single action causes limited changes in images, thereby minimizing MLLM API calls. If a plan fails or triggers an invalid action, the agent restarts planning from the latest state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we conduct comprehensive experiments to evaluate the performance of various MLLMs in EMBODIED-BENCH, followed by ablation studies in Sections 5.3 and 5.4 and error analysis in Section 5.5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setups</head><p>We benchmark 13 models, including leading proprietary models (GPT-4o / 4o-mini (OpenAI, 2024a;b), Claude-3.5-Sonnet <ref type="bibr" target="#b4">(Anthropic, 2024)</ref>, Gemini Pro / Flash <ref type="bibr" target="#b73">(Team et al., 2023;</ref><ref type="bibr">2024a;</ref><ref type="bibr">DeepMind, 2024)</ref>, and SOTA open-source models (LLaMA3.2 11B / 90B Vision Instruct (Meta, 2024), InternVL 2.5 8B / 38B / 78B <ref type="bibr" target="#b18">(Chen et al., 2025)</ref>, Qwen2-VL 7B / 72B <ref type="bibr" target="#b78">(Wang et al., 2024)</ref>). For consistency, all models are set with a temperature of 0 and a maximum completion token length of 2048. All images are standardized to a resolution of 500×500 pixels. The maximum number of environment steps is 30 for high-level tasks, 20 for EB-Navigation, and 15 for EB-Manipulation. We use the task success rate as the primary metric in our main experiments. More results and ablations are deferred to Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Benchmark Results</head><p>Overall Results. Tables <ref type="table" target="#tab_5">2</ref> and <ref type="table" target="#tab_7">3</ref> summarize the results for high-level and low-level tasks, respectively. Overall, current MLLMs demonstrate strong performance on highlevel tasks but struggle with low-level tasks, especially EB-Manipulation. Among proprietary models, we observe that different models excel at different task levels: Claude-3.5-Sonnet achieves the highest average accuracy on high-level tasks, with 64.0% on EB-ALFRED and 68.0% on EB-Habitat, while GPT-4o leads in low-level tasks, scoring 57.7% on EB-Navigation and 28.9% on EB-Manipulation. Gemini-1.5-Pro performs the worst among the three large proprietary models, but Gemini-1.5 / 2.0-Flash outperforms GPT-4o-mini by a large margin. For open-source models, InternVL2 5 model family exhibits the best overall performance, with its largest 78B version outperforming Llama-3.2-90B-Vision-Ins and Qwen2-VL-72B-Ins across all 4 environments. Additionally, open-source models exhibit a clear scaling effect, as their performance improves with increasing model parameters. Moreover, although large opensource models are closing the gap with smaller proprietary models like GPT-4o-mini, a notable performance difference remains between large proprietary and open-source models.</p><p>The Role of Vision in Embodied Agent. By comparing the performance of embodied agents with and without visual information (marked as "Lang") in Tables <ref type="table" target="#tab_5">2</ref> and <ref type="table" target="#tab_7">3</ref>, we observe a clear distinction between low-level and high-level tasks. Low-level tasks show a much stronger reliance on vision compared to high-level tasks. For example, disabling vision causes GPT-4o's EB-Navigation performance to drop sharply from 57.7% to 17.4%, with long-horizon planning completely collapsing to 0%. This sharp decline highlights the critical importance of visual signals for lowlevel control tasks. Conversely, high-level tasks show much less dependence on visual input. GPT-4o (Lang) and GPT-4o-mini (Lang) perform on par with or even outperform their vision-enabled counterparts in EB-ALFRED and EB-Habitat, suggesting that these tasks may rely more heavily on textual information rather than visual input. We will further investigate the impact of language-centric factors in Section 5.3. These findings emphasize two key insights: (1) when designing MLLM-based embodied AI benchmarks, it is essential to consider action-level taxonomy, with greater attention to low-level action tasks, and (2) more advanced methods are needed to effectively leverage visual input for high-level embodied tasks.</p><p>Fine-grained Results across Subsets. We have the following findings based on our evaluation across 6 subsets.</p><p>(1) Performance Varies across Different Subsets. We observe that models perform differently across various subsets.</p><p>For instance, while Claude-3.5-Sonnet is the best model on EB-Habitat overall, GPT-4o surpasses it on long-horizon subsets (64% vs. 58%), indicating GPT-4o's stronger ability in long-horizon planning. This trend is even more pronounced in low-level tasks. For example, Gemini-1.5-Pro scores 10 points higher than GPT-4o on the spatial awareness subset but lags significantly in other capabilities. These results highlight the importance of fine-grained evaluations to uncover nuanced limitations in current models.</p><p>(2) Long-Horizon Planning Is the Most Challenging Task.</p><p>The long-horizon subset consistently proves to be the most difficult, showing the largest performance gap compared to base scores. For instance, in EB-Habitat, Claude-3.5-Sonnet achieves 96% on the base subset but drops to 58% on the long-horizon subset. Similarly, GPT-4o falls from 86% to 64%. This trend holds true across both high-level and lowlevel tasks, suggesting that long-horizon planning remains a significant bottleneck for current MLLM-based agents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Language-centric Ablation</head><p>We explore the role of the language-centric components, specifically focusing on environment feedback and the number of in-context examples. Comparisons are conducted using the base subset of EB-ALFRED. Our findings in Figure <ref type="figure" target="#fig_3">4</ref> reveal that removing environment feedback-which provides critical information during interaction-causes a 10% drop in success rate for GPT-4o and an 8% drop for Claude-3.5-Sonnet. Furthermore, while our experiments use 10 in-context examples by default, reducing this number significantly affects performance. In a 0-shot setting, the success rate drops to around 40%. Comparing the results in Table <ref type="table" target="#tab_5">2</ref> and <ref type="table" target="#tab_7">3</ref>, these results emphasize that high-level tasks depend more on textual information than on visual input. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Visual-centric Ablation</head><p>Visual information is critical for the performance of lowlevel tasks. In this section, we thoroughly analyze the impact of four factors or potential enhancements: camera resolution, detection boxes, multi-step images, and visual in-context learning. All comparisons are based on the base subset of EB-Manipulation. Additional ablation results can be found in Appendix E.</p><p>Camera Resolutions. We investigate the effect of three camera resolutions on task performance. Our results, shown in Figure <ref type="figure">5</ref> (a), indicate that mid-range resolutions (500 × 500) achieve better results compared to both lower (300 × 300) and higher (700 × 700) resolutions. While lowresolution images may lack fine-grained details necessary for task execution, excessively high resolutions can introduce unnecessary complexity, making it harder for MLLMs to focus on relevant information for decision-making. These results highlight the importance of selecting an appropriate resolution when deploying MLLM-based embodied agents.</p><p>Detection Boxes. In EB-Manipulation, detection boxes and visual markers are used to align language instructions with visual information, helping to localize key objects in the scene. Figure <ref type="figure">5</ref> (b) shows that removing detection boxes reduces success rates from 39.6% to 27.1% for GPT-4o and from 37.5% to 29.2% for Claude-3.5-Sonnet, emphasizing their important role in object localization for low-level tasks.</p><p>Multi-step Image Input. We also explore whether incorporating multi-step historical observations can enhance performance in our agent framework, as they may help address partial observability. For EB-Manipulation, we include observations from the past two steps in addition to the current perception error 4% reasoning error 41% planning error 55% wrong recog. 3% insufficient exploration 12% wrong termination decision 13% reflection error 17% inaccurate action 9% missing step 23% action id mismatch 1% invalid action 22% (a) EB-ALFRED perception error 33% reasoning error 23% planning error 44% hallucination 4% wrong recog. 22% spatial understanding 8% spatial reasoning 10% reflection error 13% inaccurate action 42% invalid action 1% (b) EB-Manipulation Figure 6. Error Analysis.</p><p>Sonnet achieves a 16.7% performance boost. These findings underscore the potential of visual ICL as a promising avenue for future research in embodied agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Error Analysis</head><p>We conducted an error analysis on GPT-4o to identify potential failure modes in EB-ALFRED and EB-Manipulation.</p><p>For each environment, we sample 10 failure episodes from each subset, resulting in a total of 110 failed episodes to be analyzed. We found three main types of errors: perception errors, reasoning errors, and planning errors. Each error category corresponds to a specific stage in our agent pipeline, with definitions of sub-errors provided in Appendix F.</p><p>Overall, planning errors are the most common issue in both environments, while perception errors are more prevalent in low-level tasks. In EB-ALFRED, planning errors (55%) and reasoning errors (41%) dominate, while only 4% of errors are perception errors. Among planning errors, missing steps (23%) and invalid actions (22%) are the most common issues, highlighting challenges in generating complete and valid plans. Reflection errors (17%) suggest the model often fails to recognize planning mistakes in its action history. Another common failure is wrong termination errors (13%), where the model prematurely assumes the task is complete and stops too early. For EB-Manipulation, planning errors remain the primary cause of failure (44%), due to inaccurate actions, indicating difficulties in estimating precise gripper poses. Perception errors make up 33% of failures, with wrong recognition errors (22%) being the most frequent. These errors show that even with detection boxes annotated in the visual input, the model still fails to recognize object attributes correctly. This highlights considerable room for improvement in the visual capabilities of GPT-4o.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conlcusion</head><p>We introduce EMBODIEDBENCH, a comprehensive evaluation framework designed to assess MLLM-based embodied agents across tasks with varying action levels and capabilityoriented subsets.</p><p>Through extensive experiments, we identified key challenges, including difficulties in low-level manipulation and long-horizon planning, and the varying significance of vision input across tasks. By highlighting these areas for improvement, we hope EMBODIEDBENCH will inspire and guide future research, driving the development of more capable and versatile vision-driven embodied agents. Future Research Directions. While EMBODIEDBENCH represents a significant step forward in evaluating MLLMbased embodied agents, several challenges remain, offering rich opportunities for future research. Below, we outline potential research directions: • Expanding Task Diversity. Current benchmarks for MLLM-based embodied agents are still limited in task diversity. Future research could explore more realistic and complex environments with different action levels, such as autonomous driving (Gulino et al., 2024; Ma <ref type="bibr">et al., 2024a;</ref><ref type="bibr">Gao et al., 2024a)</ref>, multi-agent collaboration <ref type="bibr">(Liu et al., 2024d)</ref>, and human-agent interaction <ref type="bibr" target="#b11">(Chang et al., 2024)</ref>. These scenarios would better assess the agents' adaptability and generalization capabilities in real-world settings.</p><p>• Low-Level Tasks and Spatial Reasoning. Our findings show that current MLLM-based agents struggle with spatial reasoning and low-level control. Future research could improve these capabilities by better integrating spatial reasoning with low-level action planning, including 3D visual grounding <ref type="bibr">(Chen et al., 2024a;</ref><ref type="bibr" target="#b19">Cheng et al., 2024)</ref> and alignment <ref type="bibr" target="#b2">(Ahn et al., 2022;</ref><ref type="bibr">Yang et al., 2024d)</ref>.</p><p>• Long-Horizon Planning. Long-horizon planning is still challenging for embodied agents. Future research can study techniques like hierarchical planning <ref type="bibr" target="#b66">(Song et al., 2023;</ref><ref type="bibr" target="#b3">Ajay et al., 2023)</ref>, memory-augmented methods <ref type="bibr">(Sarch et al., 2024a)</ref>, and world models <ref type="bibr" target="#b44">(Mazzaglia et al., 2024)</ref> to enhance their ability to plan and execute complex, multi-step tasks more effectively.</p><p>• Multi-step/Multi-view Image Understanding. Our experiments show that current MLLMs struggle with multi-step and multi-view image inputs. Future research could improve multi-frame and multi-view comprehension, temporal reasoning, and spatial awareness to enhance MLLM agents' visual perception and reasoning. One promising direction is leveraging video pretraining <ref type="bibr" target="#b42">(Madan et al., 2024;</ref><ref type="bibr" target="#b78">Wang et al., 2024)</ref> to better equip embodied agents for these challenges.</p><p>• Visual In-context Learning (ICL). Our experiments confirm the effectiveness of visual ICL <ref type="bibr">(Zhou et al., 2024b;</ref><ref type="bibr">Sarch et al., 2024b)</ref> in embodied decision-making. This approach is promising because it enables adaptability and versatility without fine-tuning, allowing better use of off-the-shelf MLLMs. However, designing more effective visual ICL methods for embodied tasks remains an open problem for future research.</p><p>• Training Multimodal Embodied Agents. While our work focuses on evaluation, fine-tuning MLLMs for embodied tasks could significantly enhance their performance <ref type="bibr" target="#b46">(Mu et al., 2024;</ref><ref type="bibr" target="#b72">Szot et al., 2024;</ref><ref type="bibr" target="#b98">Zawalski et al., 2024)</ref>. Future research can explore embodied pretraining, imitation learning, and both offline and online reinforcement learning <ref type="bibr" target="#b69">(Sun, 2023)</ref> to better optimize MLLMs for embodied decision-making. Additionally, developing end-to-end learning approaches that seamlessly integrate perception, reasoning, and action could reduce the need for designing complex agent frameworks, leading to more adaptive and generalizable agents.</p><p>• Robustness and Generalization of MLLM Agents. Ensuring real-world applicability requires a thorough study of MLLM agents' robustness and generalization capabilities. While related studies are emerging in other domains <ref type="bibr" target="#b104">(Zou et al., 2024;</ref><ref type="bibr" target="#b89">Xu et al., 2024;</ref><ref type="bibr">Yang et al., 2023b;</ref><ref type="bibr">2024b;</ref><ref type="bibr">Zhang et al., 2024b)</ref>, research on MLLM agents remains limited. Potential methods involve incorporating adversarial settings <ref type="bibr">(Liu et al., 2024b;</ref><ref type="bibr">Wu et al.)</ref>, dynamically generated environments <ref type="bibr">(Wang et al., 2023c)</ref>, or domain shifts <ref type="bibr" target="#b12">(Chattopadhyay et al., 2021)</ref> to assess and enhance the ability of embodied agents to perform reliably in varying conditions.</p><p>By exploring these directions, the field can move closer to realizing the full potential of MLLM-based embodied agents in real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact Statement</head><p>This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Journal of</head><p>Robotics Research, pp. 02783649241273668, 2023. Choi, J.-W., Yoon, Y., Ong, H., Kim, J., and Jang, M. Lotabench: Benchmarking language-oriented task planners for embodied agents. arXiv preprint arXiv:2402.08178, 2024. Contributors, L. Lmdeploy: A toolkit for compressing, deploying, and serving llm. <ref type="url" target="https://github.com/InternLM/lmdeploy">https://github.com/  InternLM/lmdeploy</ref>, 2023. DeepMind, G. Introducing gemini 2.0: our new ai model for the agentic era, 2024. URL <ref type="url" target="https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/">https://blog.google/  technology/google-deepmind/google- gemini-ai-update-december-2024/</ref>. Driess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., et al. Palm-e: an embodied multimodal language model. In Proceedings of the 40th International Conference on Machine Learning, pp. 8469-8488, 2023. Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang, D., Yuan, E., Lu, E., Tang, F., Sung, F., Wei, G., Lai, G., Guo, H., Zhu, H., et al. Kimi k1.5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Du, Y., Yang, M., Florence, P., Xia, F., Wahid, A., Ichter, B., Sermanet, P., Yu, T., Abbeel, P., Tenenbaum, J. B., et al. Video language planning. arXiv preprint arXiv:2310.10625, 2023. Durante, Z., Huang, Q., Wake, N., Gong, R., Park, J. S., Sarkar, B., Taori, R., Noda, Y., Terzopoulos, D., Choi, Y., et al. Agent ai: Surveying the horizons of multimodal interaction. arXiv preprint arXiv:2401.03568, 2024. Gao, C., Zhao, B., Zhang, W., Mao, J., Zhang, J., Zheng, Z., Man, F., Fang, J., Zhou, Z., Cui, J., et al. Embodiedcity: A benchmark platform for embodied agent in real-world city environment. arXiv preprint arXiv:2410.09604, 2024a. Gao, J., Sarkar, B., Xia, F., Xiao, T., Wu, J., Ichter, B., Majumdar, A., and Sadigh, D. Physically grounded visionlanguage models for robotic manipulation. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 12462-12469. IEEE, 2024b. Gu, Q., Kuwajerwala, A., Morin, S., Jatavallabhula, K. M., Sen, B., Agarwal, A., Rivera, C., Paul, W., Ellis, K., Chellappa, R., et al. Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 5021-5028. IEEE, 2024. Gulino, C., Fu, J., Luo, W., Tucker, G., Bronstein, E., Lu, Y., Harb, J., Pan, X., Wang, Y., Chen, X., et al. Waymax: An accelerated, data-driven simulator for large-scale autonomous driving research. Advances in Neural Information Processing Systems, 36, 2024. Huang, H., Lin, F., Hu, Y., Wang, S., and Gao, Y. Copa: General robotic manipulation through spatial constraints of parts with foundation models. arXiv preprint arXiv:2403.08248, 2024a. Huang, J., Yong, S., Ma, X., Linghu, X., Li, P., Wang, Y., Li, Q., Zhu, S.-C., Jia, B., and Huang, S. An embodied generalist agent in 3d world. arXiv preprint arXiv:2311.12871, 2023a. Huang, S., Jiang, Z., Dong, H., Qiao, Y., Gao, P., and Li, H. Instruct2act: Mapping multi-modality instructions to robotic actions with large language model. arXiv preprint arXiv:2305.11176, 2023b. Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pp. 9118-9147. PMLR, 2022a. Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022b. Huang, W., Wang, C., Zhang, R., Li, Y., Wu, J., and Fei-Fei, L. Voxposer: Composable 3d value maps for robotic manipulation with language models. arXiv preprint arXiv:2307.05973, 2023c. Huang, W., Xia, F., Shah, D., Driess, D., Zeng, A., Lu, Y., Florence, P., Mordatch, I., Levine, S., Hausman, K., et al. Grounded decoding: Guiding text generation with grounded models for robot control. arXiv preprint arXiv:2303.00855, 2023d. Huang, W., Wang, C., Li, Y., Zhang, R., and Fei-Fei, L. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. arXiv preprint arXiv:2409.01652, 2024b. James, S., Ma, Z., Arrojo, D. R., and Davison, A. J. Rlbench: The robot learning benchmark &amp; learning environment. IEEE Robotics and Automation Letters, 5(2):3019-3026, 2020. Jiang, H., Huang, B., Wu, R., Li, Z., Garg, S., Nayyeri, H., Wang, S., and Li, Y. Roboexp: Action-conditioned scene graph via interactive exploration for robotic manipulation. arXiv preprint arXiv:2402.15487, 2024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Related Works</head><p>Foundation models <ref type="bibr" target="#b6">(Bommasani et al., 2021)</ref>, particularly Large Language Models (LLMs) <ref type="bibr" target="#b9">(Brown et al., 2020;</ref><ref type="bibr" target="#b0">Achiam et al., 2023;</ref><ref type="bibr" target="#b76">Touvron et al., 2023;</ref><ref type="bibr">Yang et al., 2024a;</ref><ref type="bibr">c)</ref> and Multi-Modal Large Language Models (MLLMs) <ref type="bibr" target="#b53">(Radford et al., 2021;</ref><ref type="bibr">Team et al., 2024a;</ref><ref type="bibr" target="#b78">Wang et al., 2024;</ref><ref type="bibr" target="#b84">Wu et al., 2024;</ref><ref type="bibr">Du et al., 2025;</ref><ref type="bibr">Chen et al., 2024b;</ref><ref type="bibr" target="#b88">Xie et al., 2024)</ref>, fundamentally transform how embodied agents perceive, make decisions, and act in physical and simulated environments.</p><p>The integration of these models into embodied agents evolves through several key approaches. Initially, Large Language Models (LLMs) are introduced to assist with high-level planning <ref type="bibr" target="#b2">(Ahn et al., 2022;</ref><ref type="bibr">Huang et al., 2022a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b54">Rana et al., 2023;</ref><ref type="bibr">Gao et al., 2024b;</ref><ref type="bibr">Huang et al., 2023d;</ref><ref type="bibr">Wang et al., 2023a;</ref><ref type="bibr">Huang et al., 2023b;</ref><ref type="bibr">Liu et al., 2023a;</ref><ref type="bibr">Wang et al., 2023b;</ref><ref type="bibr">Chen et al., 2023a;</ref><ref type="bibr">Huang et al., 2023a;</ref><ref type="bibr">Zhou et al., 2024a)</ref>. They are also adopted for low-level controls <ref type="bibr" target="#b43">(Mao et al., 2023;</ref><ref type="bibr" target="#b97">Yin et al., 2024)</ref>. MLLMs are then incorporated for perception tasks such as object attribute identification, visual relation extraction, and action recognition <ref type="bibr" target="#b87">(Xiao et al., 2022;</ref><ref type="bibr">Chen et al., 2023b;</ref><ref type="bibr">Wang et al., 2023d;</ref><ref type="bibr">e;</ref><ref type="bibr">Gao et al., 2024b;</ref><ref type="bibr">Gu et al., 2024)</ref>. Subsequently, the role of MLLMs extends into policy-making through various approaches. Some works implement MLLMs in an end-to-end manner for direct action generation <ref type="bibr" target="#b64">(Shridhar et al., 2022;</ref><ref type="bibr" target="#b8">Driess et al., 2023;</ref><ref type="bibr">Du et al., 2023;</ref><ref type="bibr">Yang et al., 2024d;</ref><ref type="bibr" target="#b46">Mu et al., 2024)</ref>. Others enhance policy generation by using MLLMs to create visual markers or generate constraints or guidance with visual masks <ref type="bibr" target="#b60">(Sharma et al., 2023;</ref><ref type="bibr" target="#b68">Stone et al., 2023;</ref><ref type="bibr">Nasiriany et al., 2024b;</ref><ref type="bibr">Huang et al., 2024a;</ref><ref type="bibr">Jiang et al., 2024)</ref>. A different approach involves prompting MLLMs to generate code for creating policy or value functions <ref type="bibr" target="#b30">(Liang et al., 2023;</ref><ref type="bibr">Huang et al., 2023c;</ref><ref type="bibr">2024b)</ref>.</p><p>Most recently, Vision Language Action Models (VLAs) <ref type="bibr" target="#b7">(Brohan et al., 2022;</ref><ref type="bibr" target="#b69">2023;</ref><ref type="bibr">Chi et al., 2023;</ref><ref type="bibr" target="#b5">Belkhale et al., 2024;</ref><ref type="bibr">Team et al., 2024b;</ref><ref type="bibr">Liu et al., 2024c;</ref><ref type="bibr" target="#b23">Kim et al., 2024)</ref> have emerged as a promising direction. These models typically utilize MLLMs or language-conditioned diffusion models as their foundation and are trained on low-level robotics action data. Another promising direction leverages world models as action simulators <ref type="bibr" target="#b86">(Xiang et al., 2024;</ref><ref type="bibr" target="#b1">Agarwal et al., 2025;</ref><ref type="bibr" target="#b18">Liu et al., 2025)</ref>. These approaches employ diffusion models conditioned on language inputs to predict future states given actions or task descriptions.</p><p>In response to the rapid advancements in this field, various simulators <ref type="bibr" target="#b25">(Kolve et al., 2017;</ref><ref type="bibr" target="#b51">Puig et al., 2018;</ref><ref type="bibr">Shridhar et al., 2020a;</ref><ref type="bibr" target="#b85">Xiang et al., 2020;</ref><ref type="bibr" target="#b61">Shen et al., 2021;</ref><ref type="bibr" target="#b26">Li et al., 2021;</ref><ref type="bibr" target="#b69">2023;</ref><ref type="bibr">Nasiriany et al., 2024a)</ref> and evaluation benchmarks <ref type="bibr">(Shridhar et al., 2020b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b101">Zheng et al., 2022;</ref><ref type="bibr" target="#b27">Li et al., 2023;</ref><ref type="bibr" target="#b71">Szot et al., 2023;</ref><ref type="bibr" target="#b39">Luo et al., 2023;</ref><ref type="bibr">Li et al., 2024a;</ref><ref type="bibr" target="#b24">Koh et al., 2024;</ref><ref type="bibr">Choi et al., 2024;</ref><ref type="bibr" target="#b22">Khanna et al., 2024;</ref><ref type="bibr">Liu et al., 2024e;</ref><ref type="bibr">Li et al., 2024b;</ref><ref type="bibr">Zhang et al., 2024a;</ref><ref type="bibr" target="#b67">Song et al., 2024)</ref> have been developed. However, existing benchmarks exhibit notable limitations. For instance, ALFWorld <ref type="bibr">(Shridhar et al., 2020b)</ref>, AgentBench <ref type="bibr">(Liu et al., 2023b)</ref>, <ref type="bibr">Lota-bench (Choi et al., 2024)</ref>, and Embodied Agent Interface <ref type="bibr">(Li et al., 2024b)</ref> lack support for multimodal input evaluation. Furthermore, most benchmarks are narrowly focused on specific domains, particularly high-level household tasks <ref type="bibr">(Shridhar et al., 2020a;</ref><ref type="bibr" target="#b27">Li et al., 2023;</ref><ref type="bibr" target="#b71">Szot et al., 2023)</ref>, while others, such as VLMbench <ref type="bibr" target="#b101">(Zheng et al., 2022)</ref> and GOAT-bench <ref type="bibr" target="#b22">(Khanna et al., 2024)</ref>, concentrate on low-level control for manipulation and navigation, respectively. Although VisualAgentBench <ref type="bibr">(Liu et al., 2024e)</ref> pioneers the evaluation of MLLMs across multiple domains, it is limited to high-level tasks like household activities and Minecraft, and does not support fine-grained capability assessment. Embodied Agent Interface <ref type="bibr">(Li et al., 2024b)</ref> and VLABench <ref type="bibr">(Zhang et al., 2024a)</ref> introduce fine-grained evaluation metrics with language model support, but their focus remains primarily on LLMs and VLAs rather than MLLMs. Concurrently, EmbodiedEval <ref type="bibr" target="#b20">(Cheng et al., 2025)</ref> introduces a multi-domain benchmark for evaluating MLLMs across navigation, object interaction, social interaction, attribute question answering, and spatial question answering. While they overlap with our work in navigation and object interaction, their benchmark lacks low-level manipulation tasks and capability-oriented evaluation. Additionally, it is limited in scale, with only 328 testing instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details about EMBODIEDBENCH Environments and Datasets</head><p>Below, we provide detailed descriptions of four environments and their corresponding datasets. Please note that the maximum number of environment steps varies by task: 30 steps for high-level tasks (EB-ALFRED and EB-Navigation), 20 steps for EB-Navigation, and 15 steps for EB-Manipulation. In addition to task completion and exceeding the maximum step limit, we introduce two additional stopping conditions: (1) Invalid Action Limit: If the model generates more than 10 invalid actions in a single trajectory, indicating a lack of understanding and difficulty in producing valid actions.</p><p>(2) Empty Plan Generation: If the model generates an empty plan because it incorrectly assumes the task is complete. This issue mainly occurs in high-level tasks, and once it happens, the model tends to keep generating empty plans without making progress. These additional stopping conditions help reduce unnecessary computational costs and improve evaluation efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. EB-ALFRED</head><p>Task Description. We develop the EB-ALFRED tasks based on the ALFRED dataset and the AI2-THOR simulator, which are well-regarded within the embodied AI community for their diverse household tasks and scenes. These tasks aim to evaluate an agent's ability to organize and execute sequences of high-level actions in household scenarios, such as "Put washed lettuce in the refrigerator." Each task in ALFRED can be described using the Planning Domain Definition Language (PDDL), which helps assess the agent's success in completing the task or subgoals.</p><p>The ALFRED dataset includes 7 task types, Pick &amp; Place, Stack &amp; Place, Pick Two &amp; Place, Clean &amp; Place, Heat &amp; Place, Cool&amp; Place, and Examine in Light.</p><p>Our simulator is based on Lota-Bench's implementation for 8 high-level action types: "pick up", "open", "close", "turn on", "turn off", "slice", "put down", and "find". Each action can be parameterized with a specific object to form an action, e.g.,"find an apple" or "pick up an apple". The simulation offers an egocentric view and text feedback on the validity of action execution and potential reasons for any invalid actions. For example, it may indicate "failure to pick up an object because another object is already being held."</p><p>Despite its strengths, Lota-Bench's simulator has three notable limitations: (1) it does not support the Pick Two &amp; Place task type due to the inability to handle multiple instances of one object type.</p><p>(2) Some actions lead to incorrect task execution, such as the "put down" action erroneously placing an object on top of the sink instead of inside it, causing a correct action but unsuccessful outcome.</p><p>(3) Additionally, some instructions in the original ALFRED dataset suffer from low quality. We observe the erroneous use of "potato" in task related to "tomato", which prevents agents from successfully completing the tasks due to these incorrect instructions.</p><p>To enhance the simulation, we implemented several improvements. Firstly, we introduced support for multi-instance settings in ALFRED by appending index suffixes to objects, such as "find a cabinet 2," to accommodate multiple instances of the same object type. Therefore, we can support all 7 task types in ALFRED. Given the dynamic number of objects in the ALFRED dataset, we made the action space of EB-ALFRED dynamic, ranging from 171 to 298 actions. To minimize redundancy in the action space, we merge all "put down" actions into a single action, since only one object can be held at a time. Additionally, we manually corrected bugs in the original simulation and improved the quality of language instructions to ensure tasks are solvable and actions can be executed more accurately. These enhancements make EB-ALFRED a high-quality benchmark for evaluating embodied agents.</p><p>Dataset Collection. Following Lota-Bench (Choi et al., 2024), we use the valid seen set from the ALFRED dataset. We first partition the dataset based on the number of steps in the oracle policy. Specifically, we select 50 samples from the subset with fewer than 15 steps, carefully refining their instructions to minimize ambiguity and improve task solvability.</p><p>The commonsense and complex instruction subsets are primarily derived from this base subset, with GPT-4o augmentation tailored to specific capabilities. Additionally, we select 50 tasks with more than 15 steps to form the long-horizon subset. The visual appearance and spatial awareness subsets are chosen directly from the original dataset based on language descriptions of color/shape, or relative positions. In total, EB-ALFRED comprises 300 testing instances, evenly distributed across six subsets (50 instances each).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. EB-Habitat</head><p>Task Description. EB-Habitat is developed based on the Language Rearrangement benchmark <ref type="bibr" target="#b71">(Szot et al., 2023)</ref>, featuring 282 diverse language instruction templates designed for robotic rearrangement tasks. It leverages the Habitat 2.0 simulator <ref type="bibr" target="#b70">(Szot et al., 2021)</ref> and includes object data from the YCB dataset <ref type="bibr" target="#b10">(Calli et al., 2015)</ref> and ReplicaCAD <ref type="bibr" target="#b70">(Szot et al., 2021)</ref>.</p><p>The benchmark focuses on planning and executing 70 high-level skills to achieve user-defined goals, such as "Find a toy airplane and move it to the right counter." These skills are categorized into five action types: "navigation", "pick", "place", "open", and "close", each parameterized by specific objects.</p><p>Unlike ALFRED, which permits navigation to any object, EB-Habitat constrains navigation to receptacle-type objects, requiring robots to visit multiple locations to locate target items. Task and subgoal completion are evaluated using PDDL, with agents receiving visual input and textual feedback similar to ALFRED. Given its broad range of language instructions and distinct navigation constraints, EB-Habitat serves as a complementary counterpart to EB-ALFRED, expanding the scope of our high-level embodied tasks.</p><p>Dataset Collection. Habitat already provides fine-grained evaluation datasets with multiple subsets. We reorganize the subsets to formulate our dataset. Specifically, we merge "new scenes", "novel objects", and "instruction rephrasing" to form our base subset; we use the "context" set as our commonsense subset; we merge the "conditional instructions" and "irrelevant instruction text" as our complex instruction subset; we use the "referring expressions" as our visual appearance subset; we use the "spatial relationship" as our spatial awareness subset; we merge the "multiple rearrangements" and "multiple objects" as our long-horizon subset. Then, we sample 50 instances from each subset to form our EB-Habitat dataset, resulting in a total of 300 testing instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. EB-Navigation</head><p>Task Description. EB-Navigation is an evaluation suite built on AI2-THOR, designed to assess the navigation capabilities of embodied agents. In each task, the agent is placed at a starting position and must use visual observations and behavior feedback to execute low-level actions. The goal is to locate a target object and navigate to its vicinity. The agent's action space consists of seven actions that are executable by physical robots: (1) Move forward/backward by ∆x.</p><p>(2) Move rightward/leftward by ∆y.</p><p>(3) Rotate to the right/left by ∆θ degrees. (4) Tilt the camera upward/downward by ∆φ degrees.</p><p>At the start of each task, the agent is provided with a textual description of the action space, where each action is mapped to a unique index. Then, the agent selects an action by outputting the corresponding index, which the environment then executes.</p><p>At the beginning of each step, the environment provides the agent with a first-person visual observation. Using this visual input, the agent performs planning and decision-making to choose its next action. After executing an action, the environment evaluates its validity. For example, it checks for collisions or obstacles that might cause the action to fail. The environment then provides this valid or invalid signal as feedback to the agent. This signal is the only feedback the agent receives, as it is feasible to obtain in real-world scenarios. Together with the visual observations, this feedback equips the agent with sufficient information to perform navigation tasks effectively.</p><p>Dataset Collection. We constructed the dataset based on the original dataset provided by AI2-THOR. In AI2-THOR <ref type="bibr" target="#b25">(Kolve et al., 2017)</ref>, there are diverse scenes including environments such as kitchens, living rooms, and bedrooms, we designed a total of 90 navigation tasks, one for each scene. Each task dataset includes the following information: (1) Initial Robot Pose: Including its (x, y, z) coordinates and initial orientation.</p><p>(2) Target object information: Specifying the object type, ID and the 3D coordinates of the object's center. (4) Language navigation instruction: A human-readable instruction specifying the target object the agent needs to navigate to. We ensure the validness of the task dataset through the implementation of the following characteristics: (1) Initial distance: The agent's starting position is carefully constrained to be at least a certain adjustable distance (denoted as α) from the target object. This adjustable α allows users to customize the number of navigation steps required for each task.</p><p>(2) Target object accessibility: All target objects are exposed in the environment, reachable without requiring the agent to leave the ground. (3) Task completion conditions: A task is considered complete if the agent reaches a position within a specified distance threshold from the target object or if the maximum number of steps is exceeded. Additionally, the dataset includes an automated task-generation script. This script allows users to create custom task datasets by specifying parameters such as the target object type, initial distance threshold, and random seed for each scene. This flexibility ensures the dataset can be adapted to various research needs and scenarios.</p><p>For the capability-oriented subsets, we begin by sampling 60 instances from the original 90 tasks to form the base subset.</p><p>We then use GPT-4 to perform instruction augmentation, generating more complex instructions and incorporating common sense knowledge to create the complex instruction and common sense subsets. The visual appearance subset is manually curated to include detailed descriptions of the target object's color and shape. Finally, the long horizon subset is constructed by ensuring the target object is not visible in the agent's initial view, requiring extended navigation to locate it. In total, we collect 300 testing instances across these 5 subsets (excluding the spatial awareness subset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. EB-Manipulation</head><p>Task description. EB-Manipulation is an extension of VLMBench <ref type="bibr" target="#b101">(Zheng et al., 2022)</ref> using the CoppeliaSim simulator <ref type="bibr" target="#b57">(Rohmer et al., 2013)</ref> to control a 7-DoF Franka Emika Panda robotic arm. EB-Manipulation includes four task categories:</p><p>(1) Pick &amp; Place Objects, (2) Stack Objects, (3) Shape Sorter Placement, and (4) Table <ref type="table">Wiping</ref>, each with randomly varied instances in color, position, shape, and orientation for diverse evaluation. The action space is a 7-dimensional vector. The simulator processes these actions and performs automatic motion planning to achieve the desired position. To facilitate motion planning, the environment operates in ABS EE POSE PLAN WORLD FRAME mode, ensuring automatic trajectory execution from the current pose to the target pose. This simplifies the agent's role in predicting keypoints necessary for task completion.</p><p>Direct low-level manipulation is challenging for MLLMs due to insufficient domain-specific training. To overcome this, we implemented enhancements.</p><p>(1) Action space discretization <ref type="bibr" target="#b97">(Yin et al., 2024)</ref>, which divides the position component into 100 bins and the orientation component into 120 bins, enabling valid actions to take forms like <ref type="bibr">[x, y, z, roll, pitch, yaw, gripper] = [57, 61, 20, 10, 60, 25, 1]</ref>. Here, the first three dimensions (X, Y, Z) range from 0 to 100, while the next three (Pitch, Yaw, Gripper) range from 0 to 120. The gripper state remains binary (0.0 or 1.0). By discretizing the originally continuous action space, the model can predict actions using integer values, reducing complexity for MLLMs.</p><p>(2) Additional information like YOLO <ref type="bibr" target="#b55">(Redmon, 2016)</ref> detection boxes with index markers and 3D object pose estimation for indexed objects, reducing the need for precise 3D location. Instead, the agent can focus on perceiving and reasoning about each object's relationship to the indexed objects. With these improvements and additional in-context examples, our MLLM agent effectively tackles complex low-level manipulation tasks.</p><p>At each step, the environment provides a front-view visual observation capturing a wooden table, a robotic arm positioned at the center corner, and multiple objects placed on the table. Each object is enclosed within a detection box labeled with a numerical index, sorted in descending order based on its Y-coordinate. A 3D XYZ coordinate system is displayed at the robot's frame origin for spatial reference. The field of view (FOV) and image resolution are configurable, offering flexibility in visual input settings. Additionally, all visible objects in the scene are provided with discrete 3D coordinates, sorted in descending order based on their Y-coordinate, and labeled with object index (e.g., "object 1"). This setup requires the agent to understand the correlation between objects mentioned in the instruction and their corresponding object indices. Using this position information, the agent can plan and execute action sequences to achieve the manipulation goal. To ensure validity, the environment evaluates each action, preventing constraint violations such as invalid trajectories or out-of-range movements. The validity signal serves as the sole feedback mechanism.</p><p>Dataset Collection. For the base and spatial subsets, we select and curate samples from the VLMBench dataset. To generate instructions for each subset, we provide GPT-4o with 10 in-context examples. The common sense, visual appearance, and complex instruction subsets are derived from the base subset, with modifications designed to assess specific capabilities. The visual appearance subset consists of 36 tasks, as the table wiping task is excluded due to the inability to distinguish objects based on appearance. Each of the remaining 4 subsets comprises 48 tasks evenly distributed across four categories, with 12 tasks per category. In total, EB-Manipulation consists of 228 testing instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Versions</head><p>Table 4 lists the versions or full names of the models used in our experiments. We accessed proprietary models through API calls and open-source models via local deployment using lmdeploy (Contributors, 2023). Model Name Creator Full Name GPT-4o OpenAI gpt-4o-2024-08-06 GPT-4o-mini OpenAI gpt-4o-mini-2024-07-18 Claude</p><formula xml:id="formula_3">-3.5-Sonnet Anthropic claude-3-5-sonnet-20241022 Gemini-1.5-Pro Google gemini-1.5-pro Gemini-2.0-flash Google gemini-2.0-flash-exp Gemini-1.5-flash Google gemini-1.5-flash Llama-3.2-90B-Vision-Ins Meta meta-llama/Llama-3.2-90B-Vision-Instruct Llama-3.2-11B-Vision-Ins Meta meta-llama/Llama-3.2-11B-Vision-Instruct InternVL2 5-78B OpenGVLab OpenGVLab/InternVL2 5-78B InternVL2 5-38B OpenGVLab OpenGVLab/InternVL2 5-38B InternVL2 5-8B OpenGVLab OpenGVLab/InternVL2 5-8B Qwen2-VL-72B-Ins Qwen Qwen/Qwen2-VL-72B-Instruct Qwen2-VL-7B-Ins Qwen Qwen/Qwen2-VL-7B-Instruct</formula><p>Table 4. Full names of MLLMs used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Definitions and Examples of Capability-oriented Subsets</head><p>As listed in Table <ref type="table">5</ref>, we provide definitions and examples of six capability-oriented subsets in EMBODIEDBENCH.</p><p>Table 5. Definitions and examples of six capability-oriented subsets in EMBODIEDBENCH. Four environments EB-ALFRED, EB-Habitat, EB-Manipulation, and EB-Navigation are abbreviated as ALF, Hab, Man, and Nav, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subset Name Instruction Example Description</head><p>Base ALF: Put washed lettuce in the refrigerator.</p><p>Instructions used to describe basic tasks. Hab: Move one of the pear items to the indicated sofa. Man: Pick up the star and place it into the silver container. Nav: Navigate to the pillow in the room and be as close as possible to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Common Sense</head><p>ALF: Place washed leafy green vegetable in a receptacle that can keep it fresh for several days.</p><p>Refer to objects indirectly using common sense knowledge. Hab: Prepare for a game by delivering something to play with to the TV stand. Man: Pick up the bright object that usually appears in the night sky alongside the moon and place it into the silver box used for storing things. Nav: I'm feeling thirsty and need a small container to hold water or coffee. Please navigate to that object and stay near it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complex Instruction</head><p>ALF: For freshness, place the washed lettuce in the refrigerator. This way, it's ready for any delightful recipe ideas you have.</p><p>Add longer relevant or irrelevant context to obscure the instruction. This is used to evaluate the ability of understanding complex instructions. Hab: When you find the fridge door open, go ahead and move an bowl to the sofa; otherwise, transport an hammer to the sofa. Man: The objects on the desk seem perfect for children to play with. Can you now pick up the star and place it into the silver container? We're tidying up. Nav: The rhythmic ticking of the kitchen clock blends with the occasional drip from the faucet. There's a small pile of onions on the table, freshly chopped. Please move towards the stove burner for me. The kitchen has a comforting hum to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial Awareness</head><p>ALF: Put two spray bottles in the cabinet under the sink against the wall.</p><p>Refer to objects by their location relative to other receptacles or objects. Hab: Move a spatula from the right counter to the right receptacle of the left counter. Man: Pick up the left object and place it into the front container.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Appearance</head><p>ALF: Put a knife in a blue container onto the black table in the corner.</p><p>Refer to objects indirectly by their visual appearance. Hab: Deliver a small red object with green top to the intended a large gray piece of furniture with a backrest by physically moving it there. Man: Put the green object with five evenly spaced points into the sorting container. Nav: Find the rectangular yellowish object with a soft and smooth surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long Horizon</head><p>ALF: Pick up knife, slice apple, put knife in bowl, heat slice of apple in microwave, put apple slice on table.</p><p>Describe a task that requires a long sequence of actions to complete. For EB-navigation, the instruction is similar to the base subset but the location of object is not visible at initialization. Hab: Move the rubriks cube to the left counter, the wrench to the left counter, and the bowl to the brown table. Nav: Navigate to the Toaster in the room and be as close as possible to it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Experiment Results</head><p>To thoroughly evaluate the performance of MLLMs as agents within EMBODIEDBENCH, we present additional metric results, including subgoal success rate (Appendix E.1) and average step counts (Appendix E.2), and conduct a series of ablation studies. These ablation studies, spanning from Appendix E.3 to Appendix E.7, focus on five critical factors: (1) varying camera resolutions, (2) the use of detection boxes, (3) multi-step images, (4) multi-view images, and (5) visual in-context learning. In the subsequent sections, we systematically analyze each of these factors, offering insights into their effectiveness and potential limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Subgoal Success Rate</head><p>In addition to the task success rates presented in Table <ref type="table" target="#tab_5">2</ref>, we further analyze the subgoal success rates for high-level tasks (EB-ALFRED and EB-Habitat), as detailed in Table <ref type="table" target="#tab_15">6</ref>. Given the use of symbolic expressions (Planning Domain Definition Language, PDDL) in high-level tasks, calculating subgoal success rates is straightforward. For instance, a task success condition can be expressed as "condition A and condition B." Completing condition A alone results in a 50% subgoal success rate, even though the final task success rate remains 0%.</p><p>The results in Table <ref type="table" target="#tab_15">6</ref> generally align with those in Table <ref type="table" target="#tab_5">2</ref>. For most models, the subgoal success rates are higher than their final task success rates, which is expected. Notably, Gemini-1.5-Pro achieves higher subgoal success rates than Claude-3.5-Sonnet on the EB-ALFRED benchmark, despite Gemini-1.5-Pro having a lower final task success rate. Additionally, GPT-4o demonstrates subgoal performance comparable to Claude-3.5, with a gap of less than 0.2 in both environments, despite a substantial gap in their final task success rates. These findings suggest that while models demonstrate better ability to achieve subgoals, completing the final task remains a significant challenge. Additionally, the capability to achieve subgoals may slightly differ from the ability to accomplish the entire task. Since our primary objective is to achieve the full task, future research should focus on developing strategies to improve the final task success rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Average Planner and Environment Steps</head><p>This section presents the results of average planner steps and environment steps, which quantify the number of model inferences and interactions with the environment, respectively. Since we employ a multi-step planning strategy, the number of environment steps exceeds that of planner steps. However, it is important to note that neither planner steps nor environment steps serve as precise metrics for evaluating agent performance, unlike the success rate. This is because the agent may generate empty plans or produce more than 10 invalid actions, potentially triggering early termination. Consequently, fewer steps do not always indicate superior planning performance. Nevertheless, meaningful insights can still be derived from Table <ref type="table" target="#tab_16">7</ref> and <ref type="table">Table 8:</ref> • The multi-step planning strategy demonstrates significant efficiency in most cases, reducing average planner steps by around 50% to 80% compared to average environment steps. This is particularly evident in the EB-Manipulation task, where the average planner step for GPT-4o is 2.6, and the average environment step is 12.9, resulting in nearly 80% fewer model inferences. This highlights the model's ability to generate long action sequences and we can effectively leverage this capability to minimize costs. Such efficiency is especially advantageous when utilizing expensive large proprietary MLLMs.</p><p>• Despite the inherent inaccuracies in average step counts, it is still possible to observe that more capable models tend to achieve smaller average planner and environment steps. For instance, Claude-3.5-Sonnet achieves the lowest planner and environment steps in both EB-ALFRED and EB-Habitat tasks, while GPT-4o records the lowest average planner and environment steps in EB-Manipulation. Additionally, larger models generally require fewer steps than their smaller counterparts, as evidenced by the comparison between GPT-4o and GPT-4o-mini, as well as Gemini-1.5-Pro and Gemini-2.0-Flash. Table 8. Average planner steps and environment steps in EB-Navigation and EB-Manipulation for different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Camera Resolution</head><p>As shown in Figure <ref type="figure" target="#fig_6">7</ref>, we tested three camera resolutions-300×300, 500×500, and 700×700-on EB-ALFRED, EB-Manipulation, and EB-Navigation tasks. Our results reveal a task-dependent pattern: for EB-ALFRED, where vision plays a secondary role, increasing the resolution slightly improves performance for both GPT-4o and Claude-3.5-Sonnet, with accuracy gains of 2% ∼ 4%. In contrast, for EB-Manipulation and EB-Navigation, resolution is more critical, with the best performance achieved at 500×500. This suggests that while low-resolution images may lack the fine details needed for task execution, overly high resolutions can introduce unnecessary complexity, making it harder for MLLMs to focus on relevant information. These findings underscore the importance of choosing the right resolution when deploying MLLM-based embodied agents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4. Detection Boxes</head><p>Figure <ref type="figure" target="#fig_7">8</ref> demonstrates the effect of using bounding boxes. The results show that detection boxes are helpful for EB-ALFRED and EB-Manipulation, improving object recognition and interaction for both GPT-4o and Claude-3.5-Sonnet. Notably, EB-Manipulation sees a significant improvement of nearly 10%. However, for EB-Navigation, bounding boxes tend to cause confusion, likely because they obscure spatial cues essential for path planning, resulting in lower success rates. This highlights the need to tailor visual augmentation techniques to the specific demands of each task. As a result, we only enable detection boxes by default for EB-Manipulation and exclude them for other task groups. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5. Multi-step Images</head><p>Using sequences of images is a common approach to address partial observation. As shown in Figure <ref type="figure">9</ref> and 10, observation images from the previous two environment steps are also included in addition to the planner's original visual input. We explore the effectiveness of multi-step images-sequential frames shown in Figure <ref type="figure" target="#fig_0">11</ref>-where the latest three images are included as input. Surprisingly, adding temporal context does not improve decision-making; instead, it leads to a decline in performance, particularly for EB-Manipulation. This may be due to the models' struggle to interpret the relationship between multiple sequential images and their current state. These results emphasize the challenges of effectively utilizing temporal continuity in vision-language tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.6. Multi-view Images</head><p>In addition to multi-step images, another approach is to incorporate multi-view images from different cameras at the same time step. As shown in Figure <ref type="figure" target="#fig_0">12</ref>, the planner receives images from two different viewpoints as input. For EB-Navigation, the input consists of a front-view image and a top-down view image. For EB-Manipulation, the planner receives a front-view image and a wrist-view image. To evaluate whether multi-view images enhance performance in EB-Manipulation and EB-Navigation, we present the results in Figure <ref type="figure" target="#fig_2">13</ref>. Surprisingly, using multi-view data also results in a performance decline, particularly for GPT-4o. While multiple viewpoints theoretically offer richer spatial context, GPT-4o and Claude-3.5-Sonnet seem to struggle with effectively integrating and leveraging these additional perspectives. This limitation may arise from challenges in multi-view feature fusion or the increased complexity of the input.  <ref type="bibr">[50,</ref><ref type="bibr">77,</ref><ref type="bibr">17]</ref>.","reasoning_and_reflection": "The task is to pick up the star located at <ref type="bibr">[50,</ref><ref type="bibr">42,</ref><ref type="bibr">17]</ref> and place it into the yellow container located at <ref type="bibr">[70,</ref><ref type="bibr">43,</ref><ref type="bibr">17]</ref>. The plan involves moving the gripper to the star's position, closing the gripper to pick it up, moving to the yellow container's position, and opening the gripper to release the cube into the container.","executable plan": " <ref type="bibr">[[50, 42, 17, 6, 61, 36, 1]</ref>, <ref type="bibr">[50,</ref><ref type="bibr">42,</ref><ref type="bibr">17,</ref><ref type="bibr">6,</ref><ref type="bibr">61,</ref><ref type="bibr">36,</ref><ref type="bibr">0]</ref>, <ref type="bibr">[47,</ref><ref type="bibr">41,</ref><ref type="bibr">26,</ref><ref type="bibr">6,</ref><ref type="bibr">61,</ref><ref type="bibr">36,</ref><ref type="bibr">0]</ref>, <ref type="bibr">[69,</ref><ref type="bibr">39,</ref><ref type="bibr">24,</ref><ref type="bibr">88,</ref><ref type="bibr">32,</ref><ref type="bibr">77,</ref><ref type="bibr">0]</ref>, <ref type="bibr">[69,</ref><ref type="bibr">39,</ref><ref type="bibr">24,</ref><ref type="bibr">87,</ref><ref type="bibr">32,</ref><ref type="bibr">77,</ref><ref type="bibr">1]</ref>]"} target object. Perception Errors. The first category involves the model's ability to interpret visual observations and recognize the spatial position of the target object. We observed two common failure patterns: (1) Wrong Recognition: In some cases, the model failed to identify the target object even when it was present in the visual input. This suggests limitations in object recognition, possibly due to inadequate feature extraction or attention mechanisms. (2)Hallucination of the Target Object:</p><p>In other instances, the model incorrectly claimed to have detected the target object when it was not actually present. This issue is particularly problematic, as it leads to premature conclusions and incorrect decisions instead of further exploration. Ideally, the model should acknowledge its inability to locate the target and continue navigating appropriately. Error Type Definition Perception Errors Hallucination Perceiving objects or attributes that are not present in the visual input Wrong Recognition Incorrectly identifying object types or attributes Spatial Understanding Misjudging object positions / depths in 3D space Reasoning Errors Spatial Reasoning Failure to understand / reason about spatial relationships Insufficient Exploration Only giving a suboptimal exploration strategy through reasoning Wrong Termination Decision Ending task execution before completing the goal Reflection Error Failing to realize previous errors or adapt plans using environmental feedback Planning Errors Inaccurate Action Executing actions with incorrect parameters / poses Missing Steps Omitting necessary actions in sequential plans Invalid Action Attempting physically impossible interactions Action ID Mismatch Misaligning action names with wrong action IDs Reasoning Errors. The second category arises from the model's limitations in reasoning. This problem appears in two main ways: flawed logical reasoning about possible paths, even when visual observations are accurate; and weak reflection on spatial structure after failed attempts or feedback from previous steps. These issues indicate a lack of strong 3D spatial reasoning. The model often struggles to build a coherent 3D representation from sequential 2D observations, resulting in poor movement decisions. Furthermore, the model is not good at refining its actions based on ongoing feedback.</p><p>Planning Errors. Even when the model correctly identified the general direction, it often struggled with movement precision ("inaccurate actions"). For instance, it might overshoot the target by taking three steps instead of two. This highlights problems with spatial quantification, as the model's distance estimation and movement execution frequently did not align with real-world needs.</p><p>These main categories of errors reveal significant limitations in the navigation capabilities of current MLLM-based agents. To address these challenges, improvements are needed in several areas: strengthening object recognition, reducing hallucinations, enhancing 3D spatial reasoning, and aligning the model output with the action space to generate accurate plans. These advancements would enable more reliable and efficient autonomous navigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3. Format Errors</head><p>In addition to the aforementioned error types, we also observe output format errors when generating JSON. Specifically, smaller-scale models can fail to produce valid JSON files. Table <ref type="table" target="#tab_20">10</ref> shows the number of format errors across all six subsets, revealing that even proprietary models are not immune to these errors. A clear trend emerges: larger models tend to have fewer format errors, while smaller models are more prone to such issues. This highlights the need for further alignment of small models to improve the accuracy of structured outputs. Additionally, EB-ALFRED exhibits a higher number of errors, largely due to its greater complexity and the increased number of action steps in a trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Name EB-ALFRED EB-Habitat</head><p>GPT-4o 0.0067 0.0067 GPT-4o-mini 0.0867 0.0200 Claude-3.5-Sonnet 1.5400 0.0000 Gemini-1.5-Pro 0.0000 0.0000 Gemini-2.0-flash 0.0000 0.0000 Llama-3. Aim for about 5-6 actions in this step to be closer to the target object. !!!Notice: You cannot assess the situation until the whole plan in this planning step is finished and executed, so plan accordingly. At last, output the action id(s) (0 -7) from the available actions to execute.</p><p>The input given to you is a first-person view observation. Plan accordingly based on the visual observation.</p><p>Prompt for EB-Manipulation ## You are a Franka Panda robot with a parallel gripper. You can perform various tasks and output a sequence of gripper actions to accomplish a given task with images of your status. The input space, output action space, and color space are defined as follows:</p><p>** Input Space ** -Each input object is represented as a 3D discrete position in the following format: [X, Y, Z].</p><p>-There is a red XYZ coordinate frame located in the top-left corner of the table. The X-Y plane is the table surface.</p><p>- ** Color space ** -Each object can be described using one of the colors below: ["red", "maroon", "lime", "green", "blue", "navy", "yellow", "cyan", "magenta", "silver", "gray", "olive", "purple", "teal", "azure", "violet", "rose", "black", "white"],</p><p>Below are some examples to guide you in completing the task. EB-ALFRED utilizes 10 examples from the training set of the original ALFRED dataset for all subsets except the long-horizon subsets. We make sure the 10 examples diversely cover all 7 task types of ALFRED. For the long-horizon subsets, we include 7 different demonstrations closely related to the task types "pick heat then place in recep" and "pick coll then place in recep." It's important to note that the in-context examples do not include scenarios with multiple instances of the same object. Additionally, we ensure the examples do not involve variable action indices (i.e., for actions with IDs below 162, object names remain static without suffixes, while actions with IDs 162 or higher involve changing objects).</p><p>Similarly, EB-Habitat features 10 demonstrations from the 'train' set of the original Language Rearrangement dataset <ref type="bibr" target="#b71">(Szot et al., 2023</ref>) across all subsets. For EB-Navigation, we provide 3 demonstrations for all subsets, ensuring they are distinct from the testing instances. In EB-</p><p>Manipulation, the number of examples varies depending on task complexity: 2 examples for Pick &amp; Place, 4 examples for Stacking and Shape Sorting, and 8 examples for Table Wiping. All demonstrations are carefully selected to differ from the testing instances. In-context example for EB-ALFRED long horizon tasks ## Example 1: Human instruction: Place the microwaved slice of apple on to the white table. Output: { "language_plan": "Locate an apple and pick it up, find a dining table and put the apple down, look for a knife and pick it up, find the apple and slice it. After slicing, I need to find a place like the dining table to put the knife down. After that, find an apple slice and pick it up, search for a microwave and open it, put the apple in the microwave and close it, turn on the microwave to heat the apple slice, turn off the microwave and open it, pick up the microwaved apple slice, close the microwave, find the white table, and place the microwaved apple slice on the white table.", "executable_plan": [</p><p>{"action_id": 22, "action_name": "find a Apple" }, {"action_id": 107, "action_name": "pick up the Apple" }, {"action_id": 18, "action_name": "find a DiningTable" }, {"action_id": 133, "action_name": "put down the object in hand" }, {"action_id": 34, "action_name": "find a Knife" }, {"action_id": 127, "action_name": "pick up the Knife" }, {"action_id": 22, "action_name": "find a Apple" }, {"action_id": 160, "action_name": "slice the Apple" }, {"action_id": 18, "action_name": "find a DiningTable" }, {"action_id": 133, "action_name": "put down the object in hand" }, {"action_id": 22, "action_name": "find a Apple" }, {"action_id": 107, "action_name": "pick up the Apple" }, {"action_id": 38, "action_name": "find a Microwave" }, {"action_id": 143, "action_name": "open the Microwave" }, {"action_id": 133, "action_name": "put down the object in hand" }, {"action_id": 144, "action_name": "close the Microwave" }, {"action_id": 149, "action_name": "turn on the Microwave" }, {"action_id": 150, "action_name": "turn off the Microwave" }, {"action_id": 143, "action_name": "open the Microwave" }, {"action_id": 22, "action_name": "find a Apple" }, {"action_id": 107, "action_name": "pick up the Apple" }, {"action_id": 144, "action_name": "close the Microwave" }, {"action_id": 18, "action_name": "find a DiningTable" }, {"action_id": 133, "action_name": "put down the object in hand" } ] } Human Instruction: Move the spoon to the brown table, the sponge to the brown table, and the cleanser to the black table.</p><p>1. navigate to the left counter in the kitchen 2. pick up the spoon 3. navigate to the table 2 4. place at the table 2 5. navigate to the TV stand 6. pick up the sponge 7. navigate to the left counter in the kitchen 8. pick up the sponge 9. navigate to the table 2 10. place at the table 2 11. navigate to the TV stand 12. pick up the cleanser 13. navigate to the left counter in the kitchen 15. navigate to the table 1 14. pick up the cleanser 16. place at the table 1</p><p>Planning Step 1 (env step 1 -env step 6):</p><p>Visual state description: The image shows a kitchen with wooden flooring, a refrigerator, a sink, and several cabinets. There are no visible objects such as a spoon, sponge, or cleanser in the immediate view.</p><p>Reasoning &amp; Reflection: The task requires moving a spoon, sponge, and cleanser to specific tables. Since these objects are not visible in the current view, the robot needs to navigate to locations where these objects might be found. The robot will start by navigating to the left counter in the kitchen to search for the spoon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Planning</head><p>Step 3 (env step 13 -env step 16):</p><p>Visual state description: The image shows a room with a dark blue floor and a black wall.</p><p>There is a can on the floor and a small object near it. The robot arm is visible in the upper right corner of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reasoning &amp; Reflection:</head><p>The robot has successfully moved the spoon and the sponge to the brown table (table <ref type="table" target="#tab_5">2</ref>). However, the last attempt to pick up the cleanser from the TV stand was unsuccessful because the cleanser was not near the robot. The robot needs to locate the cleanser before attempting to pick it up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Planning</head><p>Step 2 (env step 7 -env step 12):</p><p>Visual state description: The image shows a room with a blue carpet, a can, and a small object on the floor. There is a cabinet and a table visible in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reasoning &amp; Reflection:</head><p>The robot has successfully moved the spoon to the brown table (table <ref type="table" target="#tab_5">2</ref>). However, the attempt to pick up the sponge from the TV stand was unsuccessful because the sponge was not near the robot. The robot needs to locate the sponge and the cleanser to complete the task. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Overview of EMBODIEDBENCH. Two key features of our benchmark: various action levels and capability-oriented evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>8</head><figDesc>low-level actions: (1) Move forward/backward/left/right by ∆x. (2) Rotate to the right/left by ∆θ degrees. (3) Tilt the camera upward/downward by ∆φ degrees. The environment provides textual feedback on action validity, such as collision detection. Additionally, we offer a script for automatic task generation, allowing users to create custom task datasets by specifying the configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Planning examples in EB-ALFRED and EB-Manipulation based on GPT-4o.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Language-centric ablations on EB-ALFRED.</figDesc><graphic coords="7,55,52,570,26,112,34,74,18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>Figure 5. Visual-centric ablations on EB-Manipulation.</figDesc><graphic coords="7,424,89,606,08,111,09,73,41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>step. Two multi-step image examples are shown in Figure 9 and 10. Figure 5 (c) presents the quantitative results. Our experiments reveal that current MLLMs struggle to effectively utilize multiple image inputs, often leading to confusion about their current state. Future work could focus on developing methods to better leverage multiple images for enhanced understanding and reasoning. Visual In-context Learning (ICL). Previous work has primarily relied on text-based ICL demonstrations. In this study, we investigate the impact of visual ICL for embodied agents by including image observations as part of the in-context examples for EB-Manipulation. This approach helps the model better understand the relationship between successful low-level actions and the object positions in the image. Visual ICL examples are demonstrated in Figure 15. We limit the number of examples to two to avoid overwhelming the model with excessive visual input. This may slightly lower the baseline performance, as the main results use more than two text-based examples. As shown in Figure 5 (d), the results demonstrate that visual ICL significantly outperforms language-only ICL. For instance, Claude-3.5-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Impact of different camera resolutions on EMBODIEDBENCH.</figDesc><graphic coords="23,380,12,102,06,161,29,105,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Impact of detection boxes on EMBODIEDBENCH.</figDesc><graphic coords="23,380,32,345,11,160,84,106,29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>{</head><figDesc>"reasoning": "I can see the garbage can in the upper left corner of the image, next to the kitchen sink. To move there, we can go forward-left, but since there's a kitchen counter directly ahead, we should go left first. Following the strategy, I can go by first moving leftward.", "output": "[[3, 'Move leftward by 0.25'], [3, 'Move leftward by 0.25']]", "env_feedback": "Last action is executed successfully."} Planning Step 2 {"reasoning": "From the secene, I see that by moving leftward, we are getting closer to the garbage can. Now, the garbage can is in front of me, slightly to the left. And there's a large area ahead of us. Following the strategy, I can go by first moving forward then moving leftward.", "output": "[[0, 'Move forward by 0.25'], [0, 'Move forward by 0.25'], [0, 'Move forward by 0.25'],[0, 'Move leftward by 0.25']]", "env_feedback": "Last action is executed successfully."} {"reasoning": "From the image we can see the garbage can is very close to us, still to our front-left. Moving leftward might be blocked but i can see that there is still space in front of me to get closer to the garbage can. Following the strategy, we can take about two steps forward then one step left to reach the garbage can.", "output": "[[3, 'Move forward by 0.25'],[3, 'Move forward by 0.25'],[3, 'Move leftward by 0.25']]", "env_feedback": "Success"} ": "From left to right, I can see a purple cylinder at[45,<ref type="bibr" target="#b111">13,</ref> 18], a blue moon at[72, 20, 18], a green star at[50, 42, 17], a purple cube at[36, 42, 18], a yellow container at[70, 43, 17]  and a purple container at</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Visual in-context learning examples for EB-Navigation &amp; EB-Manipulation</figDesc><graphic coords="26,104,18,462,56,90,76,90,76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Impact of visual in-context learning on EMBODIEDBENCH.</figDesc><graphic coords="27,128,41,67,89,169,16,111,64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><figDesc>The allowed range of X, Y, Z is[0, 100]. -Objects are ordered by Y in ascending order. ** Output Action Space ** -Each output action is represented as a 7D discrete gripper action in the following format: [X, Y, Z, Roll, Pitch, Yaw, Gripper]. -X, Y, Z are the 3D discrete positions of the gripper in the environment. It follows the same coordinate system as the input object coordinates. -The allowed range of X, Y, Z is [0, 100]. -Roll, Pitch, and Yaw are the 3D discrete orientations of the gripper in the environment, represented as discrete Euler Angles. -The allowed range of Roll, Pitch, and Yaw is [0, 120] and each unit represents 3 degrees. -Gripper state is 0 for close and 1 for open.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><figDesc>you are supposed to follow the above examples to generate a sequence of discrete gripper actions that completes the below human instruction. Human Instruction: {TASK INSTRUCTION} Input: {TASK INPUT} Output gripper actions: {ACTION HISTORY &amp; ENVIRONMENT FEEDBACK (if available)}G.3. In-context examplesThe in-context examples provided to the agent are detailed below. Each environment includes one to ten representative examples, with the complete set in our source code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 18 .</head><label>18</label><figDesc>Figure 18. Planning example in EB-Habitat for InternVL2.5-78B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Task success rates on 6 subsets of EB-ALFRED and EB-Habitat, with the best proprietary model in bold and open-source model underlines per column. Success rates for subsets are integers since each subset consists of 50 test instances.</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell>EB-ALFRED</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>EB-Habitat</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Avg</cell><cell cols="5">Base Common Complex Visual Spatial</cell><cell>Long</cell><cell>Avg</cell><cell cols="5">Base Common Complex Visual Spatial</cell><cell>Long</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Proprietary MLLMs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GPT-4o</cell><cell>56.3</cell><cell>64</cell><cell>54</cell><cell>68</cell><cell>46</cell><cell>52</cell><cell>54</cell><cell>59.0</cell><cell>86</cell><cell>44</cell><cell>56</cell><cell>68</cell><cell>36</cell><cell>64</cell></row><row><cell>GPT-4o-mini</cell><cell>24.0</cell><cell>34</cell><cell>28</cell><cell>36</cell><cell>24</cell><cell>22</cell><cell>0</cell><cell>32.7</cell><cell>74</cell><cell>22</cell><cell>32</cell><cell>22</cell><cell>32</cell><cell>14</cell></row><row><cell>Claude-3.5-Sonnet</cell><cell>64.0</cell><cell>72</cell><cell>66</cell><cell>76</cell><cell>60</cell><cell>58</cell><cell>52</cell><cell>68.0</cell><cell>96</cell><cell>68</cell><cell>78</cell><cell>70</cell><cell>38</cell><cell>58</cell></row><row><cell>Gemini-1.5-Pro</cell><cell>62.3</cell><cell>70</cell><cell>64</cell><cell>72</cell><cell>58</cell><cell>52</cell><cell>58</cell><cell>56.3</cell><cell>92</cell><cell>52</cell><cell>48</cell><cell>56</cell><cell>38</cell><cell>52</cell></row><row><cell>Gemini-2.0-flash</cell><cell>52.3</cell><cell>62</cell><cell>48</cell><cell>54</cell><cell>46</cell><cell>46</cell><cell>58</cell><cell>42.3</cell><cell>82</cell><cell>38</cell><cell>38</cell><cell>36</cell><cell>34</cell><cell>26</cell></row><row><cell>Gemini-1.5-flash</cell><cell>39.3</cell><cell>44</cell><cell>40</cell><cell>56</cell><cell>42</cell><cell>26</cell><cell>28</cell><cell>39.3</cell><cell>76</cell><cell>32</cell><cell>48</cell><cell>36</cell><cell>32</cell><cell>12</cell></row><row><cell>GPT-4o (Lang)</cell><cell>58.0</cell><cell>62</cell><cell>64</cell><cell>70</cell><cell>52</cell><cell>46</cell><cell>54</cell><cell>56.0</cell><cell>82</cell><cell>52</cell><cell>58</cell><cell>74</cell><cell>34</cell><cell>36</cell></row><row><cell>GPT-4o-mini (Lang)</cell><cell>31.3</cell><cell>42</cell><cell>36</cell><cell>46</cell><cell>30</cell><cell>20</cell><cell>14</cell><cell>36.7</cell><cell>82</cell><cell>30</cell><cell>34</cell><cell>30</cell><cell>30</cell><cell>14</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Open-Source MLLMs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Llama-3.2-90B-Vision-Ins</cell><cell>32.0</cell><cell>38</cell><cell>34</cell><cell>44</cell><cell>28</cell><cell>32</cell><cell>16</cell><cell>40.3</cell><cell>94</cell><cell>24</cell><cell>50</cell><cell>32</cell><cell>28</cell><cell>14</cell></row><row><cell>Llama-3.2-11B-Vision-Ins</cell><cell>13.7</cell><cell>24</cell><cell>8</cell><cell>16</cell><cell>22</cell><cell>6</cell><cell>6</cell><cell>25.0</cell><cell>70</cell><cell>16</cell><cell>28</cell><cell>10</cell><cell>20</cell><cell>6</cell></row><row><cell>InternVL2 5-78B</cell><cell>37.7</cell><cell>38</cell><cell>34</cell><cell>42</cell><cell>34</cell><cell>36</cell><cell>42</cell><cell>49.0</cell><cell>80</cell><cell>42</cell><cell>56</cell><cell>58</cell><cell>30</cell><cell>28</cell></row><row><cell>InternVL2 5-38B</cell><cell>23.3</cell><cell>36</cell><cell>30</cell><cell>36</cell><cell>22</cell><cell>14</cell><cell>26</cell><cell>38.3</cell><cell>60</cell><cell>28</cell><cell>48</cell><cell>34</cell><cell>32</cell><cell>28</cell></row><row><cell>InternVL2 5-8B</cell><cell>2.0</cell><cell>4</cell><cell>6</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>11.3</cell><cell>36</cell><cell>4</cell><cell>0</cell><cell>10</cell><cell>16</cell><cell>2</cell></row><row><cell>Qwen2-VL-72B-Ins</cell><cell>33.7</cell><cell>40</cell><cell>30</cell><cell>40</cell><cell>30</cell><cell>32</cell><cell>30</cell><cell>35.7</cell><cell>70</cell><cell>30</cell><cell>36</cell><cell>32</cell><cell>28</cell><cell>18</cell></row><row><cell>Qwen2-VL-7B-Ins</cell><cell>1.7</cell><cell>6</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>18.3</cell><cell>48</cell><cell>6</cell><cell>16</cell><cell>20</cell><cell>18</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Task success rates on 5 subsets of EB-Navigation and EB-Manipulation, with the best proprietary model in bold and open-source model underlines per column.</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell cols="2">EB-Navigation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">EB-Manipulation</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Avg</cell><cell cols="4">Base Common Complex Visual</cell><cell>Long</cell><cell>Avg</cell><cell cols="5">Base Common Complex Visual Spatial</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Proprietary MLLMs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GPT-4o</cell><cell>57.7</cell><cell>55.0</cell><cell>60.0</cell><cell>58.3</cell><cell>60.0</cell><cell>55.0</cell><cell>28.9</cell><cell>39.6</cell><cell>29.2</cell><cell>29.2</cell><cell>19.4</cell><cell>25.0</cell></row><row><cell>GPT-4o-mini</cell><cell>32.8</cell><cell>31.7</cell><cell>33.3</cell><cell>35.0</cell><cell>28.3</cell><cell>33.3</cell><cell>4.8</cell><cell>4.2</cell><cell>6.3</cell><cell>2.1</cell><cell>0.0</cell><cell>10.4</cell></row><row><cell>Claude-3.5-Sonnet</cell><cell>44.7</cell><cell>66.7</cell><cell>51.7</cell><cell>41.7</cell><cell>36.7</cell><cell>26.7</cell><cell>25.4</cell><cell>37.5</cell><cell>16.7</cell><cell>29.2</cell><cell>19.4</cell><cell>22.9</cell></row><row><cell>Gemini-1.5-Pro</cell><cell>24.3</cell><cell>23.3</cell><cell>25.0</cell><cell>25.0</cell><cell>28.3</cell><cell>20.0</cell><cell>21.1</cell><cell>14.6</cell><cell>14.6</cell><cell>22.9</cell><cell>16.7</cell><cell>35.4</cell></row><row><cell>Gemini-2.0-flash</cell><cell>48.7</cell><cell>63.3</cell><cell>65.0</cell><cell>50.0</cell><cell>51.7</cell><cell>13.3</cell><cell>16.7</cell><cell>14.6</cell><cell>8.3</cell><cell>14.6</cell><cell>13.9</cell><cell>31.3</cell></row><row><cell>Gemini-1.5-flash</cell><cell>41.7</cell><cell>56.7</cell><cell>50.0</cell><cell>46.7</cell><cell>50.0</cell><cell>5.0</cell><cell>9.6</cell><cell>14.6</cell><cell>10.4</cell><cell>4.2</cell><cell>8.3</cell><cell>10.4</cell></row><row><cell>GPT-4o (Lang)</cell><cell>17.4</cell><cell>21.7</cell><cell>21.7</cell><cell>26.7</cell><cell>16.7</cell><cell>0.0</cell><cell>16.2</cell><cell>16.7</cell><cell>16.7</cell><cell>14.6</cell><cell>19.4</cell><cell>14.6</cell></row><row><cell>GPT-4o-mini (Lang)</cell><cell>8.3</cell><cell>3.3</cell><cell>13.3</cell><cell>10.0</cell><cell>15.0</cell><cell>0.0</cell><cell>6.6</cell><cell>12.5</cell><cell>0.0</cell><cell>2.1</cell><cell>2.8</cell><cell>14.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Open-Source MLLMs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Llama-3.2-90B-Vision-Ins</cell><cell>30.0</cell><cell>48.3</cell><cell>23.3</cell><cell>38.3</cell><cell>33.3</cell><cell>6.7</cell><cell>14.9</cell><cell>10.4</cell><cell>12.5</cell><cell>16.7</cell><cell>10.4</cell><cell>20.8</cell></row><row><cell>Llama-3.2-11B-Vision-Ins</cell><cell>21.4</cell><cell>23.3</cell><cell>21.7</cell><cell>26.7</cell><cell>18.3</cell><cell>17.0</cell><cell>0.9</cell><cell>0.0</cell><cell>0.0</cell><cell>2.1</cell><cell>0.0</cell><cell>2.1</cell></row><row><cell>InternVL2 5-78B</cell><cell>30.7</cell><cell>36.7</cell><cell>38.3</cell><cell>33.3</cell><cell>21.7</cell><cell>23.3</cell><cell>18.0</cell><cell>16.7</cell><cell>16.7</cell><cell>14.6</cell><cell>22.2</cell><cell>20.8</cell></row><row><cell>InternVL2 5-38B</cell><cell>30.3</cell><cell>35.0</cell><cell>28.3</cell><cell>38.3</cell><cell>26.7</cell><cell>23.3</cell><cell>15.8</cell><cell>22.9</cell><cell>16.7</cell><cell>8.3</cell><cell>13.9</cell><cell>16.7</cell></row><row><cell>InternVL2 5-8B</cell><cell>21.3</cell><cell>35.0</cell><cell>23.3</cell><cell>21.7</cell><cell>26.7</cell><cell>0.0</cell><cell>7.0</cell><cell>8.3</cell><cell>2.1</cell><cell>6.3</cell><cell>8.3</cell><cell>10.4</cell></row><row><cell>Qwen2-VL-72B-Ins</cell><cell>21.2</cell><cell>26.7</cell><cell>30.0</cell><cell>28.3</cell><cell>16.0</cell><cell>5.0</cell><cell>13.6</cell><cell>18.8</cell><cell>20.8</cell><cell>4.2</cell><cell>8.3</cell><cell>14.6</cell></row><row><cell>Qwen2-VL-7B-Ins</cell><cell>14.0</cell><cell>26.7</cell><cell>10.0</cell><cell>15.0</cell><cell>15.0</cell><cell>3.3</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 6 .</head><label>6</label><figDesc>Subgoal success rates on 6 subsets of EB-ALFRED and EB-Habitat, with the best proprietary model in bold and open-source model underlines per column.</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell cols="2">EB-ALFRED</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>EB-Habitat</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Avg</cell><cell cols="5">Base Common Complex Visual Spatial</cell><cell>Long</cell><cell>Avg</cell><cell cols="5">Base Common Complex Visual Spatial</cell><cell>Long</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Proprietary MLLMs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GPT-4o</cell><cell>65.1</cell><cell>74.0</cell><cell>60.3</cell><cell>74.0</cell><cell>58.3</cell><cell>61.3</cell><cell>62.5</cell><cell>70.7</cell><cell>90.7</cell><cell>56.0</cell><cell>68.0</cell><cell>75.2</cell><cell>62.1</cell><cell>72.2</cell></row><row><cell>GPT-4o-mini</cell><cell>34.3</cell><cell>47.8</cell><cell>35.3</cell><cell>43.5</cell><cell>33.3</cell><cell>29.0</cell><cell>17.0</cell><cell>44.0</cell><cell>77.5</cell><cell>32.5</cell><cell>42.0</cell><cell>33.1</cell><cell>57.8</cell><cell>21.3</cell></row><row><cell>Claude-3.5-Sonnet</cell><cell>65.3</cell><cell>72.0</cell><cell>66.0</cell><cell>76.7</cell><cell>63.0</cell><cell>59.7</cell><cell>54.5</cell><cell>70.8</cell><cell>97.5</cell><cell>68.5</cell><cell>79.5</cell><cell>72.0</cell><cell>43.8</cell><cell>63.3</cell></row><row><cell>Gemini-1.5-Pro</cell><cell>67.4</cell><cell>74.3</cell><cell>66.7</cell><cell>76.5</cell><cell>62.8</cell><cell>59.0</cell><cell>65.0</cell><cell>61.0</cell><cell>92.5</cell><cell>53.5</cell><cell>49.5</cell><cell>59.4</cell><cell>50.0</cell><cell>61.2</cell></row><row><cell>Gemini-2.0-flash</cell><cell>56.3</cell><cell>65.7</cell><cell>51.3</cell><cell>58.3</cell><cell>50.7</cell><cell>50.0</cell><cell>62.0</cell><cell>48.2</cell><cell>82.0</cell><cell>39.5</cell><cell>43.0</cell><cell>39.0</cell><cell>49.6</cell><cell>36.2</cell></row><row><cell>Gemini-1.5-flash</cell><cell>46.1</cell><cell>49.5</cell><cell>45.2</cell><cell>60.2</cell><cell>48.3</cell><cell>32.2</cell><cell>41.5</cell><cell>46.8</cell><cell>79.0</cell><cell>33.0</cell><cell>50.0</cell><cell>41.2</cell><cell>55.5</cell><cell>22.0</cell></row><row><cell>GPT-4o (Lang)</cell><cell>65.6</cell><cell>67.7</cell><cell>70.3</cell><cell>77.0</cell><cell>59.7</cell><cell>54.0</cell><cell>65.0</cell><cell>66.7</cell><cell>85.2</cell><cell>58.5</cell><cell>67.5</cell><cell>79.2</cell><cell>62.1</cell><cell>47.7</cell></row><row><cell>GPT-4o-mini (Lang)</cell><cell>40.1</cell><cell>44.8</cell><cell>41.2</cell><cell>54.2</cell><cell>36.0</cell><cell>24.7</cell><cell>39.5</cell><cell>48.1</cell><cell>85.8</cell><cell>39.0</cell><cell>43.5</cell><cell>39.0</cell><cell>56.8</cell><cell>24.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Open-Source MLLMs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Llama-3.2-90B-Vision-Ins</cell><cell>37.6</cell><cell>43.7</cell><cell>37.3</cell><cell>49.2</cell><cell>35.3</cell><cell>36.0</cell><cell>24.0</cell><cell>50.6</cell><cell>94.5</cell><cell>32.5</cell><cell>53.0</cell><cell>39.7</cell><cell>59.6</cell><cell>24.3</cell></row><row><cell>Llama-3.2-11B-Vision-Ins</cell><cell>19.7</cell><cell>29.7</cell><cell>13.0</cell><cell>25.7</cell><cell>28.7</cell><cell>9.3</cell><cell>12.0</cell><cell>33.2</cell><cell>72.0</cell><cell>23.8</cell><cell>36.5</cell><cell>16.2</cell><cell>39.7</cell><cell>11.2</cell></row><row><cell>InternVL2 5-78B</cell><cell>41.0</cell><cell>42.3</cell><cell>35.3</cell><cell>43.3</cell><cell>35.7</cell><cell>40.3</cell><cell>49.0</cell><cell>55.2</cell><cell>82.0</cell><cell>43.0</cell><cell>59.0</cell><cell>63.9</cell><cell>45.1</cell><cell>38.2</cell></row><row><cell>InternVL2 5-38B</cell><cell>31.3</cell><cell>37.3</cell><cell>33.0</cell><cell>38.3</cell><cell>25.3</cell><cell>17.3</cell><cell>36.5</cell><cell>44.0</cell><cell>61.5</cell><cell>32.5</cell><cell>49.0</cell><cell>39.5</cell><cell>46.3</cell><cell>35.0</cell></row><row><cell>InternVL2 5-8B</cell><cell>2.0</cell><cell>4.0</cell><cell>6.0</cell><cell>2.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>19.4</cell><cell>40.2</cell><cell>11.5</cell><cell>11.0</cell><cell>16.0</cell><cell>30.7</cell><cell>7.3</cell></row><row><cell>Qwen2-VL-72B-Ins</cell><cell>38.7</cell><cell>45.3</cell><cell>33.3</cell><cell>44.7</cell><cell>35.7</cell><cell>33.0</cell><cell>40.0</cell><cell>42.2</cell><cell>72.0</cell><cell>33.0</cell><cell>39.0</cell><cell>37.0</cell><cell>52.0</cell><cell>20.3</cell></row><row><cell>Qwen2-VL-7B-Ins</cell><cell>5.2</cell><cell>8.3</cell><cell>5.3</cell><cell>7.0</cell><cell>1.7</cell><cell>3.3</cell><cell>5.5</cell><cell>26.1</cell><cell>53.3</cell><cell>9.0</cell><cell>24.0</cell><cell>25.8</cell><cell>41.2</cell><cell>3.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 7 .</head><label>7</label><figDesc>Average planner steps and environment steps in EB-ALFRED and EB-Habitat for different models.</figDesc><table><row><cell></cell><cell>EB-ALFRED</cell><cell></cell><cell>EB-Habitat</cell><cell></cell></row><row><cell></cell><cell cols="4">Avg Planner Step Avg Env Steps Avg Planner Step Avg Env Steps</cell></row><row><cell>GPT-4o</cell><cell>4.4</cell><cell>16.3</cell><cell>5.5</cell><cell>13.1</cell></row><row><cell>GPT-4o-mini</cell><cell>7.7</cell><cell>20.6</cell><cell>7.4</cell><cell>18.8</cell></row><row><cell>Claude-3.5-Sonnet</cell><cell>4.0</cell><cell>12.1</cell><cell>4.2</cell><cell>10.9</cell></row><row><cell>Gemini-1.5-Pro</cell><cell>3.9</cell><cell>15.7</cell><cell>5.4</cell><cell>12.6</cell></row><row><cell>Gemini-2.0-flash</cell><cell>4.4</cell><cell>16.3</cell><cell>6.8</cell><cell>14.8</cell></row><row><cell>Llama-3.2-90B-Vision-Ins</cell><cell>7.3</cell><cell>16.7</cell><cell>7.3</cell><cell>16.2</cell></row><row><cell>InternVL2 5-78B</cell><cell>5.5</cell><cell>13.9</cell><cell>6.3</cell><cell>14.1</cell></row><row><cell>Qwen2-VL-72B-Ins</cell><cell>6.1</cell><cell>13.7</cell><cell>6.8</cell><cell>14.2</cell></row><row><cell>Model</cell><cell>EB-Navigation</cell><cell></cell><cell cols="2">EB-Manipulation</cell></row><row><cell></cell><cell cols="4">Avg Planner Step Avg Env Steps Avg Planner Step Avg Env Steps</cell></row><row><cell>GPT-4o</cell><cell>6.2</cell><cell>15.5</cell><cell>2.6</cell><cell>12.9</cell></row><row><cell>GPT-4o-mini</cell><cell>7.6</cell><cell>17.5</cell><cell>3.4</cell><cell>14.7</cell></row><row><cell>Claude-3.5-Sonnet</cell><cell>6.2</cell><cell>15.6</cell><cell>2.7</cell><cell>13.3</cell></row><row><cell>Gemini-1.5-Pro</cell><cell>8.8</cell><cell>16.5</cell><cell>2.7</cell><cell>13.4</cell></row><row><cell>Gemini-2.0-flash</cell><cell>9.2</cell><cell>16.0</cell><cell>2.8</cell><cell>14.0</cell></row><row><cell>Llama-3.2-90B-Vision-Ins</cell><cell>7.5</cell><cell>17.4</cell><cell>3.0</cell><cell>13.9</cell></row><row><cell>InternVL2 5-78B</cell><cell>13.2</cell><cell>17.3</cell><cell>2.9</cell><cell>13.5</cell></row><row><cell>Qwen2-VL-72B-Ins</cell><cell>11.1</cell><cell>17.8</cell><cell>2.9</cell><cell>13.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 9 .</head><label>9</label><figDesc>Error Taxonomy with Definitions</figDesc><table><row><cell></cell><cell cols="3">hallucination</cell></row><row><cell></cell><cell></cell><cell>4%</cell><cell></cell></row><row><cell cols="2">inaccurate</cell><cell></cell><cell></cell></row><row><cell>action 23%</cell><cell>error planning</cell><cell cols="2">perception error</cell><cell>22% wrong recog.</cell></row><row><cell></cell><cell>23%</cell><cell></cell><cell>31%</cell></row><row><cell>reflection error 23%</cell><cell cols="2">reasoning error 46%</cell><cell cols="2">spatial understanding 4% spatial</cell></row><row><cell></cell><cell>insufficient</cell><cell></cell><cell cols="2">reasoning</cell></row><row><cell></cell><cell>exploration</cell><cell></cell><cell>23%</cell></row><row><cell></cell><cell>1%</cell><cell></cell><cell></cell></row><row><cell cols="5">Figure 16. Error Analysis on EB-Navigation.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 10 .</head><label>10</label><figDesc>Format error number per trajectory in EB-ALFRED and EB-Habitat across all subsets.We provide the agent input prompts used as textual input to MLLMs for all four environments.Parameterized by the name of the receptacle to navigate to. So long as the object is present in the scene, this skill is always valid.• Pick up: Parameterized by the name of the object to pick. Only valid if the robot is close to the object, not holding another object, and the object is not inside a closed receptacle.• Put down: Parameterized by the name of the object to put down to a nearby receptacle. Only valid if the robot is holding an object.• Drop: Parameterized by the name of the object to put down. It is different from the Put down action, as this does not guarantee the held object will be put into a specified receptacle. • Open: Parameterized by the name of the receptacle to open. Only valid if the receptacle is closed and the robot is close to the receptacle. • Close: Parameterized by the name of the receptacle to close. Only valid if the receptacle is open and the robot is close to the receptacle. • Turn on: Parameterized by the name of the object to turn on. Only valid if the object is turned off and the robot is close to the object. • Turn off: Parameterized by the name of the object to turn off. Only valid if the object is turned on and the robot is close to the object. • Slice: Parameterized by the name of the object to slice. Only valid if the object is sliceable and the robot is close to the object.Make sure to match the action name and its corresponding action id in the output. Avoid performing actions that do not meet the defined validity criteria. For instance, if you want to put an object in a receptacle, use 'put down' rather than 'drop' actions. 4. **Prevent Repeating Action Sequences**: Do not repeatedly execute the same action or sequence of actions. Try to modify the action sequence because previous actions do not lead to success. 5. **Multiple Instances**: There may be multiple instances of the same object, distinguished by an index following their names, e.g., Cabinet 2, Cabinet 3. You can explore these instances if you do not find the desired object in the current receptacle. 6. **Reflection on History and Feedback**: Use interaction history and feedback from the environment to refine and improve your current plan. If the last action is invalid, reflect on the reason, such as not adhering to action rules or missing preliminary actions, and adjust your plan accordingly.You are a robot operating in a home. Given a task, you must accomplish the task using a defined set of actions to achieve the desired outcome.## Action Descriptions and Validity Rules• Navigation: Parameterized by the name of the receptacle to navigate to. So long as the receptacle is present in the scene, this skill is always valid.• Pick: Parameterized by the name of the object to pick. Only valid if the robot is close to the object, not holding another object, and the object is not inside a closed receptacle. • Place: Parameterized by the name of the receptacle to place the object on. Only valid if the robot is close to the receptacle and is holding an object. • Open: Parameterized by the name of the receptacle to open. Only valid if the receptacle is closed and the robot is close to the receptacle. • Close: Parameterized by the name of the receptacle to close. Only valid if the receptacle is open and the robot is close to the receptacle. ## The available action id (0 -69) and action names are: {SKILL SET}.Make sure to match the action name and its corresponding action id in the output. Avoid performing actions that do not meet the defined validity criteria. 4. **Prevent Repeating Action Sequences**: Do not repeatedly execute the same action or sequence of actions. Try to modify the action sequence because previous actions do not lead to success. 5. **Multiple Instances**: There may be multiple instances of the same object, distinguished by an index following their names, e.g., cabinet 2, cabinet 3. You can explore these instances if you do not find the desired object in the current receptacle. 6. **Reflection on History and Feedback**: Use interaction history and feedback from the environment to refine and enhance your current strategies and actions. If the last action is invalid, reflect on the reason, such as not adhering to action rules or missing preliminary actions, and adjust your plan accordingly. Now the human instruction is: {TASK INSTRUCTION} You are supposed to output in json. You need to describe the current visual state from the image, output your reasoning steps, and plan. At the end, output the action id (0 -69) from the available actions to execute.Prompt for EB-Navigation at step 0 ## You are a robot operating in a home. You can do various tasks and output a sequence of actions to accomplish a given task with images of your status.Locate the Target Object Type: Clearly describe the spatial location of the target object from the observation image (i.e. on the front left side, a few steps from the current standing point).2. Navigate by *** Using Move forward and Move right/left as the main strategy ***, since any point can be reached through a combination of those. When planning for movement, reason based on target object's location and obstacles around you.3. Focus on the primary goal: Only address invalid action when it blocks you from moving closer in the direction to target object. In other words, do not overly focus on correcting invalid actions when direct movement toward the target object can still bring you closer.4. *** Use Rotation Sparingly ***, only when you lose track of the target object and it's not in your view. If so, plan nothing but ONE ROTATION at a step until that object appears in your view. After the target object appears, start navigation and avoid using rotation until you lose sight of the target again.5. *** Do not complete the task too early until you can not move any closer to the object, i.e. try to be as close as possible. Now the human instruction is: {TASK INSTRUCTION}. To achieve the task, 1. Reason about the current visual state and your final goal, and 2. Reflect on the effect of previous actions. 3. Summarize how you learned from the Strategy and Examples provided. Aim for about 2 actions in this step. !!!Notice: You cannot assess the situation until the whole plan in this planning step is finished and executed, so plan accordingly. At last, output the action id(s) (0 -7) from the available actions to execute.You are a robot operating in a home. You can do various tasks and output a sequence of actions to accomplish a given task with images of your status.Locate the Target Object Type: Clearly describe the spatial location of the target object from the observation image (i.e. on the front left side, a few steps from the current standing point). 2. Navigate by *** Using Move forward and Move right/left as the main strategy ***, since any point can be reached through a combination of those. When planning for movement, reason based on target object's location and obstacles around you. 3. Focus on the primary goal: Only address invalid action when it blocks you from moving closer in the direction to target object. In other words, do not overly focus on correcting invalid actions when direct movement toward the target object can still bring you closer. 4. *** Use Rotation Sparingly ***, only when you lose track of the target object and it's not in your view. If so, plan nothing but ONE ROTATION at a step until that object appears in your view. After the target object appears, start navigation and avoid using rotation until you lose sight of the target again. 5. *** Do not complete task too early until you can not move any closer to the object, i.e. try to be as close as possible. Now the human instruction is: {TASK INSTRUCTION}. {ACTION HISTORY &amp; ENVIRONMENT FEEDBACK (if available)} To achieve the task, 1. Reason about the current visual state and your final goal, and 2. Reflect on the effect of previous actions. 3. Summarize how you learned from the Strategy and Examples provided.</figDesc><table><row><cell>2-90B-Vision-Ins</cell><cell>1.8033</cell><cell>0.0233</cell></row><row><cell>Llama-3.2-11B-Vision-Ins</cell><cell>1.6767</cell><cell>1.3233</cell></row><row><cell>InternVL2 5-78B</cell><cell>1.1467</cell><cell>0.0000</cell></row><row><cell>InternVL2 5-38B</cell><cell>2.4600</cell><cell>1.7433</cell></row><row><cell>InternVL2 5-8B</cell><cell>8.0400</cell><cell>4.3967</cell></row><row><cell>Qwen2-VL-72B-Ins</cell><cell>1.6100</cell><cell>0.0767</cell></row><row><cell>Qwen2-VL-7B-Ins</cell><cell>1.6933</cell><cell>0.7067</cell></row></table><note><p>## The available action id (0 -{len(SKILL SET) -1}) and action names are: {SKILL SET}. ## Now the human instruction is: {TASK INSTRUCTION} You are supposed to output in json. You need to describe the current visual state from the image, output your reasoning steps, and plan. At the end, output the action id (0 -{len(SKILL SET) -1}) from the available actions to execute. **Output Plan**: Avoid generating empty plan. Each plan should include no more than 20 actions. 2. **Visibility**: If an object is not currently visible, use the "Navigation" action to locate it or its receptacle before attempting other operations. 3. **Action Validity**:</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Step t-2</p><p>Step t-1</p><p>Step t</p><p>Figure <ref type="figure">9</ref>. Multi-step observation example in EB-Navigation</p><p>Step t-2</p><p>Step t-1</p><p>Step t Previous research has mainly focused on text-based in-context learning (ICL) demonstrations. In this study, we explore the impact of visual ICL for embodied agents by including image observations as part of the in-context examples for EB-Manipulation. This approach helps the model better grasp the connection between successful low-level actions and the positions of objects in the image. We provide two visual ICL examples in Figure <ref type="figure">14</ref>, where the planner receives images corresponding to the textual in-context examples. To avoid overwhelming the model with excessive visual input, we limit the number of examples to two, which might slightly reduce performance compared to the main results without visual ICL. As illustrated in Figure <ref type="figure">15</ref>, the results show that visual ICL significantly outperforms language-only ICL, with particularly impressive gains in manipulation tasks. For instance, Claude-3.5-Sonnet achieves a 16.7% improvement in performance. These findings highlight the potential of visual ICL as a promising direction for future research in vision-driven embodied agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.8. Additional Ablation Study Conclusion</head><p>Overall, our ablation studies reveal that while certain visual enhancements-such as moderate resolution increases, boundingbox detection, and visual in-context learning-can be beneficial, others-like extreme high-resolution inputs, multistep/multi-view images, or detection boxes for navigation-may have limited or even negative effects. These findings highlight that the effectiveness of visual strategies heavily depends on the specific task and how additional visual information is integrated. Future research should focus on developing more advanced fusion techniques for embodied agents to better optimize the use of diverse visual inputs from multiple images.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Error Definitions and Additional Analysis F.1. Error Type Definition</head><p>In this section, we define the types of errors and sub-errors encountered. We categorize errors into three main types: perception errors, reasoning errors, and planning errors. Each error type corresponds to a specific stage in our agent pipeline. For example, perception errors occur during the visual state description stage, reasoning errors arise in the reflection and reasoning stages, and planning errors occur during the language plan and executable plan generation stages. A detailed breakdown of sub-errors for each error type is provided in Table <ref type="table">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Error Analysis for EB-Navigation</head><p>In evaluating the performance of MLLMs on navigation tasks, we identified three main types of errors: perception errors, reasoning errors, and planning errors. These errors significantly hinder the model's ability to successfully navigate to the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. Skill Sets</head><p>Below are the skill sets for EB-ALFRED and EB-Habitat. Note that the objects for EB-ALFRED vary depending on the scene, and the example provided here is illustrative. In contrast, the skill set for EB-Habitat remains static.</p><p>The skill sets (or action spaces) for EB-Navigation and EB-Manipulation are already included in the planner input prompt. For detailed prompts, please refer to Appendix G.1.   { "language_plan": "First, locate the alarm clock in the room. Once found, carefully pick up the alarm clock. Next, identify the location of the desk lamp. After locating the desk lamp, proceed to turn it on.", "executable_plan": [ {"action_id": 57, "action_name": "find a AlarmClock" }, {"action_id": 98, "action_name": "pick up the AlarmClock" }, {"action_id": 63, "action_name": "find a DeskLamp" }, {"action_id": 151, "action_name": "turn on the DeskLamp" } ] }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>## Example 2:</head><p>Human instruction: Examine a credit card by the light of a tall lamp. Output:</p><p>{ "language_plan": "Start by locating the credit card in the room. Once you have found the credit card, proceed to pick it up. Then, search for the floor lamp and find its location. After finding the floor lamp, turn it on to illuminate the area so you can examine the credit card effectively.", "executable_plan": [ {"action_id": 56, "action_name": "find a CreditCard" }, {"action_id": 96, "action_name": "pick up the CreditCard" }, {"action_id": 27, "action_name": "find a FloorLamp" }, {"action_id": 153, "action_name": "turn on the FloorLamp" } ] }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>## Example 3:</head><p>Human instruction: Use lamp light to look at the remote. Output:</p><p>{ "language_plan": "First, locate the remote control in the room. Once the remote control is found, pick it up carefully. Next, search for the floor lamp and determine its location. After you have located the floor lamp, turn it on to provide sufficient light for examining the remote control.", "executable_plan": [ {"action_id": 68, "action_name": "find a RemoteControl" }, {"action_id": 116, "action_name": "pick up the RemoteControl" }, {"action_id": 27, "action_name": "find a FloorLamp" }, {"action_id": 153, "action_name": "turn on the FloorLamp" } ] }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In-context examples for EB-Habitat</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>## Example 1:</head><p>Human instruction: Put both a toy airplane and a bowl onto the black table. Output:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{</head><p>"language_plan": "To achieve the goal, the robot must locate the toy airplane and the bowl in the room and then move each item to the black table. There are two tables in the room, and the robot identifies table 1 as the black table. Therefore, the objective is to place both objects on table 1. The plan is as follows: first, navigate to the sofa, pick up the airplane, move to table 1, and place the airplane there. Then, proceed to table 2, where the bowl might be, pick up the bowl, return to table 1, and set the bowl there.", "executable_plan": [ {"action_id": 12, "action_name": "navigate to the sofa" }, {"action_id": 47, "action_name": "pick up the toy airplane" }, {"action_id": 6, "action_name": "navigate to table 1" }, {"action_id": 50, "action_name": "place at table 1" }, {"action_id": 7, "action_name": "navigate to table 2" }, {"action_id": 42, "action_name": "pick up the bowl" }, {"action_id": 6, "action_name": "navigate to table 1" }, {"action_id": 50, "action_name": "place at { "language_plan": "The objective is for the robot to close the refrigerator. To do so, the robot first navigates to the refrigerator and then closes it.", "executable_plan": [ {"action_id": 13, "action_name": "navigate to the refrigerator" }, {"action_id": 61, "action_name": "close the refrigerator" } ] }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>## Example 3:</head><p>Human instruction: Displace the sponge from the left counter. Output:</p><p>{ "language_plan": "To accomplish this task, the robot should navigate to the left counter to pick up the sponge. It should then move to another location, such as table 2, and place the sponge there.", "executable_plan": [ {"action_id": 11, "action_name": "navigate to the left counter in the kitchen"}, {"action_id": 28, "action_name": "pick up the sponge" }, {"action_id": 7, "action_name": "navigate to table 2" }, {"action_id": 51, "action_name": "place at  <ref type="bibr">[39,</ref><ref type="bibr">73,</ref><ref type="bibr">18]</ref>.", "reasoning_and_reflection": "I need to understand the instruction first. To stack the objects in sequence, the red star at <ref type="bibr">[39,</ref><ref type="bibr">73,</ref><ref type="bibr">18</ref>] should be at the bottom and the olive star at <ref type="bibr">[54,</ref><ref type="bibr">53,</ref><ref type="bibr">19</ref>] should be placed on top. The plan involves moving the gripper to the olive star's position, closing the gripper to pick it up, moving to the top of the red star, and opening the gripper to release the olive star onto the red star.", "executable_plan": <ref type="bibr">[ [54, 51, 28, 3, 66, 37, 1]</ref>, <ref type="bibr">[54,</ref><ref type="bibr">54,</ref><ref type="bibr">20,</ref><ref type="bibr">3,</ref><ref type="bibr">66,</ref><ref type="bibr">37,</ref><ref type="bibr">0]</ref>, <ref type="bibr">[54,</ref><ref type="bibr">51,</ref><ref type="bibr">30,</ref><ref type="bibr">3,</ref><ref type="bibr">66,</ref><ref type="bibr">37,</ref><ref type="bibr">0]</ref>, <ref type="bibr">[39,</ref><ref type="bibr">74,</ref><ref type="bibr">29,</ref><ref type="bibr">3,</ref><ref type="bibr">66,</ref><ref type="bibr">7,</ref><ref type="bibr">0]</ref>, <ref type="bibr">[39,</ref><ref type="bibr">74,</ref><ref type="bibr">26,</ref><ref type="bibr">3,</ref><ref type="bibr">66,</ref><ref type="bibr">7</ref>, 1] ] }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In-context examples for EB-Manipulation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>## Example 3:</head><p>Human instruction: Put the red star into the shape sorter. Input: {'object 1': <ref type="bibr">[65,</ref><ref type="bibr" target="#b112">11,</ref><ref type="bibr">17]</ref>, 'object 2': <ref type="bibr">[50,</ref><ref type="bibr">38,</ref><ref type="bibr">17]</ref>, 'object 3': <ref type="bibr">[40,</ref><ref type="bibr">64,</ref><ref type="bibr">37]</ref>, 'object 4': <ref type="bibr">[45,</ref><ref type="bibr">83,</ref><ref type="bibr">17]</ref>} Output:</p><p>{ "visual_state_description": "From left to right, I can see a purple star at <ref type="bibr">[65,</ref><ref type="bibr" target="#b112">11,</ref><ref type="bibr">17]</ref>, a red star at <ref type="bibr">[50,</ref><ref type="bibr">38,</ref><ref type="bibr">17]</ref>, a shape sorter at <ref type="bibr">[40,</ref><ref type="bibr">64,</ref><ref type="bibr">37</ref>] and a green star at <ref type="bibr">[45,</ref><ref type="bibr">83,</ref><ref type="bibr">17]</ref>.", "reasoning_and_reflection": "I need to identify the location of the star hole on the shape sorter first. I can see that the star hole is at the center of the shape sorter, which is at <ref type="bibr">[40,</ref><ref type="bibr">64,</ref><ref type="bibr">37]</ref>. Then, the task is to pick up the red star at <ref type="bibr">[50,</ref><ref type="bibr">38,</ref><ref type="bibr">17]</ref> and put it into the star hole on the shape sorter. The plan involves moving the gripper to the star's position, closing the gripper to pick it up, moving upwards, moving to the top of the star hole, and releasing the star into the hole.", "executable_plan": <ref type="bibr">[ [51, 38, 25, 119, 59, 26, 1]</ref>, <ref type="bibr">[51,</ref><ref type="bibr">38,</ref><ref type="bibr">17,</ref><ref type="bibr">119,</ref><ref type="bibr">59,</ref><ref type="bibr">26,</ref><ref type="bibr">0]</ref>, <ref type="bibr">[51,</ref><ref type="bibr">38,</ref><ref type="bibr">27,</ref><ref type="bibr">119,</ref><ref type="bibr">59,</ref><ref type="bibr">26</ref> { "visual_state_description": "From left to right, I can see a yellow sponge at <ref type="bibr">[33,</ref><ref type="bibr">36,</ref><ref type="bibr">19]</ref>, a green rectangle area at <ref type="bibr">[52,</ref><ref type="bibr">38,</ref><ref type="bibr">15</ref>] and an orange rectangle area at <ref type="bibr">[59,</ref><ref type="bibr">68,</ref><ref type="bibr">15]</ref>.", "reasoning_and_reflection": "I need to identify which area is horizontal first. I can see that the orange rectangle area at <ref type="bibr">[59,</ref><ref type="bibr">68,</ref><ref type="bibr">15]</ref> is horizontal since it is parallel to the green arrow representing the y-axis of the coordinate system annotated in the image. Then, the task is to pick up the sponge at <ref type="bibr">[33,</ref><ref type="bibr">36,</ref><ref type="bibr">19]</ref> and use it to wipe the horizontal area at <ref type="bibr">[59,</ref><ref type="bibr">68,</ref><ref type="bibr">15]</ref>. The plan involves moving the gripper to the sponge's position, closing the gripper to pick it up, moving to the side of the horizontal area, and moving the sponge along the main direction of the horizontal area to wipe it.", "executable_plan": <ref type="bibr">[ [32, 34, 25, 0, 60, 34</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4. Output JSON Schema</head><p>Below are the JSON schemas that guide the output structure of MLLMs.</p><p>Output JSON Schema for EB-ALFRED, EB-Habitat, and EB-Navigation { "type": "object", "properties": { "visual_state_description": { "type": "string", "description": "Description of current state from the visual image" }, "reasoning_and_reflection": { "type": "string", "description": "Summarize the history of interactions and any available environmental feedback. Additionally, provide reasoning as to why the last action or plan failed and did not finish the task." }, "language_plan": { "type": "string", "description": "The list of actions to achieve the user instruction. Each action is started by the step number and the action name." }, "executable_plan": { "type": "array", "description": "A list of actions needed to achieve the user instruction, with each action having an action ID and a name. Do not output an empty list." , "items": { "type": "object", "properties": { "action_id": { "type": "integer", "description": "The action ID to select from the available actions given by the prompt" }, "action_name": { "type": "string", "description": "The name of the action" } }, "required": ["action_id", "action_name"] } } }, "required": [ "visual_state_description", "reasoning_and_reflection", "language_plan", "executable_plan" ] }</p><p>Output JSON Schema for EB-Manipulation { "type": "object", "properties": { "visual_state_description": { "type": "string", "description": "Describe the color and shape of each object in the detection box in numerical order in the image. Then provide the 3D coordinates of the objects chosen from input." }, "reasoning_and_reflection": { "type": "string", "description": "Reason about the overall plan that needs to be taken on the target objects, and reflect on the previous actions taken if available." }, "language_plan": { "type": "string", "description": "A list of natural language actions to achieve the user instruction. Each language action is started by the step number and the language action name." }, "executable_plan": { "type": "array", "description": "A list of discrete actions needed to achieve the user instruction, with each discrete action being a 7-dimensional discrete action.", "items": { "type": "object", "properties": { "action": { "type": "string", "description": "The 7-dimensional discrete action in the format of a list given by the prompt." } }, "required": ["action"] } } }, "required": [ "visual_state_description", "reasoning_and_reflection", "language_plan", "executable_plan" ] }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Supplementary Case Studies of Successful Planning</head><p>In this section, we present successful planning examples for Claude-3.5-Sonnet, InternVL2.5-78B, GPT-4o, and Gemini-1.5-pro across EB-ALFRED, EB-Habitat, EB-Navigation, and EB-Manipulation. Refer to <ref type="bibr">Figures 17,</ref><ref type="bibr">18,</ref><ref type="bibr">19,</ref><ref type="bibr">and 20</ref> for detailed reasoning and planning.</p><p>Human Instruction: slice a tomato, heat it up in the microwave, and place it in the sink. Visual state description: The image shows a kitchen environment with white cabinets and a stove. There are two copper-colored pots visible on the stovetop. The countertop appears to be white, and there's a microwave mounted above. The floor appears to be a beige or brown color.</p><p>Reasoning &amp; Reflection: Based on the visible kitchen environment, we need to execute a sequence of actions to slice, heat, and move a tomato. We'll need to first locate necessary tools like a knife, then find a tomato to slice it. After slicing, we'll need to heat it in the visible microwave, and finally transfer it to a sink. From previous examples, I understand we need to put down tools like knives before picking up food items for safety.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Planning Step 1 (env step 1):</head><p>Visual state description: The image shows a kitchen environment with white cabinets and a stove. There are two copper-colored pots visible on the stovetop. The countertop appears to be white, and there's a microwave mounted above. The floor appears to be a beige or brown color.</p><p>Reasoning &amp; Reflection: Based on the visible kitchen environment, we need to locate the necessary tools and ingredients first. We'll need a knife to slice the tomato, then use the microwave that's visible in the scene, and finally locate the sink for the final placement. The plan needs to follow the sequence of first preparing the tomato, then heating it, and finally transferring it to its destination. Human Instruction: navigate to the Pot in the room and be as close as possible to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Planning Step 1 (env step 1 -env step 2):</head><p>The pot is located on the countertop to the front left of my current position. I can approach it by moving forward and leftward. I will start with moving forward towards the pot since the path ahead seems clear."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Planning</head><p>Step 2 (env step 3 -env step 5):</p><p>I moved forward successfully, which brought me closer to the pot. The pot is now more directly in front and slightly to the left. The approach has been effective so far, following the planned movements, without any obstacles blocking my path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Planning Step 3 (env step 6):</head><p>I've successfully moved closer to the pot. It's now directly in front of the left countertop but I haven't reached the closest proximity possible. Continuing to adjust my position slightly leftward and forward will get me as close as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Planning</head><p>Step 4 (env step 7 -env step 8):</p><p>I've moved forward and left to get closer to the pot, but my last left movement was blocked by the baseboard. This suggests I'm near an edge or obstacle. I can still move forward to get closer to the pot since it's in front.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Planning</head><p>Step 5 (env step 9 -env step 11):</p><p>I managed to move forward but my sideward movements were blocked by the baseboard, indicating a limitation at this spot. Further approaches will be more successful by moving forward and then attempting lateral moves to align with the pot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Planning</head><p>Step 6 (env step 12 -env step 16):</p><p>I've moved forward several times towards where the pot was located. Despite attempts to move left to get closer, these actions were repeatedly blocked by obstacles. This indicates a misalignment with the countertop or edge preventing approach. Since I've lost sight of the pot, rotation is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Planning Step 7 (step 17):</head><p>After rotating, the pot appeared on the counter. I've moved forward and then left to approach it. I can now see it clearly in my current position, slightly ahead. 1. <ref type="bibr">[47,</ref><ref type="bibr">69,</ref><ref type="bibr">26,</ref><ref type="bibr">0,</ref><ref type="bibr">60,</ref><ref type="bibr">0,</ref><ref type="bibr">1]</ref> 2. <ref type="bibr">[47,</ref><ref type="bibr">69,</ref><ref type="bibr">18,</ref><ref type="bibr">0,</ref><ref type="bibr">60,</ref><ref type="bibr">0,</ref><ref type="bibr">0]</ref> 3. <ref type="bibr">[47,</ref><ref type="bibr">69,</ref><ref type="bibr">28,</ref><ref type="bibr">0,</ref><ref type="bibr">60,</ref><ref type="bibr">0,</ref><ref type="bibr">0]</ref> 4. <ref type="bibr">[50,</ref><ref type="bibr">89,</ref><ref type="bibr">28,</ref><ref type="bibr">0,</ref><ref type="bibr">60,</ref><ref type="bibr">0,</ref><ref type="bibr">0]</ref> 5. <ref type="bibr">[50,</ref><ref type="bibr">89,</ref><ref type="bibr">25,</ref><ref type="bibr">0,</ref><ref type="bibr">60,</ref><ref type="bibr">0,</ref><ref type="bibr">1]</ref> 6. <ref type="bibr">[33,</ref><ref type="bibr">65,</ref><ref type="bibr">27,</ref><ref type="bibr">0,</ref><ref type="bibr">60,</ref><ref type="bibr">0,</ref><ref type="bibr">1]</ref> 7. <ref type="bibr">[33,</ref><ref type="bibr">65,</ref><ref type="bibr">19,</ref><ref type="bibr">0,</ref><ref type="bibr">60,</ref><ref type="bibr">0,</ref><ref type="bibr">0]</ref> 8. <ref type="bibr">[33,</ref><ref type="bibr">65,</ref><ref type="bibr">29,</ref><ref type="bibr">0,</ref><ref type="bibr">60,</ref><ref type="bibr">0,</ref><ref type="bibr">0]</ref> 9. <ref type="bibr">[51,</ref><ref type="bibr">90,</ref><ref type="bibr">29,</ref><ref type="bibr">0,</ref><ref type="bibr">60,</ref><ref type="bibr">0,</ref><ref type="bibr">0]</ref> 10. <ref type="bibr">[51,</ref><ref type="bibr">90,</ref><ref type="bibr">21,</ref><ref type="bibr">0,</ref><ref type="bibr">60,</ref><ref type="bibr">0,</ref><ref type="bibr">1]</ref> Planning Step 1 (env step 1 -env step 5):</p><p>Visual state description: From left to right, I can see a magenta cylinder at <ref type="bibr">[47,</ref><ref type="bibr">31,</ref><ref type="bibr">20]</ref>, a gray gear at <ref type="bibr">[33,</ref><ref type="bibr">64,</ref><ref type="bibr">19]</ref>, a yellow star at <ref type="bibr">[47,</ref><ref type="bibr">69,</ref><ref type="bibr">18]</ref>, and a navy cylinder at <ref type="bibr">[50,</ref><ref type="bibr">89,</ref><ref type="bibr">20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reasoning &amp; Reflection:</head><p>The instruction is to stack the front star on top of the right cylinder. The yellow star at <ref type="bibr">[47,</ref><ref type="bibr">69,</ref><ref type="bibr">18]</ref> should be picked up and placed on the navy cylinder at <ref type="bibr">[50,</ref><ref type="bibr">89,</ref><ref type="bibr">20]</ref>. The plan involves moving the gripper to the yellow star's position, closing the gripper to pick it up, moving to the top of the navy cylinder, and opening the gripper to release the yellow star onto the navy cylinder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Planning</head><p>Step 2 (env step 6 -env step 10):</p><p>Visual state description: From left to right, I can see a magenta cylinder at <ref type="bibr">[47,</ref><ref type="bibr">31,</ref><ref type="bibr">20]</ref>, a gray star at <ref type="bibr">[33,</ref><ref type="bibr">65,</ref><ref type="bibr">19]</ref>, a yellow star at <ref type="bibr">[50,</ref><ref type="bibr">88,</ref><ref type="bibr">25]</ref>, and a blue cylinder at <ref type="bibr">[51,</ref><ref type="bibr">90,</ref><ref type="bibr">20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reasoning &amp; Reflection:</head><p>The task is to stack the front star on top of the right cylinder. The front star is the gray star at <ref type="bibr">[33,</ref><ref type="bibr">65,</ref><ref type="bibr">19]</ref>, and the right cylinder is the blue cylinder at <ref type="bibr">[51,</ref><ref type="bibr">90,</ref><ref type="bibr">20]</ref>. The plan involves moving the gripper to the gray star's position, closing the gripper to pick it up, moving to the top of the blue cylinder, and opening the gripper to release the gray star onto the blue cylinder. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Supplementary Case Studies of Unsuccessful Planning</head><p>We also present failure cases across different tasks, highlighting key challenges in perception, reasoning, and planning. In EB-ALFRED, we showcase a planning error where the model fails to generate an effective executable plan to accomplish the task. In EB-Manipulation, we analyze a perception error where the agent misidentifies key objects, leading to incorrect execution. In EB-Navigation, we illustrate a reasoning error where the model struggles to interpret spatial relationships. These examples provide insights into the limitations of current models and highlight areas for improvement in object recognition, planning, and spatial reasoning. Refer to Figures 21, 22, and 23 for details.</p><p>Human Instruction: move two books from the bed to the desk. Visual state description: The image shows a room corner with a red trash bin, a light switch on the wall, and a closed black door. The floor is carpeted, and there is a small part of a table visible on the left.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reasoning &amp; Reflection:</head><p>The task is to move two books from the bed to the desk. However, the current image does not show a bed, books, or a desk. Therefore, the first step is to locate these objects. The plan will involve finding the bed, picking up the books, finding the desk, and placing the books on it.</p><p>Language Plan: First, locate the bed in the room. Once the bed is found, identify and pick up the first book. Then, locate the desk and place the book on it. Repeat the process for the second book by returning to the bed, picking up the second book, and placing it on the desk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Executable Plan:</head><p>Planning Step 2 (env step 11):</p><p>Visual state description: The image shows a black desk with a lamp, a mug, a pen, a keychain, a book titled 'Clean Code', and three tissue boxes. There is a chair next to the desk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reasoning &amp; Reflection:</head><p>The task was to move two books from the bed to the desk. The previous actions successfully moved two books to the desk. The task is complete as per the instruction, and no further actions are needed.</p><p>Language Plan: The task of moving two books from the bed to the desk has been completed successfully. No further actions are required.</p><p>{"action_id":32, "action_name": "find a Bed"}, {"action_id":54, "action_name": "find a Book"}, {"action_id":94, "action_name": "pick up the Book"}, {"action_id":49, "action_name": "find a Desk"}, {"action_id":133, "action_name": "put down the object in hand"}, {"action_id":32, "action_name": "find a Bed"}, {"action_id":54, "action_name": "find a Book"}, {"action_id":94, "action_name": "pick up the Book"}, {"action_id":49, "action_name": "find a Desk"}, {"action_id":133, "action_name": "put down the object in hand"} Figure <ref type="figure">21</ref>. Planning Error Example in EB-ALFRED: The agent was supposed to locate "Book 2" by the 7th action but instead continued interacting with the first book.</p><p>Human Instruction: navigate to the Pot in the room and be as close as possible to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Planning Step 1 (env step 1 -env step 2):</head><p>My goal is to locate the pot in this kitchen. Given the current view, there's no pot visible directly. I should move forward to explore more of my surroundings, as moving forward will help me view more of the countertop and other areas not fully visible right now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Planning</head><p>Step 4 (env step 6):</p><p>The previous action sequence of moving forward eventually led to an invalid move due to being blocked by the countertop. This indicates proximity to the target. The pot is now in front and a bit to the left, requiring leftward and forward movements to get closer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Planning Step 13 (env step 19 -env step 20):</head><p>The pot is visible now, slightly to the right. Previous attempts resulted in being blocked by the counter. In order to move closer, we should navigate around obstacles and ensure our path aligns more effectively towards the target, adjusting the direction as needed to avoid obstructions. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anadkat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Cosmos world foundation model platform for physical ai</title>
		<author>
			<persName><forename type="first">N</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.03575</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hausman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.01691</idno>
		<title level="m">Do as i can, not as i say: Grounding language in robotic affordances</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compositional foundation models for hierarchical planning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ajay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="22304" to="22325" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Claude 3.5 sonnet</title>
		<author>
			<persName><surname>Anthropic</surname></persName>
		</author>
		<ptr target="https://www.anthropic.com/news/claude-3-5-sonnet" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Belkhale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sadigh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.01823</idno>
		<title level="m">Rt-h: Action hierarchies using language</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rt-1: Robotics transformer for real-world control at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbajal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.06817</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rt-2: Vision-language-action models transfer web knowledge to robotic control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbajal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Driess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.15818</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS&apos;20</title>
		<meeting>the 34th International Conference on Neural Information Processing Systems, NIPS&apos;20<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The ycb object and model set: Towards common benchmarks for manipulation research</title>
		<author>
			<persName><forename type="first">B</forename><surname>Calli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 international conference on advanced robotics (ICAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="510" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clegg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Cote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hlavac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Karashchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krantz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Parashar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.00081</idno>
		<title level="m">A benchmark for planning and reasoning in embodied multi-agent tasks</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards benchmarking robustness in embodied navigation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><surname>Robustnav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15691" to="15700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Endowing vision-language models with spatial reasoning capabilities</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kirmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sadigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><surname>Spatialvlm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="14455" to="14465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.15649</idno>
		<title level="m">Robogpt: an intelligent agent of making embodied long-term decisions for daily instruction tasks</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual structural knowledge extraction via curriculum guided code-vision representation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vistruct</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. The 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>The 2023 Conference on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Solo: A single transformer for scalable vision-language modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Internvl: Scaling up vision foundation models and aligning for generic visuallinguistic tasks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.14238</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2412.05271" />
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">A.-C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Spatialrgpt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.01584</idno>
		<title level="m">Grounded spatial reasoning in vision language model</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.11858</idno>
		<title level="m">Evaluate multimodal llms as embodied agents</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cousineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Burchfiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tedrake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<title level="m">Diffusion policy: Visuomotor policy learning via action diffusion</title>
		<imprint/>
	</monogr>
	<note type="report_type">The International</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Goat-bench: A benchmark for multi-modal lifelong navigation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramrakhya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yenamandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gervet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="16373" to="16383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pertsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karamcheti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balakrishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sanketi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.09246</idno>
		<title level="m">An open-source vision-languageaction model</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Duvvur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><surname>Visualwebarena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.13649</idno>
		<title level="m">Evaluating multimodal agents on realistic visual web tasks</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vanderbilt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Weihs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herrasti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deitke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ehsani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05474</idno>
		<title level="m">An interactive 3d environment for visual ai</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">igibson 2.0: Object-centric simulation for robot learning of everyday household tasks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lingelbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vainio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gokmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.03272</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gokmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lingelbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="80" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Muep: A multimodal benchmark for embodied planning with foundation models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2024/15</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2024/15.MainTrack" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Larson</surname></persName>
		</editor>
		<meeting>the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gokmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.07166</idno>
		<title level="m">Embodied agent interface: Benchmarking llms for embodied decision making</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Code as policies: Language model programs for embodied control</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="9493" to="9500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="https://llava-vl.github.io/blog/2024-01-30-llava-next/" />
		<title level="m">Llava-next: Improved reasoning, ocr, and world knowledge</title>
		<imprint>
			<date type="published" when="2024-01">January 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A language first approach for procedural planning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. The 61st Annual Meeting of the Association for Computational Linguistics (ACL2023) Findings</title>
		<meeting>The 61st Annual Meeting of the Association for Computational Linguistics (ACL2023) Findings</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploring the robustness of decision-level through adversarial attacks on llm-based embodied models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd ACM International Conference on Multimedia</title>
		<meeting>the 32nd ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8120" to="8128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Rdt-1b: a diffusion foundation model for bimanual manipulation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.07864</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rigidbody physics-grounded image-to-video generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Physgen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="360" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.03688</idno>
		<title level="m">Evaluating llms as agents</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Heterogeneous embodied multi-agent collaboration</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">L</forename><surname>Iong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.06327</idno>
		<title level="m">Visualagentbench: Towards large multimodal models as visual foundation agents</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fmb: a functional manipulation benchmark for generalizable robotic learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="page">02783649241276017</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Lampilot: An open benchmark dataset for autonomous driving with language model programs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdelraouf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="15141" to="15151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A survey on vision-language-action models for embodied ai</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.14093</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Møgelmose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.03770</idno>
		<title level="m">Foundation models for video understanding: A survey</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Gpt-Driver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01415</idno>
		<title level="m">Learning to drive with gpt</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Mazzaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Verbelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dhoedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName><surname>Genrl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.18043</idno>
		<title level="m">Multimodal-foundation world models for generalization in embodied agents</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Llama 3.2: Revolutionizing edge ai and vision with open, customizable models</title>
		<ptr target="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/" />
	</analytic>
	<monogr>
		<title level="j">Meta</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Embodiedgpt: Vision-language pre-training via embodied chain of thought</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Largescale simulation of everyday tasks for generalist robots</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nasiriany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maddukuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mandlekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><surname>Robocasa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.02523</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Nasiriany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Driess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.07872</idno>
		<title level="m">Iterative visual prompting elicits actionable knowledge for vlms</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Hello gpt-4o</title>
		<ptr target="https://openai.com/index/hello-gpt-4o/" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
		<respStmt>
			<orgName>OpenAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Gpt-4o mini: advancing cost-efficient intelligence</title>
		<ptr target="https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
		<respStmt>
			<orgName>OpenAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Simulating household activities via programs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><surname>Virtualhome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8494" to="8502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Pushing language models to think outside the box</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Escapebench</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>In arxiv</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Sayplan: Grounding large language models using 3d scene graphs for scalable task planning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haviland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Abou-Chakra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Suenderhauf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</title>
		<author>
			<persName><forename type="first">M</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.05530</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">V-rep: A versatile and scalable robot simulation framework</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rohmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Freese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE/RSJ international conference on intelligent robots and systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1321" to="1326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Helper-x: A unified instructable embodied agent to tackle four interactive vision-language domains with memory-augmented language models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sarch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Somani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Tarr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.19065</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Vlm agents generate their own memories: Distilling experience into embodied programs of thought</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Sarch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Tarr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eighth Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hoque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.12915</idno>
		<title level="m">Semantic mechanical search with large vision and language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">igibson 1.0: A simulation environment for interactive tasks in large realistic scenes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pérez-D'arpino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tchapmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7520" to="7527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Alfred: A benchmark for interpreting grounded instructions for everyday tasks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10740" to="10749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Alfworld: Aligning text and embodied environments for interactive learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Cote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Cliport: What and where pathways for robotic manipulation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Manuelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on robot learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="894" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Progprompt: Generating situated robot task plans using large language models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Blukis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<biblScope unit="page" from="11523" to="11530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Llm-planner: Few-shot grounded planning for embodied agents with large language models</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Washington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2998" to="3009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.09082</idno>
		<title level="m">Towards long-horizon vision-language navigation: Platform, benchmark and method</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Open-world object manipulation using pre-trained vision-language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kirmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zitkovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.00905</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Reinforcement learning in the era of llms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06147</idno>
	</analytic>
	<monogr>
		<title level="m">What is essential? what is needed? an rl perspective on rlhf, prompting, and beyond</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Habitat 2.0: Training home assistants to rearrange their habitat</title>
		<author>
			<persName><forename type="first">A</forename><surname>Szot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clegg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Undersander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maestre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mukadam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Maksymets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="251" to="266" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Large language models as generalizable policies for embodied tasks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Szot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schwarzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mazoure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Metcalf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Talbott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mackraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Toshev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Szot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mazoure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Attia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.08442</idno>
		<title level="m">From multimodal llms to generalist embodied agents: Methods and lessons</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11805</idno>
		<title level="m">a family of highly capable multimodal models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</title>
		<author>
			<persName><forename type="first">G</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Burnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tanzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.05530</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Walke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pertsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dasari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hejna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.12213</idno>
		<title level="m">An open-source generalist robot policy</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and finetuned chat models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mandlekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><surname>Voyager</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.16291</idno>
		<title level="m">An openended embodied agent with large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Qwen2-vl: Enhancing vision-language model&apos;s perception of the world at any resolution</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.12191</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Multimedia generative script learning for task planning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Girish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. The 61st Annual Meeting of the Association for Computational Linguistics (ACL2023) Findings</title>
		<meeting>The 61st Annual Meeting of the Association for Computational Linguistics (ACL2023) Findings</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Robogen: Towards unleashing infinite data for automated robot learning via generative simulation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.01455</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Paxion: Patching video-language foundation models with action knowledge</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2023 Conference on Neural Information Processing Systems</title>
		<meeting>2023 Conference on Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2023">NeurIPS2023</date>
		</imprint>
	</monogr>
	<note>Spotlight Paper], 2023d</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Describe, explain, plan and select: Interactive planning with large language models enables open-world multitask agents</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.01560</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Dissecting adversarial robustness of multimodal lm agents</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2024 Workshop on Open-World Agents</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Deepseek-vl2: Mixtureof-experts vision-language models for advanced multimodal understanding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.10302</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Sapien: A simulated part-based interactive environment</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11097" to="11107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.09455</idno>
		<title level="m">Towards general world model with natural language actions and video states</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Robotic skill acquisition via instruction augmentation with visionlanguage models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.11736</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Large multimodal agents: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.15116</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Robust decision transformer: Tackling data corruption in offline rl via sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.04285</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.15115</idno>
		<title level="m">Qwen2. 5 technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.11441</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.12955</idno>
		<title level="m">Towards robust offline reinforcement learning under diverse data corruption</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Regularizing hidden states enables learning generalizable reward model for llms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.10216</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Rewards-in-context: Multi-objective alignment of foundation models with dynamic preference adjustment</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.10207</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Embodied multi-modal agent trained by an llm from a parallel textworld</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="26275" to="26285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">React: Synergizing reasoning and acting in language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Herzig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.12782</idno>
		<title level="m">-context learning enables robot action prediction in llms</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Robotic control via embodied chain-of-thought reasoning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zawalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pertsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.08693</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Vlabench: A large-scale benchmark for language-conditioned robotics manipulation with long-horizon reasoning tasks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.18194</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">On the out-of-distribution generalization of multimodal large language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.06599</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Vlmbench: A compositional benchmark for vision-andlanguage manipulation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">C</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="665" to="678" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Explicit reasoning in vision-and-language navigation with large language models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Navgpt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="7641" to="7649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Visual in-context learning for large vision-language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.11574</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Dynamath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.00836</idno>
		<title level="m">A dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Reasoning and reflection: I can see the floorlmap is at left side in front of me, next to the TVStand. Around me is a Cabinet at my left side. Following the guideline, since the floorlamp is at left side, I can approach it by a series of moving forward and left / rightward. Since currently there is an obstacle at my left, I will first move forward twice</title>
	</analytic>
	<monogr>
		<title level="m"># Example 1: Human Instruction : Navigate to the floorlamp and be as close as possible</title>
		<imprint/>
	</monogr>
	<note>Executable plans: [[0, Move forward by 0.25], [0, Move forward by 0</note>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Environment Feedback: Step 1 is executed successfully; Step 2 is executed successfully; Reasoning and reflection: I can see now I get closer to the floorlamp. It is still located in front of me at the left. I will try move forward and move leftward in series</title>
		<imprint/>
	</monogr>
	<note>Executable plans: [[0, Move forward by 0.25], [3, Move leftward by 0.25], [0, Move forward by 0.25], [3, Move leftward by 0.25], [0, Move forward by 0</note>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">It is still in front of me. However, the last action is invalid, showing that I am blocked from moving forward. That means there is an obstacle in front of me and I can see it is probably a small table. Following the strategy, I can get away by first moving to the left and move forward</title>
		<imprint/>
	</monogr>
	<note>Environment Feedback: Step 3 is executed successfully; Step 4 is executed successfully Step 5 is executed successfully; Step 6 is executed successfully; Step 7 is Invalid, as a StandardIslandHeight is blocking you from moving forward. Reasoning and reflection: I am a lot closer to the floorlamp by a series a forward and leftward movement Executable plans 3, Move leftward by 0.25], [3, Move leftward by 0.25], [0, Move forward by 0.25], [0, Move forward by 0</note>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Step 10 is executed successfully; Step 11 is executed successfully; Reasoning and reflection: I can see that the floorlamp is at the right side of me, slight in the front. I think it in effect of consecutively moving leftward. Also, i have got away with the obstable. Now I need to move right and I will probably be close to the floorlamp</title>
		<imprint/>
	</monogr>
	<note>Environment Feedback: Step 8 is executed successfully Step 9 is executed successfully Executable plans: [[2, Move rightward by 0.25], [2, Move rightward by 0</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Environment Feedback: Step 12 is executed successfully; Step 13 is executed successfully. Reasoning and reflection: Now I can see I am almost standing right next to the floorlamp, I can complete the task</title>
		<imprint/>
	</monogr>
	<note>Executable plans: [[8, Complete the current task</note>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Task Success 1. Move forward by 0.25 2. Move forward by 0.25 3. Move forward by 0</title>
		<imprint/>
	</monogr>
	<note>25 4. Move leftward by 0.25 5. Move forward by 0.25 6. Move leftward by 0.25</note>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Move forward by 0.25 14. Move forward by 0.25 15. Move forward by 0.25 16. Move leftward by 0.25 17</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Human Instruction: Pick up the star and place it into the blue container. Planning Step 1 (env step 1 -env step 5): Visual state description: From left to right, I can see a magenta cylinder at</title>
		<imprint>
			<date>18, 17, 0, 60, 90, 1] 12. [66, 18, 17, 0, 60, 90, 0] 13. [66, 18, 27, 0, 60, 90, 0] 14. [26, 14, 27, 0, 60, 90, 0] 15. [26, 14, 27, 0, 60, 90, 1</date>
			<biblScope unit="volume">66</biblScope>
		</imprint>
	</monogr>
	<note>34, 55, 17 and an olive container at [57, 63, 17</note>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Reasoning &amp; Reflection: The task is to pick up the star located at [34, 55, 17] and place it into the blue container located at [68, 17, 17]. The plan involves moving the gripper to the star&apos;s position, closing the gripper to pick it up, moving to the blue container&apos;s position, and opening the gripper to release the star into the container. Planning Step 2 (env step 6 -env step 10): Visual state description: From left to right, I can see a pink star at</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
	<note>a blue container at [67, 18, 17], and a green container at [57, 63, 17</note>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Reasoning &amp; Reflection: The task is to pick up the star located at [26, 16, 17] and place it into the blue container located at [67, 18, 17]. The plan involves moving the gripper to the star&apos;s position, closing the gripper to pick it up, moving to the blue container&apos;s position, and opening the gripper to release the star into the container</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
	<note>Planning Step 3 (env step 11 -env step 15): Visual state description: From left to right, I can see a blue container at a purple star at [66, 18, 17], and a green container at [57, 63, 17</note>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Reasoning &amp; Reflection: The task is to pick up the star located at [66, 18, 17] and place it into the blue container located at [26, 14, 17]. The plan involves moving the gripper to the star&apos;s position, closing the gripper to pick it up, moving to the blue container&apos;s position</title>
		<imprint/>
	</monogr>
	<note>and opening the gripper to release the star into the container</note>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Perception Error Example in EB-Manipulation: the agent erroneously observed the color of the object</title>
		<imprint/>
	</monogr>
	<note>Figure 22</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
