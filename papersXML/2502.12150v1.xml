<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Idiosyncrasies in Large Language Models</title>
				<funder>
					<orgName type="full">Bosch Center for Artificial Intelligence</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2025-02-17">17 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yida</forename><surname>Yin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhiqiu</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Zico Kolter</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
						</author>
						<title level="a" type="main">Idiosyncrasies in Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-17">17 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">7E01516F161044793ACFE520FB7FAC12</idno>
					<idno type="arXiv">arXiv:2502.12150v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-03-04T12:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we unveil and study idiosyncrasies in Large Language Models (LLMs) -unique patterns in their outputs that can be used to distinguish the models. To do so, we consider a simple classification task: given a particular text output, the objective is to predict the source LLM that generates the text. We evaluate this synthetic task across various groups of LLMs and find that simply fine-tuning existing text embedding models on LLM-generated texts yields excellent classification accuracy. Notably, we achieve 97.1% accuracy on held-out validation data in the fiveway classification problem involving ChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals that these idiosyncrasies are rooted in word-level distributions. These patterns persist even when the texts are rewritten, translated, or summarized by an external LLM, suggesting that they are also encoded in the semantic content. Additionally, we leverage LLM as judges to generate detailed, open-ended descriptions of each model's idiosyncrasies. Finally, we discuss the broader implications of our findings, particularly for training on synthetic data and inferring model similarity. Code is available at github.com/locuslab/llm-idiosyncrasies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As the adoption of generative models such as LLMs accelerates, it becomes increasingly important to understand the origin and provenance of such generated content. While a great deal of past work has focused on the classification of human-written and AI-written content <ref type="bibr" target="#b16">(Jawahar et al., 2020;</ref><ref type="bibr" target="#b18">Krishna et al., 2023;</ref><ref type="bibr" target="#b25">Mitchell et al., 2023;</ref><ref type="bibr" target="#b34">Sadasivan et al., 2025)</ref>, there has been little work on classifying between content generated by different LLMs, either between the outputs of entirely different models or between those of * Equal contribution 1 Carnegie Mellon University 2 UC Berkeley 3 University of Pennsylvania 4 Princeton University. Correspondence to: Mingjie Sun &lt;mingjies@cs.cmu.edu&gt;, Yida Yin &lt;davidyinyida0609@berkeley.edu&gt;.</p><p>different variants of the same model family. If possible, the ability to distinguish between source models in this manner would be valuable for a number of applications: it could shed light on the relative uptake of different LLMs, beyond what is reported by individual companies, and on the nature of data used to build different models. Additionally, it could offer insights into what features the output generation are most "unique" to each LLM.</p><p>In this paper, we investigate whether LLMs exhibit idiosyncrasies that enable their outputs to be reliably differentiated. Drawing inspirations from recent studies on dataset bias in computer vision <ref type="bibr" target="#b22">(Liu &amp; He, 2025;</ref><ref type="bibr" target="#b41">Zeng et al., 2024)</ref>, which demonstrated that images from different large-scale vision datasets can be accurately distinguished by a standard neural network classifier, we consider a similar synthetic classification task to assess the separability of responses generated between different LLMs. Specifically, we sample a large number of text outputs from each LLM using the same set of prompts and then train a classifier to recognize which model generates a specific text. Figure <ref type="figure">1</ref> provides an overview of our framework. The illustrated example on ChatGPT, Claude, Grok, Gemini, and DeepSeek presents a five-way classification problem.</p><p>We find that a classifier based upon simple fine-tuning text embedding models on LLM outputs is able to achieve remarkably high accuracy on this task. This indicates the clear presence of idiosyncrasies in LLMs. The observation is highly robust over a large variety of LLM combinations. For instance, trained on the combined set of texts from ChatGPT, Claude, Grok, Gemini, and DeepSeek, a model can achieve 97.1% classification accuracy on the held-out validation data, compared to a 20.0% chance-level guess. Within the same model family, we obtain a non-trivial 59.8% accuracy across 4 model sizes in Qwen-2.5 series <ref type="bibr" target="#b30">(Qwen et al., 2024)</ref>. Further, we observe strong out-of-distribution generalization of these classifiers when tested on responses from prompts outside the training distribution.</p><p>We observe several interesting properties of this task. When controlling the length and format of outputs through prompt instructions, we still obtain high classification accuracy. Furthermore, for post-trained LLMs, the classifier demonstrates non-trivial accuracy even with only the first few tokens of the generated text. However, when classifying generations Prompt: Now everyone in a room is silent and working on their own thing, and you need to break the silence with a three-sentence joke. What would you say? Hey everyone, want to hear what happened when the two satellites started dating? They had a great first date, but things fell apart after that... Turns out they had too much space between them! Neural Network Classifier Hey, did you know I just got a new job at the calendar factory? The hours are great, but the days are really hard to get off. And the worst part? Every time I try to take a break, I get a month behind! A snail went to buy a car. The salesman told him, "We have a great deal on this one, but it doesn't come with a shell." The snail looked surprised and asked, "Well, how am I supposed to drive it?" ChatGPT, Claude, Grok, Gemini, or DeepSeek? Why did the scarecrow win an award? Because he was outstanding in his field! ...Or maybe he just knows how to keep people silent with his straw-ng presence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claude Grok Gemini DeepSeek</head><p>Why don't scientists trust atoms? Because they make up everything! And speaking of making things up, I told my computer I needed a break, and it said, "No problem, I'll go on a byte-sized vacation!"</p><p>ChatGPT Figure <ref type="figure">1</ref>. Our framework for studying idiosyncrasies in Large Language Models (LLMs). We show that each LLM is unique in its expression. In the example shown here on ChatGPT, Claude, Grok, Gemini, and DeepSeek, a neural network classifier is able to distinguish them with a near-perfect 97.1% accuracy.</p><p>from the same LLM but using different sampling strategies, we achieve accuracy only slightly above the chance level.</p><p>In addition, we observe certain behaviors of this task that resemble those of standard text classification, where improvements in text embeddings and availability of larger training datasets lead to better classification performance.</p><p>We analyze the contributing factors to the idiosyncratic behaviors in LLMs. Our analysis is based on isolating different levels of information through text transformations. We find that after randomly shuffling words in the LLM-generated responses, we observe a minimal decline in classification accuracy. This suggests that a substantial portion of distinctive features is encoded in the word-level distribution. We then highlight distinct sets of characteristic phrases that are consistently associated with each LLM. We also find that markdown formatting contributes to a moderate degree of idiosyncrasies in LLMs following post-training.</p><p>At the same time, we obtain over 90% accuracy when the word distribution is disrupted through transformations that preserve semantics, such as rephrasing or translating. Even with the most aggressive transformation -summarizing, classification accuracy remains well above chance-level guess. This finding implies that semantic information also shapes the idiosyncrasies in LLMs. Through open-ended language analysis, we provide further insights into these characteristics. For instance, ChatGPT has a preference for detailed, in-depth explanations, whereas Claude produces more concise and direct responses, prioritizing clarity.</p><p>Last, we discuss the broader implications of our findings.</p><p>One should be cautious when using synthetic data to train LLMs, as we show that many of these idiosyncrasies can be inherited in such a process. Our framework also serves as a tool for assessing model similarities among frontier models, either open-source or proprietary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Evaluating Idiosyncrasies in LLMs</head><p>Large Language Models (LLMs) share several common characteristics. First, they all utilize the Transformer architecture with self-attention <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref>. Second, they are trained using an auto-regressive objective <ref type="bibr" target="#b31">(Radford et al., 2019)</ref>, where they predict predict the next token in a sequence based on preceding context. Lastly, their training datasets significantly overlap, often incorporating vast and diverse sources such as Common Crawl, Wikipedia and Stack Overflow. Given these similarities, it is natural to ask: do LLMs speak in the same way? If not, how can we effectively measure the degree of their differences?</p><p>To address these questions, we construct a synthetic task focused on classifying outputs from different LLMs. Consider N LLMs, denoted as f 1 , . . . , f N , where each f i takes an input prompt p and outputs a text completion o. For a given dataset D of prompts, the outputs produced by each LLM f i are denoted as O i . We approach this problem with a straightforward setup. For N output sets O i , we formulate a N -way classification task, where the objective is to predict which LLM produced each output. If outputs of different LLMs were drawn from the same distribution, classification accuracy would not be better than random chance. Thus, we use the classification performance of this synthetic task as a measure of idiosyncrasies in LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Main Observations</head><p>We observe surprisingly high accuracies by neural networks to classify LLM outputs. This observation is robust across different settings, e.g., across model families and sizes. Since our task is essentially a sequence classification problem, we fine-tune a strong sequence embedding model LLM2vec <ref type="bibr" target="#b4">(BehnamGhader et al., 2024)</ref>  way classification head and report the resulting accuracy.</p><p>The training details are provided in Appendix A.</p><p>We describe the LLMs we use to generate the output datasets O 1,••• ,N . For a comprehensive and fair comparison across model families, we categorize three groups of LLMs:</p><p>1. Chat APIs ("chat"): This category includes state-ofthe-art LLMs that are primarily accessible via APIs. We consider <ref type="bibr">GPT-4o (OpenAI, 2024)</ref>, <ref type="bibr">Claude-3.5-Sonnet (Anthropic, 2024)</ref>, <ref type="bibr">Grok-2 (xAI, 2024</ref><ref type="bibr">), Gemini-1.5-Pro (Google, 2024)</ref>, and DeepSeek-V3 (DeepSeek-AI, 2024). For simplicity, we refer to them as ChatGPT, Claude, Grok, Gemini and DeepSeek. Their architectures and weights remain proprietary and undisclosed, with the exception of DeepSeek. 2. Instruct LLMs ("instruct"): These models are trained to generate high-quality responses from human instructions. We consider four LLMs of similar sizes across different families: Llama3.1-8b <ref type="bibr" target="#b8">(Dubey et al., 2024)</ref>, Gemma2-9b <ref type="bibr" target="#b33">(Riviere et al., 2024</ref><ref type="bibr">), Qwen2.5-7b (Qwen et al., 2024)</ref> and Mistral-v3-7b <ref type="bibr" target="#b17">(Jiang et al., 2023)</ref>. We will refer to them as Llama, Gemma, Qwen and Mistral. 3. Base LLMs ("base"): These are base versions of instruct LLMs. They are obtained by pretraining on extensive text corpora without any post-training stage.</p><p>Throughout the paper, we refer to these three categories as "chat", "instruct", and "base" respectively. For each LLM and a given prompt dataset, we collect 11K text sequences, splitting them into 10K for training and 1K for validation.</p><p>For chat APIs and instruct LLMs, we generate outputs from UltraChat <ref type="bibr" target="#b7">(Ding et al., 2023)</ref>, a diverse dialogue and instruction dataset. For base LLMs, we synthesize new texts using prompts from FineWeb <ref type="bibr" target="#b28">(Penedo et al., 2024)</ref>, a highquality LLM pretraining dataset. More details on response generation are in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Across model families.</head><p>In Table <ref type="table" target="#tab_0">1</ref>, we report the results for classifying outputs from various combinations of chat APIs (Table <ref type="table" target="#tab_0">1a</ref>) and instruct / base LLMs (Table <ref type="table" target="#tab_0">1b</ref>). In each of the three LLM groups, we enumerate all (C 2 N ) possible pairwise combinations when choosing 2 out of N models in the top panel of each table, as well as the case including N models in the bottom row. For the binary classification task, the neural network consistently achieves over 90% accuracy, with only one exception. Notably, for chat APIs and instruct LLMs, many combinations reach as high as 99% accuracy. In the more challenging N -way classification tasks, our classifiers maintain strong performance, achieving at least 87.3% accuracy across three groups. These results highlight the idiosyncrasies across different LLMs. We refer readers to Appendix B.1 for the confusion matrices of our classifiers.</p><p>Within the same model family. We evaluate sequence classification performance when distinguishing responses from LLMs within the same model family. Note that models from the same family typically share common training procedures, e.g., pretraining datasets and optimization schedule. First, we analyze the impact of model size by considering four Qwen2.5 instruct LLMs with 7B, 14B, 32B, and 72B parameters. As shown in Table <ref type="table" target="#tab_1">2</ref>, the classification task becomes more difficult, but our classifiers remain reasonably well above chance accuracy when distinguishing LLMs within the same family. In the binary classification setup, the highest accuracy reaches 85.5%, whereas in the full combination setup, the accuracy becomes 59.8%. In addition, we observe high accuracies when classifying responses from base and instruct versions of the same model. For example, our classifiers achieve 96.8% accuracy when distinguishing outputs from Qwen2.5-7b base and instruct models. Generalization to out-of-distribution responses. We find that our classifiers generalize robustly to responses beyond their training distribution. To evaluate this, we collect responses from instruct LLMs across four diverse datasets: i.e., UltraChat, Cosmopedia <ref type="bibr" target="#b5">(Ben Allal et al., 2024)</ref>, Lm-sysChat <ref type="bibr" target="#b44">(Zheng et al., 2024)</ref>, and WildChat <ref type="bibr" target="#b43">(Zhao et al., 2024)</ref>.  We train classifiers on LLM outputs from one prompt dataset and tested on those from another.</p><formula xml:id="formula_0">7b 14b 32b 72b instruct ✓ ✓ 77.0 ✓ ✓ 81.2 ✓ ✓ 83.4 ✓ ✓ 63.1 ✓ ✓ 85.5 ✓ ✓ 84.8 ✓ ✓ ✓ ✓ 59.8</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Controlled Experiments</head><p>We analyze the behaviors of the synthetic classification task in several controlled settings. From now on, we only report the accuracy of the N -way classification task in each group.</p><p>Prompt-level interventions. We assess the degree of idiosyncrasies in LLM outputs with explicit prompt-level interventions. Specifically, we modify the original prompt by incorporating additional instructions to constrain response length and format. We then perform sequence classification on the resulting outputs. Our interventions are:</p><p>• Length control: Please provide a concise response in a single paragraph, limited to a maximum of 100 words.  Input length of text embedding models. We control the number of input tokens to the text embedding models. Specifically, we truncate each response to a fixed number of tokens in a left-to-right fashion. Figure <ref type="figure">2</ref> presents the results. Across three groups of LLMs, the classification task benefits from seeing an increased number of tokens. Intriguingly, for chat APIs and instruct LLMs, we observe around 50% accuracy with just a single text token. This suggests that the initial token in a response contains certain distinctive signals for the classification problem. In Section 3.1, we provide further evidence supporting this observation.</p><p>Sampling methods. We consider outputs when sampled using different decoding strategies. Specifically, we use four widely used sampling methods: greedy decoding, temperature softmax, top-k, and top-p sampling. For each method, we generate a set of responses from the LLM. We then finetune the LLM2vec embedding model to predict the sampling method responsible for each response. Table <ref type="table" target="#tab_6">5</ref> presents the results for all pairs of sampling methods. Notably, the accuracy of distinguishing between responses generated by the same LLM remains relatively low, with the highest accuracy across all configurations being 59%. Furthermore, in a more fine-grained 5-way classification task distinguishing softmax sampling at five different temperatures (T = 0, 0.25, 0.5, 0.75, 1), we obtain an accuracy of 37.9%, only marginally better than the random chance level of 20%. These results suggest that outputs from the same LLM are not easily separable based on decoding strategies.</p><p>Text embedding models. We vary the underlying pretrained embedding models for sequence classification. The default setting we used in previous parts is fine-tuning the LLM2vec embedding models. We consider various generations of embeddings models spanning across architectures and training methods: ELMo <ref type="bibr" target="#b29">(Peters et al., 2018)</ref>, BERT <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref>, T5 <ref type="bibr" target="#b32">(Raffel et al., 2020)</ref>, GPT-2 <ref type="bibr" target="#b31">(Radford et al., 2019), and</ref><ref type="bibr">LLM2vec (BehnamGhader et al., 2024)</ref>. Details on the fine-tuning setting can be found in Appendix A.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Concrete Idiosyncrasies in LLMs</head><p>We have shown that modern neural networks can achieve excellent accuracies in classifying which LLM generates a given response. Here we leverage classical text similarity metrics -ROUGE-1 <ref type="bibr" target="#b20">(Lin, 2004)</ref>, ROUGE-L <ref type="bibr" target="#b20">(Lin, 2004)</ref>, and BERTScore <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref> -to quantify lexical differences between LLM outputs. We compute the mean F1-score for each metric across all response pairs generated by any two different chat API models given the same prompt.</p><p>For comparison, we also measure the similarity between responses sampled within the same model. As shown in  In the following, we identify concrete idiosyncrasies in LLMs across three dimensions: words and letters, markdown formatting elements, and semantic meaning. For each dimension, we apply text transformations to isolate potential idiosyncrasies and assess their impacts on classification performance. We then highlight specific patterns within each dimension that distinguish LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Words and Letters</head><p>Text shuffling. To decouple the effects of words and letters from other factors, we remove special characters in LLM-generated responses, such as punctuations, markdown elements, and excessive white spaces. This ensures that each response consists solely of words separated by a white space. Additionally, we apply two shuffling strategies to the preprocessed text: word-level and letter-level shuffling. These transformations disrupt the natural order and force the classifier to learn patterns from raw text statistics. Classifiers trained on responses without special characters achieve accuracies close to those using the original responses, i.e., 95.1% for chat APIs, 93.8% for instruct LLMs, and 75.4% for base LLMs. Likewise, using wordshuffled responses yields high accuracies comparable to the original ones. Further, we plot the frequencies of several commonly used words from five chat APIs in Figure <ref type="figure" target="#fig_2">5</ref> (left). We observe distinct patterns among models, even for frequent English words: Claude has much lower frequencies for words like "the", "and", "to", and "of" than other chat APIs. These results suggest that special characters and word order are not essential for distinguishing LLMs; word choices reflect substantial idiosyncrasies across models. In contrast, shuffling at the letter level results in a substantial drop in accuracy (49%-56%), approaching chance-level performance. This indicates that letter-level statistics alone are not sufficient for predicting LLM identities. To qualitatively visualize distinctions in letter distributions across models, Figure <ref type="figure" target="#fig_2">5</ref> (right) shows the frequency distribution of letters in responses generated by chat APIs. Different LLMs share almost identical letter distributions, indicating that letters contribute minimally to idiosyncrasies in LLMs.</p><p>Characteristic phrases. We use Term Frequency-Inverse Document Frequency (TF-IDF) to highlight characteristic phrases inside LLM-generated responses that reflect each model's word choices. Formally, we treat each LLM response as a document and then extract TF-IDF features on all uni-gram and bi-gram words. We then train a N -way logistic regression model to predict the origin of responses on the extracted features. This simple linear classifier achieves 85.5% / 83.7% accuracy on chat APIs / instruct LLMs, close to 95.1% / 93.8% achieved with fine-tuning embedding models on responses without special characters (Table <ref type="table" target="#tab_11">8</ref>).</p><p>Since the coefficients of a logistic regression model provide a natural ranking for its features, we leverage these coefficients to highlight important phrases in the classification task. Figure <ref type="figure" target="#fig_3">6</ref> presents the top 10 phrases with the largest logistic regression coefficients for each of the five chat API models. Notably, these phrases often serve as transitions or emphasis in sentences. For example, ChatGPT likes to generate "such as", "certainly", and "overall", whereas Claude prefers "here", "according to", and "based on".  Figure <ref type="figure" target="#fig_1">4a</ref> illustrates these characteristic phrases with example responses from ChatGPT and Claude. While ChatGPT begins responses with "certainly" and "below is", Claude usually references the original prompt using the phrases like "according to the text" and "based on the text". Moreover, Figure <ref type="figure" target="#fig_4">7</ref> reveals noticeable differences in the distribution of first word choices among chat APIs. Appendix B.3 provides characteristic phrases for other LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Markdown Formatting</head><p>We seek to understand how each LLM formats their responses, particularly in markdown. To this end, we focus on common markdown elements used by LLMs: (1) bold text, ( <ref type="formula">2</ref>) italic text, (3) header, (4) enumeration, ( <ref type="formula">5</ref>) bullet point, (6) code block. We transform the LLM outputs by retaining only these formatting components while replacing other text with the marker "xxx". Appendix C provides examples of the transformed outputs.  We count the occurrence of a markdown formatting element in each response. We then plot the distribution of these counts over all responses in Figure <ref type="figure" target="#fig_5">8</ref>. Each model exhibits a unique way to format its responses. For instance, Claude (yellow) has a high density at zero in the bold text and header count distributions, indicating that it generates many responses without bold texts or headers. On the contrary, other LLMs exhibit lower values at zero and thus decorate text with these formatting elements more often.   Results of our open-ended language analysis on ChatGPT and Claude. ChatGPT features descriptive language, sophisticated markdown formatting, and in-depth details, while Claude highlights straightforward tone, minimal structure, and summarized content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Semantics</head><p>Rewriting. One potential reason for the high classification accuracy is the unique writing style (e.g., word choice, sentence structure) of each LLM. To isolate this factor, we leverage another LLM (e.g., GPT-4o mini) to rewrite LLM responses. Our rewriting approaches include (see Appendix C for example responses after rewriting):</p><p>• Paraphrasing: Paraphrase the above text while maintaining the semantic meaning of the original text. • Translating: Translate the above text into Chinese.</p><p>• Summarizing: Summarize the above text in one paragraph.</p><p>We show the results in Table <ref type="table" target="#tab_16">10</ref>. The classifiers trained on paraphrased LLM responses maintain similar accuracy levels to those using original responses. Likewise, when using translated text, the classifiers are also able to differentiate between LLMs. These findings suggest that the semantic meanings of words play a more significant role in predicting LLM origins than the exact word choice. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implications</head><p>In this section, we explore the broader implications of our framework, regarding synthetic data and model similarity.</p><p>Idiosyncrasies via synthetic data. Using synthetic data has become a common practice when training frontier LLMs <ref type="bibr">(Abdin et al., 2024a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b21">Liu et al., 2024)</ref>. We conduct supervised fine-tuning (SFT) on two base LLMs (Llama3.1-8b and Gemma2-9b) using Ultrachat, i.e., dialogues generated by ChatGPT. After the SFT stage, we train a classifier to distinguish between responses from two fine-tuned models. We find that SFT on the same synthetic dataset significantly reduces the classification accuracy from 96.5% to 59.8%, narrowing down the differences between these two models.</p><p>In addition, we generate responses from Llama3.1-8B and Gemma2-9B in instruct LLMs using UltraChat prompts.</p><p>Then we fine-tune Qwen2.5-7B base LLM on each set of responses respectively. Interestingly, responses from the two resulting fine-tuned models can be classified with 98.9% accuracy, suggesting that each fine-tuned model retains the unique characteristics in its SFT data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phi-4</head><p>Figure <ref type="figure">10</ref>. Inferring model similarity. We consider 6 LLMs, including 5 chat API models and Phi-4. In each subfigure, we evaluate a five-way classifier on outputs from the excluded LLM and present the distribution of predicted model origins. There is a strong tendency for LLM outputs to be predicted as ChatGPT.</p><p>Results are shown in Figure <ref type="figure">10</ref>. Intriguingly, for Claude, Grok, and Gemini, we observe a strong tendency for their outputs to be classified as ChatGPT. For instance, when Grok is the excluded model, 82.8% of its responses are classified as ChatGPT. In addition, responses from ChatGPT and DeepSeek are frequently identified as coming from Phi-4, with 55.9% and 76.0% of their responses respectively. In turn, most of Phi-4's outputs are classified as originating from ChatGPT or DeepSeek.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Dataset classification.  <ref type="bibr">et al., 2008;</ref><ref type="bibr">OpenAI, 2019)</ref>, entropy <ref type="bibr" target="#b19">(Lavergne et al., 2008;</ref><ref type="bibr">Gehrmann et al., 2019)</ref> or negative probability curvature <ref type="bibr" target="#b25">(Mitchell et al., 2023;</ref><ref type="bibr" target="#b3">Bao et al., 2024)</ref>. Other works leverage neural network features to perform this task, such as fine-tuning BERT models <ref type="bibr" target="#b37">(Uchendu et al., 2021;</ref><ref type="bibr" target="#b15">Ippolito et al., 2020)</ref>. Neural authorship attribution <ref type="bibr" target="#b36">(Uchendu et al., 2020;</ref><ref type="bibr" target="#b14">Huang et al., 2024)</ref> seeks not only to identify machinegenerated text but also to attribute it to specific text generators. In this work, we focus on the distinguishability between LLMs rather than between AI vs. human.</p><p>Understanding differences between distributions. A line of research <ref type="bibr" target="#b9">(Dunlap et al., 2024;</ref><ref type="bibr" target="#b45">Zhong et al., 2024)</ref> has used foundation models to describe qualitative differences between pairs of data distributions (e.g., image datasets). <ref type="bibr" target="#b11">Gao et al. (2025)</ref> conducted hypothesis testing on sets of model outputs to check whether the underlying LLMs were identical. The most relevant work to us is <ref type="bibr" target="#b10">Dunlap et al. (2025)</ref>, which proposed VibeCheck to understand user-aligned traits in LLM outputs. They found that LLMs often vary in styles, such as being more formal or friendly. In contrast, our work aims to identify broader generalizable patterns to interpret the high classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We demonstrate the presence of idiosyncrasies in Large Language Models (LLMs) and investigate a synthetic task designed to quantify their extent. We find that simply finetuning pretrained text embedding models on LLM outputs leads to exceedingly high accuracy in predicting the origins of the text. This phenomenon persists across diverse prompt datasets, LLM combinations, and many other settings. We also pinpoint concrete forms of these idiosyncrasies within LLMs. We hope our work encourages further research into understanding idiosyncrasies in LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Prompts for Open-ended Language Analysis</head><p>We detail the procedures of our open-ended language analysis in Section 3.3. Given the same input, we sample a pair of responses from two LLMs and present them, along with an analysis prompt (see Figure <ref type="figure" target="#fig_7">11a</ref>), to an LLM judge for comparison. To avoid the LLM judge exploiting any prior knowledge of the models, we anonymize model identities using an index distribution. This process is repeated for 35 response pairs, yielding a set of detailed analyses. Finally, we use the summarization prompt (see Figure <ref type="figure" target="#fig_7">11b</ref>) to distill these analyses into 5 bullet points that characterize the idiosyncrasies of each model.</p><p>Here are some samples from 2 different distributions of text.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Confusion Matrix</head><p>In Figure <ref type="figure" target="#fig_8">12</ref>, we present the confusion matrix for the N -way classifiers that are trained on responses generated by chat APIs, instruct LLMs, and base LLMs, respectively. The results demonstrate that our classifiers can accurately predict the origin of LLM-generated responses, with minimal confusion between different LLMs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Words and Letters</head><p>Figure <ref type="figure" target="#fig_9">13</ref> presents the frequencies of the 20 most commonly used words (left) and all English letters (right) across instruct and base LLMs. Consistent with our observations in Section 3.1, we find notable differences in the distribution of commonly used words between these models, such as "the", "and", "to". In contrast, the letter distributions are nearly identical.</p><p>Idiosyncrasies in Large Language Models </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Characteristic Phrases</head><p>We provide additional results for characteristic phrases as presented in Section 3.1. We follow the same methodology in Figure <ref type="figure" target="#fig_3">6</ref> to extract characteristic phrases of instruct and base LLMs. Specifically, we train a four-way logistic regression classifier on the TF-IDF features of their responses and use the coefficients to select important phrases of each model.</p><p>As shown in Figure <ref type="figure" target="#fig_1">14</ref>, each instruct LLM contains quite distinct characteristic phrases. For example, Llama frequently employs terms "including" and "such as" to introduce specific examples in the output, whereas Gemma tends to engage with users using phrases "let me" and "know if". In contrast, the extracted phrases from base LLMs are less distinctive, primarily consisting of common words such as "the", "to", and "you".</p><p>Figure <ref type="figure" target="#fig_10">15</ref> illustrates the distribution of first word choices in instruct and base LLMs. Similar to chat APIs (Figure <ref type="figure" target="#fig_4">7</ref>), instruct LLMs display varied distributions. However, base LLMs exhibit substantial overlap in their most frequent first words, e.g., "the", "and", "of", "to", and "in".  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Unique Markdown Formatting</head><p>In this part, we provide additional results for the analysis of markdown formatting as presented in Section 3.2. Figure <ref type="figure" target="#fig_3">16</ref> illustrates the distribution counts of six markdown formatting elements across different models. For both chat API models (Figure <ref type="figure" target="#fig_3">16a</ref>) and instruct LLMs (Figure <ref type="figure" target="#fig_3">16b</ref>), we observe distinct differences in the usage of bold texts, headers, enumerations, and bullet points, while italic texts show less variation. Intriguingly, Gemini uses much more italic texts (a lower density at zero in the italic text) than other chat APIs, where similar observations can be found on Gemma2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Open-ended Language Analysis</head><p>Ablation on LLM judges. Here we demonstrate our findings in Figure <ref type="figure">9</ref> of Section 3.3 remains consistent under several LLM judges. Specifically, we change the LLM judge from ChatGPT to Claude, Grok, and Gemini. We show the results in Figure <ref type="figure" target="#fig_11">17</ref>. Regardless of the choice of LLM judges, our language analysis reveals that ChatGPT often uses detailed explanations and complex formatting structures, whereas Claude emphasizes key contents without extensive elaboration.</p><p>• Tone: Authoritative and formal, using academic language with detailed explanations and complex sentence structures • Word Choice: Rich descriptive vocabulary with technical terms, metaphors, and evaluative language ("renowned," "instrumental," "exquisite") </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ChatGPT</head><p>-Tone: Direct and Practical -Often concise, focusing on actionable insights or straightforward information.</p><p>-Word Choices: Simple and Functional -Prefers straightforward language, emphasizing utility over emotional or detailed descriptions.</p><p>-Opening Lines: Immediate and Focused -Jumps directly into the topic or lists key points without much preamble.</p><p>-Markdown Formatting: Minimalist -Uses basic markdown elements like bullet points or simple lists for clarity.</p><p>-Content: Succinct and Focused -Provides essential information with less depth, often aiming for quick readability or practicality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claude</head><p>(b) Grok as the LLM judge.</p><p>• Formal Tone: Often adopts a formal, academic, or instructional tone, prioritizing clarity and detail.</p><p>• Precise Diction: Employs specific, descriptive, and sometimes technical vocabulary relevant to the subject matter.</p><p>• Contextual Openings: Typically begins with introductory statements, context-setting, or clear purpose statements.</p><p>• Structured Formatting: Frequently utilizes headings, lists, bolding, and other markdown features for organization and emphasis.</p><p>• Comprehensive Content: Provides detailed explanations, examples, and in-depth information, often covering a broader scope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ChatGPT</head><p>• Informal Tone: Generally uses a more informal, conversational, or concise tone, prioritizing brevity and accessibility.</p><p>• Simplified Language: Favors simpler, more direct vocabulary, avoiding jargon and focusing on key information.</p><p>• Direct Openings: Often starts directly with the main point or a summary of key information.</p><p>• Minimal Formatting: Uses less markdown formatting, relying primarily on basic lists, bullet points, or plain text.</p><p>• Concise Content: Focuses on summarizing key points, offering practical advice, or providing concise overviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claude</head><p>(c) Gemini as the LLM judge. Open-ended language analysis results on other LLMs. In Section 3.3, we presented the results of open-ended language analysis for ChatGPT and Claude. Here, we extend our analysis to other chat API models and instruct LLMs. The full results are shown in Figure <ref type="figure" target="#fig_12">18</ref>, where we use ChatGPT as the LLM judge to compare responses generated by two models within the same category (chat APIs / instruct LLMs). Our analysis highlights several interesting characteristics of each model. For example, Grok's responses tend to feature rich language and comprehensive content, whereas Gemini's outputs are more concise with direct openings.</p><p>-Descriptive Tone: Emphasizes detail, narrative, or objective perspectives, often exploring themes with depth and nuance.</p><p>-Rich Language: Incorporates emotive, analytical, or descriptive language to enhance understanding and engagement.</p><p>-Detailed Openings: Introduces the subject with context, thematic elements, or guiding points, establishing a comprehensive view.</p><p>-Structured Formatting: Utilizes formatting elements like headings, lists, and bold text to organize information clearly.</p><p>-Comprehensive Content: Delivers in-depth analysis or exploration of subjects, highlighting diverse factors or broader implications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grok</head><p>-Concise Tone: Often adopts a straightforward, direct, or expressive approach focusing on succinctness and dramatic tension.</p><p>-Specific Language: Utilizes precise or vivid language to convey themes like tension, urgency, and significant events or outcomes.</p><p>-Direct Openings: Typically begins with immediate context or action, setting a focused tone or premise from the start.</p><p>-Simple Formatting: Relies mainly on plain text or minimal formatting to maintain clarity and directness.</p><p>-Core Content: Centers on key insights or motivations, providing essential information or narratives with limited embellishment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gemini (a) chat APIs</head><p>-Tone Characterization: Often formal, factual, and exacting, emphasizing detailed analysis and structured presentations.</p><p>-Word Choices Depiction: Technical and precise, leveraging industryspecific or academic terminology to convey depth and credibility.</p><p>-Opening Line Initiation: Frequently starts directly with context, setting an immediate formal or informative tone.</p><p>-Markdown Use: Prefers structured formats with headings, bullet points, or numbered lists to enhance clarity.</p><p>-Content Focus: Delivers comprehensive, detailed content with substantial data, examples, or specific tactics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Llama</head><p>-Tone Characterization: Generally more conversational, engaging, and accessible, often blending storytelling or personal insights.</p><p>-Word Choices Depiction: Simple, expressive, and often emotive language, creating relatability and narrative flair.</p><p>-Opening Line Initiation: Begins engagingly, often with context or an emotional hook, to capture the reader's interest.</p><p>-Markdown Use: Employs varied markdown elements like bullet points or emphasis for readability and structure.</p><p>-Content Focus: Centers on overarching concepts, insights, and experiences, often prioritizing engagement over exhaustive detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gemma</head><p>-Detailed and Analytical Tone: The tone is often formal, focusing on analysis, exploration, and organized narratives.</p><p>-Descriptive and Formal Word Choices: Uses technical and descriptive vocabulary that highlights complexity and depth.</p><p>-Comprehensive Opening Lines: Starts often with broad context or detailed introductions, setting a formal framework.</p><p>-Structured Markdown Formatting: Utilizes headers, sections, and lists to enhance readability and organization.</p><p>-Content Depth and Exploration: Provides comprehensive narratives with detailed discussions, focusing on systematic approaches and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qwen</head><p>-Concise and Direct Tone: The tone is generally straightforward, practical, and sometimes conversational, focusing on direct explanations.</p><p>-Simplified and Emotive Word Choices: Prefers straightforward language, occasionally prioritizing emotional or motivational impact.</p><p>-Immediate Opening Lines: Begins directly with subject matter or actions, often in conversational or narrative styles.</p><p>-Minimalist Markdown Formatting: Uses simpler formatting with lists or plain text to maintain clarity without embellishments.</p><p>-Summary Focused Content: Concentrates on key points with succinct explanations, prioritizing clarity and accessibility over comprehensive detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mistral</head><p>(b) instruct LLMs </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Response Demonstrations</head><p>In this part, we present examples of LLM responses. Table <ref type="table" target="#tab_24">12</ref> and 13 illustrate responses before and after our promptlevel interventions (Section 2.2). Table <ref type="table" target="#tab_5">14</ref> shows the transformed responses when only markdown elements are retained (Section 3.2). Table <ref type="table" target="#tab_6">15</ref> presents the rewritten responses (Section 3.3). Additionally, Table <ref type="table" target="#tab_8">16</ref> provides a pairwise comparison of responses, supporting our findings in Table <ref type="table" target="#tab_14">9</ref>. <ref type="bibr">Table 17,</ref><ref type="bibr">18,</ref><ref type="bibr">19,</ref><ref type="bibr">20,</ref><ref type="bibr">21,</ref><ref type="bibr">22,</ref><ref type="bibr">23,</ref><ref type="bibr">24,</ref><ref type="bibr">25,</ref><ref type="bibr">26,</ref><ref type="bibr">27,</ref><ref type="bibr">28,</ref><ref type="bibr">29</ref> contain example responses from each LLM considered in Section 2, including ChatGPT, Claude, Grok, Gemini, DeepSeek, LLama3.1-8b (instruct), Gemma2-9b (instruct), Qwen2.5-7b (instruct), Mistral-v3-7b (instruct), LLama3.1-8b (base), Gemma2-9b (base), Qwen2. .</p><p>Can you suggest any other cheese alternatives that I can use instead of Brie for this recipe? Generate according to: Add one or two slices Brie o the beef and let it melt. Cut the green apple in thin slices. Top with the mixed salad and finish with the apple slices.</p><p>If you're looking for cheese alternatives to Brie for your recipe, consider the following options:</p><p>1. Camembert: This cheese is quite similar to Brie in texture and flavor, offering a creamy consistency and a mild, earthy taste that will complement the beef and apple. 2. Taleggio: An Italian cheese with a soft texture and a tangy, fruity flavor. It melts well and can add a unique twist to your dish. 3. Fontina: Known for its smooth melting properties, Fontina has a nutty and slightly sweet flavor that pairs nicely with beef and apples. 4. Goat Cheese: If you prefer a tangier option, goat cheese can provide a creamy texture and a distinct flavor that contrasts well with the sweetness of the apple. 5. Havarti: A semi-soft cheese with a buttery and slightly sweet taste, Havarti melts beautifully and can be a great substitute for Brie. These alternatives should work well in your recipe, providing a delicious and melty addition to your beef and apple combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) original</head><p>Can you suggest any other cheese alternatives that I can use instead of Brie for this recipe? Generate according to: Add one or two slices Brie o the beef and let it melt. Cut the green apple in thin slices. Top with the mixed salad and finish with the apple slices.</p><p>Please provide a concise response in a single paragraph, limited to a maximum of 100 words.</p><p>For a delicious alternative to Brie, consider using Camembert or triple cream cheese, both of which offer a similar creamy texture and rich flavor. If you prefer a slightly tangier option, goat cheese can provide a delightful contrast to the beef and apple. For a milder taste, Havarti or Fontina can melt beautifully and complement the dish's flavors. Each of these cheeses will add a unique twist while maintaining the creamy, melt-in-your-mouth experience that Brie provides.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) length control</head><p>Can you suggest any other cheese alternatives that I can use instead of Brie for this recipe? Generate according to: Add one or two slices Brie o the beef and let it melt. Cut the green apple in thin slices. Top with the mixed salad and finish with the apple slices.</p><p>Please provide your response in plain text only, avoiding the use of italicized or bold text, lists, markdown, or HTML formatting.</p><p>You can use Camembert or triple cream cheese as alternatives to Brie in your recipe. Both have a similar creamy texture and rich flavor that will complement the beef and other ingredients.</p><p>(c) format control Can you suggest any other cheese alternatives that I can use instead of Brie for this recipe? Generate according to: Add one or two slices Brie o the beef and let it melt. Cut the green apple in thin slices. Top with the mixed salad and finish with the apple slices.</p><p>If you're looking for alternatives to Brie cheese in your recipe, here are some options you can consider:</p><p>1. Goat Cheese: A soft and creamy cheese with a tangy flavor that pairs well with the sweetness of the apple and the savory flavor of the beef. 2. Feta Cheese: A salty and crumbly cheese that adds a nice contrast to the softness of the beef and the crunch of the apple. 3. Camembert: A French cheese with a creamy texture and a mild, earthy flavor that complements the beef and apple nicely. 4. Blue Cheese: A strong and pungent cheese that adds a bold flavor to the dish. Use a milder blue cheese if you're not a fan of strong flavors. 5. Mascarpone: A rich and creamy cheese that adds a luxurious touch to the dish. It's a good option if you want a milder flavor than Brie. 6. Ricotta: A soft and creamy cheese that's a good option if you want a lighter flavor than Brie. It's also a good choice if you're looking for a lower-calorie alternative. 7. Taleggio: An Italian cheese with a creamy texture and a mild, fruity flavor that pairs well with the apple and beef.</p><p>When substituting Brie, keep the following in mind:</p><p>• If using a stronger cheese like Blue Cheese or Goat Cheese, use a smaller amount to avoid overpowering the other flavors.</p><p>• If using a milder cheese like Mascarpone or Ricotta, you may want to add a bit more to achieve the desired creaminess.</p><p>• If using a crumbly cheese like Feta, you may want to crumble it instead of slicing it to distribute the flavor evenly.</p><p>Remember, the key is to balance the flavors and textures in your dish, so feel free to experiment and find the combination that works best for you!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) original</head><p>Can you suggest any other cheese alternatives that I can use instead of Brie for this recipe? Generate according to: Add one or two slices Brie o the beef and let it melt. Cut the green apple in thin slices. Top with the mixed salad and finish with the apple slices.</p><p>Please provide a concise response in a single paragraph, limited to a maximum of 100 words.</p><p>If you're looking for alternatives to Brie, consider using other soft-ripened cheeses like Camembert, Goat Cheese, or Feta. Alternatively, you could opt for a semi-soft cheese like Havarti or Muenster. For a stronger flavor, try using Blue Cheese or Gorgonzola. If you prefer a milder taste, Boursin or Mascarpone could work well. Each of these cheeses will melt and complement the beef and apple nicely, adding a unique twist to your dish. Simply replace the Brie with your chosen cheese and proceed with the recipe as instructed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) length control</head><p>Can you suggest any other cheese alternatives that I can use instead of Brie for this recipe? Generate according to: Add one or two slices Brie o the beef and let it melt. Cut the green apple in thin slices. Top with the mixed salad and finish with the apple slices.</p><p>Please provide your response in plain text only, avoiding the use of italicized or bold text, lists, markdown, or HTML formatting.</p><p>You can consider the following cheese alternatives to Brie in your recipe: Camembert: This French cheese has a similar creamy texture and mild, earthy flavor to Brie, making it a suitable substitute. Goat cheese: A tangy and creamy cheese that pairs well with the sweetness of the apple and the savory flavor of the beef. Feta: A crumbly Greek cheese that adds a salty flavor to the dish, which can balance out the sweetness of the apple. Blue cheese: A strong and pungent cheese that can add a bold flavor to the dish, but be aware that it may overpower the other ingredients. Ricotta: A mild and creamy cheese that can add a light and airy texture to the dish, but it may not provide the same richness as Brie. Mascarpone: An Italian cheese with a rich and creamy texture that can add a luxurious feel to the dish, but it may be sweeter than Brie. Keep in mind that each of these alternatives will change the flavor profile of the dish, so you may need to adjust the other ingredients accordingly.</p><p>(c) format control What are some effective tactics for building a referral network and gaining new business through word-of-mouth? Building a strong referral network and gaining new business through word-of-mouth can be highly effective and cost-efficient.</p><p>Here are some tactics to achieve this:</p><p>1. Deliver Exceptional Service: The foundation of word-of-mouth marketing is consistent excellence. Providing top-notch services or products encourages satisfied customers to share their positive experiences with others. 2. Identify and Nurture Key Relationships: Focus on building strong relationships with clients, partners, and industry peers who can refer you to others. Personalize your interactions and maintain regular communication to keep these relationships strong. 3. Ask for Referrals: Don't hesitate to ask satisfied clients for referrals. Timing is crucial-make the request at the peak of their satisfaction. Be clear about the type of referrals you are seeking. 4. Referral Incentives: Offer incentives such as discounts, gift cards, or exclusive access to services to encourage customers or partners to refer new business to you. Ensure these incentives are enticing enough to motivate action. 5. Leverage Social Proof: Encourage satisfied customers to leave reviews or testimonials online. Positive reviews on platforms such as Google, Yelp, or LinkedIn can influence potential clients. 6. Engage on Social Media: Use social media platforms to share success stories, client testimonials, and updates about your business. Engage with your audience through comments, shares, and likes to build a community around your brand. 7. Host Events: Organize events such as workshops, webinars, or networking meet-ups. These events provide value, showcase your expertise, and encourage attendees to refer you to others. 8. Partner with Complementary Businesses: Collaborate with businesses that offer complementary services or products. This creates mutual referral opportunities and extends your reach to their customer base. 9. Create a Referral Program: Formalize your referral process with a structured program, ensuring referrers are aware of the benefits and the procedure. Clearly communicate the value proposition and streamline the referral process. 10. Stay Top-of-Mind: Regularly connect with your network through emails, newsletters, or personalized messages. Share updates, industry insights, or tips that position you as a valuable and knowledgeable resource. 11. Thank Referrers: Always express gratitude to those who refer new business to you. Acknowledging their support strengthens relationships and encourages further referrals. 12. Measure and Optimize: Track where your referrals are coming from and the effectiveness of different tactics. Use this data to optimize your approach, focusing on strategies that yield the best results. By implementing these tactics and consistently nurturing your network, you can effectively build a robust referral system that generates new business through word-of-mouth recommendations. What are some ways to cope with feelings of loneliness or isolation?</p><p>Coping with feelings of loneliness or isolation can be challenging, but there are several strategies that might help:</p><p>1. Connect with Others: Reach out to friends and family through phone calls, video chats, or messaging apps. Joining clubs, groups, or online communities related to your interests can also help you meet new people. 2. Volunteer: Helping others can provide a sense of purpose and community. Look for local organizations or virtual opportunities where you can volunteer your time or skills. 3. Engage in Hobbies: Pursue activities or hobbies that you enjoy, whether it's reading, painting, cooking, or playing a musical instrument. These can be fulfilling and help distract from loneliness. 4. Physical Activity: Exercise is known to boost mood and alleviate stress. Try walking, jogging, yoga, or any form of physical activity that you enjoy. 5. Mindfulness and Meditation: Practices such as meditation, deep breathing, or mindfulness can help anchor you in the present moment and reduce feelings of loneliness. 6. Professional Help: Consider speaking with a therapist or counselor, who can provide support and coping strategies tailored to your situation. 7. Pet Companionship: If feasible, consider getting a pet. Animals can offer companionship and can be a source of comfort and joy. 8. Limit Social Media: While it might seem connecting, excessive use of social media can sometimes exacerbate feelings of loneliness. Set boundaries for healthier use. 9. Self-Compassion: Practice being kind to yourself. Acknowledge your feelings without judgment and remind yourself that it's okay to feel lonely at times. 10. Set Goals: Setting small, achievable goals can provide a sense of accomplishment and forward momentum, fostering a sense of purpose. 11. Seeking Community Spaces: Libraries, community centers, or cafes can be great places to be around others, even if you're not directly interacting with them. Remember, everyone experiences loneliness differently, and it's important to find what works best for you.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) original</head><p>Dealing with feelings of loneliness or isolation can be tough, but there are various strategies that may help:</p><p>1. Reach Out: Contact friends and family through phone calls, video chats, or messaging platforms. Joining clubs, groups, or online communities that align with your interests can also help you connect with new people. 2. Volunteer Your Time: Helping others can give you a sense of purpose and community. Look for local charities or online opportunities to contribute your time or skills. 3. Pursue Your Hobbies: Engage in activities you enjoy, such as reading, painting, cooking, or playing a musical instrument.</p><p>These pursuits can be satisfying and provide a distraction from feelings of loneliness. 4. Stay Active: Regular exercise is known to improve mood and reduce stress. Try activities like walking, jogging, yoga, or any form of physical exercise that appeals to you. 5. Practice Mindfulness: Techniques such as meditation, deep breathing, or mindfulness can help ground you in the present and alleviate feelings of loneliness. 6. Consider Professional Guidance: Talking to a therapist or counselor can provide helpful support and coping techniques tailored to your needs. 7. Get a Pet: If it's an option for you, think about adopting a pet. Animals can be wonderful companions and bring joy and comfort into your life. 8. Limit Social Media Use: Though it may seem like a way to connect, extensive use of social media can sometimes increase feelings of loneliness. Establish limits for a healthier engagement. 9. Be Kind to Yourself: Practice self-compassion by acknowledging your feelings without judgment and reminding yourself that it's normal to feel lonely occasionally. 10. Set Achievable Goals: Creating small, realistic goals can foster a sense of accomplishment and help you move forward, enhancing your sense of purpose. 11. Use Community Spaces: Places like libraries, community centers, or cafes can provide an opportunity to be around others, even if you're not directly interacting with them.</p><p>Remember, everyone experiences loneliness in different ways, so it's essential to discover what strategies work best for you.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) paraphrasing</head><p>Coping with loneliness can be challenging, but several strategies can help. Connecting with friends and family, volunteering, and engaging in enjoyable hobbies can foster a sense of community. Physical activity and mindfulness practices can improve mood and reduce stress. Seeking professional help and considering companionship from pets may also provide comfort. It's essential to limit social media usage, practice self-compassion, set small goals for achievement, and explore community spaces like libraries or cafes. Ultimately, finding individualized approaches that resonate with you is key to managing feelings of loneliness.</p><p>(c) summarizing</p><p>Table <ref type="table" target="#tab_6">15</ref>. Examples of LLM rewriting using GPT-4o-mini.</p><p>What are some ways to cope with feelings of loneliness or isolation?</p><p>Coping with feelings of loneliness or isolation can be challenging, but there are several strategies that might help:   <ref type="table" target="#tab_8">16</ref>. Examples from ChatGPT and Claude that illustrate results of our open-ended language analysis in Figure <ref type="figure">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Idiosyncrasies in Large Language Models</head><p>Can polar bears migrate to new habitats if their Arctic environment changes too drastically? Polar bears are highly specialized animals adapted to the Arctic environment, primarily relying on sea ice for hunting seals, their main prey. If their Arctic environment changes drastically, such as through significant sea ice loss due to climate change, their ability to migrate to new habitats is limited by several factors:</p><p>1. Species Adaptation: Polar bears are uniquely adapted to life on sea ice and in cold climates. They have thick fur, a layer of fat for insulation, and physical characteristics like large paws for swimming and walking on ice. These adaptations make it difficult for them to survive in non-polar environments. 2. Dietary Needs: Their diet consists mainly of seals, which they hunt on sea ice. New habitats may not support a sufficient seal population or alternative prey that polar bears can easily exploit. 3. Geographical Barriers: Polar bears are largely confined to the Arctic and areas surrounding the North Pole. There are no direct routes to migrate to other cold environments that would support them in a similar way. 4. Competition and Predation: Moving to new habitats would likely involve entering territories with existing ecosystems that include competition for food from other predators or unfamiliar threats. 5. Habitat Fragmentation: The fragmentation of ice habitats does not create viable paths for migration but rather isolates populations, reducing genetic diversity and making survival more precarious. In conclusion, while polar bears might be able to make small relocations within the Arctic as sea ice conditions shift, extensive migration to entirely new habitats is not feasible due to their specific adaptations and the absence of suitably similar environments elsewhere. Conservation efforts focus more on mitigating climate change and preserving their current habitat. Can polar bears migrate to new habitats if their Arctic environment changes too drastically?</p><p>Polar bears have limited ability to migrate to new habitats if Arctic conditions become unsuitable. They are highly specialized for life on sea ice and depend on it for hunting seals, their primary food source. While they can swim long distances, moving to southern territories would be challenging because: 1) they are not well-adapted to warmer climates, 2) their hunting techniques are specific to ice-based prey, and 3) competition with other bear species and humans would increase. Their survival is closely tied to maintaining Arctic sea ice habitat.</p><p>Table <ref type="table" target="#tab_12">18</ref>. Example response of Claude.</p><p>Can polar bears migrate to new habitats if their Arctic environment changes too drastically? Polar bears are highly specialized for their Arctic environment, and their ability to migrate to entirely new habitats is limited by several factors:</p><p>1. Adaptation: Polar bears are adapted to cold environments with a diet primarily consisting of seals, which they hunt from sea ice. Their thick fur, large size, and fat reserves are suited for Arctic conditions, not temperate or tropical climates. 2. Habitat Requirements: They need sea ice for hunting, breeding, and resting. Without ice, their ability to catch seals, which is their primary food source, is severely compromised. Alternative habitats would need to provide similar conditions or abundant alternative prey, which is rare outside the Arctic. 3. Physiological Limitations: Polar bears might find it challenging to adapt to significantly different temperatures or ecosystems.</p><p>For instance, their large bodies and fur make them prone to overheating in warmer climates. 4. Behavioral Flexibility: While bears in general can be quite adaptable in terms of diet (e.g., grizzly bears), polar bears have evolved in an environment with very specific conditions. Their behavioral adaptations are finely tuned to the Arctic, reducing the likelihood of successful migration to drastically different environments. 5. Human Activity: As polar bears move southward in search of food or better conditions, they increasingly encounter human populations and infrastructure, leading to conflicts, habitat fragmentation, and often, negative outcomes for the bears. 6. Genetic Considerations: Polar bears have evolved in relative isolation in the Arctic. Mixing with other bear species or adapting to entirely new environments could lead to genetic issues or dilution of their specialized adaptations. However, there are some considerations:</p><p>• Hybridization: There's evidence of polar bears mating with grizzly bears (creating "pizzly" or "grolar" bears) in areas where their ranges overlap due to climate change. This might be seen as a form of adaptation or migration at a genetic level, though it's not migration in the traditional sense. • Range Expansion: Some polar bears have been observed traveling further south than usual, likely in search of food as ice recedes. However, this is more about extending their current habitat rather than migrating to a completely different one. • Conservation Efforts: Efforts might focus on preserving or creating corridors where polar bears can move between suitable habitats or on habitat restoration to ensure the Arctic remains viable for them. In summary, while polar bears could potentially expand their range or adapt behaviorally to some extent, true migration to entirely new, non-Arctic habitats is unlikely due to their specific ecological needs and physiological adaptations. Conservation efforts are more focused on mitigating the impacts of climate change in their current habitat or facilitating their movement within the Arctic region.</p><p>Table <ref type="table" target="#tab_14">19</ref>. Example response of Grok.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Can polar bears migrate to new habitats if their Arctic environment changes too drastically?</head><p>While polar bears have some capacity for movement and adaptation, large-scale migration to entirely new habitats in response to Arctic change is highly unlikely and faces significant obstacles:</p><p>• Physiological Specialization: Polar bears are highly specialized for Arctic life. Their thick blubber, fur, and hunting techniques are specifically adapted for cold, icy environments. They rely heavily on sea ice for hunting seals, their primary food source. Relocating to warmer environments with different prey and terrain would be extremely challenging. • Competition and Existing Ecosystems: Other bear species or large predators already occupy potential alternative habitats.</p><p>Introducing polar bears would create competition for resources and potentially disrupt existing ecosystems. Polar bears are apex predators, and their presence would have cascading effects. • Limited Land-Based Food Sources: While polar bears can sometimes scavenge on land, terrestrial food sources are insufficient to sustain their large body size and energy demands. They are primarily dependent on the rich calorie content of seals. Land-based food would not be abundant enough to support large populations. • Human-Wildlife Conflict: As polar bears are forced further south or into more populated areas in search of food due to shrinking sea ice, encounters with humans are likely to increase, leading to conflict and potential danger for both bears and people. Relocation efforts could exacerbate this issue. • Distance and Barriers: Potential alternative habitats are often geographically distant from the Arctic. Natural barriers such as mountains, oceans, and unsuitable terrain would make migration extremely difficult. Furthermore, suitable habitat may already be occupied by other species or fragmented by human development. • Genetic Adaptation Takes Time: While some level of adaptation is possible within a species, evolving to thrive in a completely different environment takes many generations. The rapid pace of Arctic change is outpacing the ability of polar bears to adapt genetically to significantly different conditions. Instead of large-scale migration, polar bears are more likely to exhibit smaller-scale shifts in range, dietary changes (though with limited success), and increased hybridization with grizzly bears. However, the long-term survival of the species ultimately depends on mitigating climate change and preserving their Arctic habitat.</p><p>Table <ref type="table" target="#tab_1">20</ref>. Example response of Gemini.</p><p>Can polar bears migrate to new habitats if their Arctic environment changes too drastically? Polar bears are highly specialized for life in the Arctic, relying on sea ice to hunt their primary prey, seals. If the Arctic environment changes too drastically, particularly due to climate change and the loss of sea ice, polar bears would face significant challenges in migrating to new habitats. Here are some key points to consider:</p><p>1. Specialization: Polar bears are adapted to the cold, icy conditions of the Arctic. Their physical and behavioral traits are finely tuned to this environment, making it difficult for them to thrive in vastly different habitats. 2. Prey Availability: Polar bears primarily hunt seals, which are also dependent on sea ice. If sea ice diminishes, both polar bears and their prey would be affected. Moving to new habitats would require finding alternative food sources, which may not be as abundant or accessible. 3. Geographic Barriers: The Arctic is relatively isolated, and polar bears would face significant geographic barriers if they attempted to migrate to new areas. Moving southward would bring them into competition with other bear species, such as grizzly bears, and into environments that are not suited to their specific needs. 4. Human Impact: Many potential new habitats for polar bears are already occupied by humans. Human activities, such as urban development, agriculture, and industry, could pose additional challenges for polar bears trying to adapt to new areas. 5. Genetic Adaptation: Over long periods, species can adapt to new environments through genetic changes. However, the rapid pace of climate change may outstrip the ability of polar bears to adapt quickly enough to survive in significantly altered or new habitats. 6. Conservation Efforts: Conservation strategies are being developed to help polar bears cope with changing environments.</p><p>These include protecting critical habitats, reducing greenhouse gas emissions to mitigate climate change, and researching potential relocation or assisted migration options. In summary, while polar bears have some capacity to adapt to changing conditions, their specialized nature and the rapid pace of environmental change pose significant challenges to their ability to migrate to and thrive in new habitats. Conservation efforts are crucial to ensuring their survival in the face of these challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 21. Example response of DeepSeek</head><p>What are some effective tactics for building a referral network and gaining new business through word-of-mouth? Building a referral network and leveraging word-of-mouth marketing can be a powerful way to grow your business. Here are some effective tactics to help you achieve this:</p><p>1. Deliver Exceptional Service: The foundation of a strong referral network is delivering exceptional service to your existing customers. Ensure that you consistently meet or exceed their expectations, and they will be more likely to refer you to others. 2. Build Relationships: Foster strong relationships with your customers, partners, and colleagues. This can be done through regular communication, shared experiences, and mutual support. 3. Encourage Referrals: Make it easy for your customers to refer you by providing incentives, such as discounts, rewards, or exclusive offers. You can also ask for referrals directly, but be sure to do so in a non-intrusive and respectful manner. 4. Leverage Social Proof: Use customer testimonials, case studies, and reviews to demonstrate your expertise and build credibility. This can be done through social media, your website, or other marketing channels. 5. Host Events and Webinars: Hosting events, webinars, or workshops can help you build relationships with potential customers and establish yourself as an expert in your field. 6. Partner with Other Businesses: Collaborate with complementary businesses to expand your network and reach new audiences. 7. Use Referral Marketing Platforms: Utilize platforms like ReferralCandy, Ambassador, or B2B referral platforms to streamline your referral process and track referrals. 8. Follow Up: Follow up with your referrals and leads to ensure they have a positive experience and to gather feedback. 9. Be Authentic and Transparent: Be genuine and transparent in your interactions with your network. This will help build trust and credibility, making it more likely that people will refer you to others. 10. Measure and Analyze: Track the effectiveness of your referral efforts and analyze the data to refine your strategy and optimize your results.</p><p>Additionally, consider the following:</p><p>• Employee Advocacy: Encourage your employees to become brand ambassadors and share their positive experiences with their networks. • Customer Advocacy: Identify and empower your most loyal customers to become advocates for your business.</p><p>• Content Marketing: Create valuable content that educates and entertains your audience, making them more likely to share it with others. • Influencer Marketing: Partner with influencers in your industry to expand your reach and credibility.</p><p>By implementing these tactics, you can build a strong referral network and leverage word-of-mouth marketing to drive growth and success for your business.</p><p>Table <ref type="table" target="#tab_1">22</ref>. Example response of Llama3. <ref type="bibr">1-8b (instruct)</ref>.</p><p>What are some effective tactics for building a referral network and gaining new business through word-of-mouth? Building a Referral Network &amp; Gaining New Business Through Word-of-Mouth: 1. Deliver Exceptional Service:</p><p>• Exceed expectations: Go above and beyond for every client. Surprise and delight them with unexpected value.</p><p>• Build strong relationships: Get to know your clients personally, understand their needs, and build genuine connections.</p><p>• Be responsive and reliable: Respond promptly to inquiries, meet deadlines, and follow through on promises.</p><p>2. Make it Easy to Refer:</p><p>• Create a referral program: Offer incentives for successful referrals, such as discounts, free services, or exclusive perks.</p><p>• Provide referral materials: Make it easy for clients to share your information with others by providing referral cards, social media share buttons, and email templates. • Streamline the process: Make it simple for clients to refer friends and family with a clear and concise process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Nurture Your Network:</head><p>• Stay in touch: Regularly communicate with your existing clients through email newsletters, social media updates, or personalized messages. • Attend industry events: Network with other professionals and potential referral partners.</p><p>• Join relevant online communities: Engage in online forums and groups related to your industry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Leverage Social Media:</head><p>• Share valuable content: Provide helpful tips, insights, and resources that your target audience will find valuable.</p><p>• Encourage user-generated content: Ask clients to share their experiences with your business on social media.</p><p>• Run contests and giveaways: Offer incentives for people to share your content and refer their friends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ask for Referrals:</head><p>• Don't be afraid to ask: Politely request referrals from satisfied clients.</p><p>• Make it personal: Explain why you value their referrals and how they can help your business grow.</p><p>• Offer specific suggestions: Provide examples of people who might benefit from your services.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Track Your Results:</head><p>• Monitor your referral rate: Track the number of referrals you receive and the conversion rate of those referrals.</p><p>• Analyze your data: Identify which tactics are most effective and adjust your strategy accordingly.</p><p>• Celebrate your successes: Recognize and reward your top referrers.</p><p>Remember, building a strong referral network takes time and effort. Be patient, consistent, and focus on providing exceptional value to your clients.</p><p>Table <ref type="table" target="#tab_3">23</ref>. Example response of Gemma2-9b (instruct).</p><p>What are some effective tactics for building a referral network and gaining new business through word-of-mouth? Building a referral network and leveraging word-of-mouth to gain new business is a powerful strategy. Here are some effective tactics to consider:</p><p>1. Identify Your Ideal Referral Sources:</p><p>• Determine who your ideal customers are and who they might trust to recommend your services or products. This could be current customers, industry influencers, or other businesses in your network. 2. Create a Referral Program:</p><p>• Develop a clear and attractive referral program that incentivizes both the referrer and the referred. Incentives can range from discounts, free services, or even just a simple thank you note.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Educate Your Network:</head><p>• Provide your network with the necessary information to make a referral. This could include testimonials, case studies, or a simple one-pager that explains the benefits of your product or service. 4. Make It Easy to Refer:</p><p>• Provide easy-to-use tools for your network to make referrals. This could be a referral form, a link, or a simple email template they can use. 5. Leverage Social Media:</p><p>• Use social media platforms to connect with potential referrers and to share your referral program. Platforms like LinkedIn, Twitter, and Facebook can be particularly effective. 6. Host Events and Networking Opportunities:</p><p>• Organize events or participate in networking opportunities where you can meet potential referrers in person. Personal connections can often lead to more referrals. 7. Provide Excellent Customer Service:</p><p>• Ensure that your current customers have a positive experience with your product or service. Satisfied customers are more likely to refer others. 8. Follow Up and Show Appreciation:</p><p>• Keep in touch with your network and show appreciation for their referrals. A simple thank you note or a small token of appreciation can go a long way. 9. Use Testimonials and Case Studies:</p><p>• Share positive testimonials and case studies from satisfied customers. This can help build trust and encourage others to refer to you. 10. Be Patient and Persistent:</p><p>• Building a referral network takes time. Be patient and persistent in your efforts. Consistency is key. 11. Monitor and Analyze:</p><p>• Keep track of your referral program's performance and analyze the data to see what's working and what isn't. Use this information to refine your approach. 12. Offer Referral Bonuses:</p><p>• Consider offering bonuses for repeat referrals or for referring a certain number of new customers. This can help keep the momentum going. By implementing these tactics, you can build a strong referral network and leverage word-of-mouth to grow your business.</p><p>Table <ref type="table" target="#tab_5">24</ref>. Example response of Qwen2. <ref type="bibr">5-7b (instruct)</ref>.</p><p>Newswise -Sepsis is a serious medical condition caused by an overwhelming immune response to infection. The body's infection-fighting chemicals , which are normally good, can cause damage to the blood vessels and organs. Sepsis is one of the leading causes of death in hospitals. Although there have been recent advances in sepsis treatment, researchers are still working hard to identify new ways to prevent and treat this complex disease. Researchers at Washington University School of Medicine in St. Louis recently published a study that may change how sepsis is diagnosed and treated. Their work suggests that two different types of immune cells are involved in the progression of sepsis: one cell type that's protective and another that's harmful. These findings could help doctors better identify who is most at risk for severe complications from the disease. "The results from our study show that there are two distinct populations of monocytes," said co-author Dr. Michael Jaffe, professor of medicine at Washington University. "One population promotes inflammation and is associated with worse outcomes; the other helps resolve inflammation and promotes healing." Monocytes are a type of white blood cell that are part of the innate immune system. They play a key role in fighting off bacteria and viruses. When someone has sepsis, their monocytes become activated. That means they increase in number and start producing cytokines, which are small proteins that regulate the body's immune response. Typically, when the body encounters an infection, these cytokines signal the immune system to fight back. But too many cytokines produced too quickly can result in a hyperinflammatory state, which can lead to organ failure and death. In the past, doctors have used blood tests to diagnose sepsis. However, those tests only measure the overall levels of cytokines in a person's blood. The new study suggests that it might be possible to determine whether someone has a particular type of monocyte that could make them more susceptible to sepsis. If so, doctors could potentially use this information to personalize treatments and improve patient outcomes. "Our findings suggest that there may be opportunities to develop therapies that target specific monocyte subsets," said co-author Dr. Daniel Levy, assistant professor of medicine at Washington University. "This could potentially reduce the risk of developing sepsis or even improve outcomes once someone already has sepsis." There's still much to learn about how these monocyte subsets contribute to sepsis development and progression, but the results of this study provide promising insights into why some patients respond differently to treatment than others.</p><p>Table <ref type="table" target="#tab_14">29</ref>. Example response of Mistral-v3-7b (base).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Different numbers of training samples. Our sequence classifiers benefit from more training samples. The classification performance converges when using about 10K training samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Example responses from ChatGPT and Claude, showcasing their idiosyncrasies: characteristic phrases (left) and unique markdown formatting (right). For clarity, we highlight each characteristic phrase with underline and model-specific color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Frequencies of words and letters. The top 20 most frequently used words of LLMs (left) exhibit distinct patterns for each model, but their letter frequencies (right) are very similar. Results are on the chat API models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Characteristic phrases. We train a logistic regression model on TF-IDF features of chat APIs' outputs and extract the top 10 phrases for each LLM based on the coefficients of these features. We remove common words shared across these LLMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. First word. We analyze the distribution of the first word in chat APIs' responses, with the top 10 most frequent words for each model. These differences in first-word usage explain the non-trivial accuracy with only the first word in Figure2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Markdown formatting elements. Each LLM has a distinctive distribution of markdown formatting elements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure</head><label></label><figDesc>Figure 4b visualizes howChatGPT and Claude structure their responses in markdown. Interestingly, ChatGPT tends to emphasize each key point within enumerations in bold and highlight a title with markdown headers, but Claude formats text with simple enumeration and bullet points. More analysis on these markdown formatting elements for other models can be found in Appendix B.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Prompts in our open-ended language analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Confusion matrices for N -way classifiers on three groups of LLMs: chat APIs, instruct LLMs, and base LLMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Word and letter frequencies in instruct and base LLMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Distribution of first word choices in instruct and base LLMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 17 .</head><label>17</label><figDesc>Figure 17. Results of our open-ended language analysis on ChatGPT and Claude with different LLM judges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 18 .</head><label>18</label><figDesc>Figure 18. Additional results of our open-ended language analysis on chat APIs (top) and instruct LLMs (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Examples of only using markdown formatting elements and replacing text content with "xxx" placeholders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>video chat with friends and family • Send messages, emails, or letters • Join online communities or social media groups • Schedule regular virtual meetups 2. Pursue hobbies and interests: • Take up a new hobby • Join clubs or groups related to your interests • Take online classes • Learn a new skill 3. Physical activities: professional help: • Talk to a therapist or counselor • Join support groups • Consider online therapy options 7. Adopt or care for a pet: • Pets provide companionship • Create a sense of purpose • Encourage routine and responsibility Remember that feeling lonely is a common human experience, and it's okay to reach out for help when needed. (b) Claude Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Classification</figDesc><table><row><cell>ChatGPT</cell><cell>Claude</cell><cell>Grok</cell><cell>Gemini</cell><cell>DeepSeek</cell><cell>acc. (chat)</cell></row><row><cell>✓</cell><cell>✓</cell><cell></cell><cell></cell><cell></cell><cell>99.3</cell></row><row><cell>✓</cell><cell></cell><cell>✓</cell><cell></cell><cell></cell><cell>97.7</cell></row><row><cell>✓</cell><cell></cell><cell></cell><cell>✓</cell><cell></cell><cell>98.7</cell></row><row><cell>✓</cell><cell></cell><cell></cell><cell></cell><cell>✓</cell><cell>97.2</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell></cell><cell></cell><cell>99.7</cell></row><row><cell></cell><cell>✓</cell><cell></cell><cell>✓</cell><cell></cell><cell>99.6</cell></row><row><cell></cell><cell>✓</cell><cell></cell><cell></cell><cell>✓</cell><cell>99.6</cell></row><row><cell></cell><cell></cell><cell>✓</cell><cell>✓</cell><cell></cell><cell>99.4</cell></row><row><cell></cell><cell></cell><cell>✓</cell><cell></cell><cell>✓</cell><cell>98.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>✓</cell><cell>✓</cell><cell>99.9</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>97.1</cell></row><row><cell></cell><cell></cell><cell cols="2">(a) chat APIs</cell><cell></cell><cell></cell></row><row><cell>Llama</cell><cell>Gemma</cell><cell>Qwen</cell><cell>Mistral</cell><cell>acc. (instruct)</cell><cell>acc. (base)</cell></row><row><cell>✓</cell><cell>✓</cell><cell></cell><cell></cell><cell>99.9</cell><cell>98.3</cell></row><row><cell>✓</cell><cell></cell><cell>✓</cell><cell></cell><cell>97.8</cell><cell>81.7</cell></row><row><cell>✓</cell><cell></cell><cell></cell><cell>✓</cell><cell>97.0</cell><cell>96.3</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell></cell><cell>99.9</cell><cell>98.3</cell></row><row><cell></cell><cell>✓</cell><cell></cell><cell>✓</cell><cell>99.9</cell><cell>98.4</cell></row><row><cell></cell><cell></cell><cell>✓</cell><cell>✓</cell><cell>96.1</cell><cell>95.7</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>96.3</cell><cell>87.3</cell></row><row><cell></cell><cell></cell><cell cols="2">(b) instruct and base LLMs</cell><cell></cell><cell></cell></row></table><note><p>with a Naccuracies for various LLM combinations. Top: results for chat APIs. Bottom: results for instruct and base LLMs. Check marks (✓) denote the models included in each combination. We observe high classification accuracies consistently across all model combinations, indicating the presence of distinct idiosyncrasies in LLMs.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Classification</figDesc><table /><note><p>within Qwen2.5 model family. The classifier can differentiate responses between LLMs within the same model family with reasonably well accuracies.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">, our classifiers</cell></row><row><cell cols="5">generalize well across different datasets, indicating that they</cell></row><row><cell cols="4">learn very robust and transferable patterns.</cell><cell></cell></row><row><cell cols="5">train / test UltraChat Cosmopedia LmsysChat WildChat</cell></row><row><cell>UltraChat</cell><cell>96.3</cell><cell>98.9</cell><cell>89.9</cell><cell>92.4</cell></row><row><cell>Cosmopedia</cell><cell>95.7</cell><cell>99.8</cell><cell>88.3</cell><cell>94.9</cell></row><row><cell>LmsysChat</cell><cell>94.7</cell><cell>97.2</cell><cell>91.8</cell><cell>92.0</cell></row><row><cell>WildChat</cell><cell>95.1</cell><cell>99.1</cell><cell>90.2</cell><cell>95.7</cell></row></table><note><p>These datasets originate from different sources and are designed for various purposes -Cosmopedia is designed for synthetic data generation, LmsysChat and WildChat capture real-world user interactions, while UltraChat consists primarily of synthetic responses. For each dataset, we train a classifier on a group of model responses and evaluate the classifier across all four datasets. We use instruct LLMs for this experiment. As shown in</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Robust</figDesc><table /><note><p>generalization to out-of-distribution responses.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>• Format control: Please provide your response in plain text only, avoiding the use of italicized or bold text, lists, markdown, or HTML formatting. Controlling</figDesc><table><row><cell cols="3">LLM outputs after these interventions are presented in Ap-</cell></row><row><cell cols="3">pendix C. We find that LLMs can follow the additional</cell></row><row><cell cols="2">instructions in generating responses.</cell><cell></cell></row><row><cell cols="3">original length control format control</cell></row><row><cell>instruct LLMs 96.3</cell><cell>93.0</cell><cell>91.4</cell></row></table><note><p>LLM outputs with prompts. An instruction is added to the original prompt to specify the output length and format. Length control limits responses to one paragraph. Format control ensures that responses are in plain text without any format.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">100</cell><cell></cell></row><row><cell>accuracy (%)</cell><cell>25 50 75</cell><cell></cell><cell>chat instruct base</cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2 input sequence length for the classifier 4 8 16 32 64 128 256 512</cell></row><row><cell cols="4">Figure 2. Ablations on input length of text embedding models.</cell></row><row><cell cols="4">Classification accuracies improve as the text embedding models</cell></row><row><cell cols="4">capture more context. Performance begins to saturate beyond an</cell></row><row><cell cols="4">input sequence length of 256. Note that the three lines represent</cell></row><row><cell cols="4">different groups of LLMs and are not directly comparable.</cell></row></table><note><p>presents the results for this analysis. We can see that neural networks still perform excellently for classifying LLM outputs applied with length and format control prompts. These findings suggest that LLM characteristics are deeply embedded in the generated text, persisting despite surface-level constraints on length and formatting.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Classifications with different sampling methods.</figDesc><table><row><cell></cell><cell cols="4">greedy softmax top-k top-p</cell></row><row><cell>greedy</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">softmax 59.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>top-k</cell><cell>58.2</cell><cell>50.0</cell><cell>-</cell><cell>-</cell></row><row><cell>top-p</cell><cell>52.9</cell><cell>51.0</cell><cell>52.1</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dis-</cell></row><row><cell cols="5">tinguishing responses generated by the same model using different</cell></row><row><cell cols="5">sampling strategies is only marginally better than chance accuracy.</cell></row><row><cell cols="5">The results are on Llama3.1-8b instruct model's responses.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Different sequence embedding models. LLM2vec achieves the best performance in classifying outputs from various LLMs among the five embedding models we study.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>2.</cell></row><row><cell>method</cell><cell cols="3">chat instruct base</cell></row><row><cell>ELMo</cell><cell>90.8</cell><cell>91.0</cell><cell>69.8</cell></row><row><cell>BERT</cell><cell>91.1</cell><cell>91.5</cell><cell>66.0</cell></row><row><cell>T5</cell><cell>90.5</cell><cell>89.8</cell><cell>67.9</cell></row><row><cell>GPT-2</cell><cell>92.1</cell><cell>92.3</cell><cell>80.2</cell></row><row><cell cols="2">LLM2vec 97.1</cell><cell>96.3</cell><cell>87.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>shows the results. All sequence embedding models</cell></row><row><cell>can achieve very high accuracies. The classification perfor-</cell></row><row><cell>mance improves with more advanced sequence embedding</cell></row><row><cell>models. Among all methods, LLM2vec demonstrates the</cell></row><row><cell>best performance, achieving 97.1% on chat APIs, 96.3% on</cell></row><row><cell>instruct LLMs, and 87.3% on base LLMs.</cell></row><row><cell>Training data size. We vary the number of training sam-</cell></row><row><cell>ples generated by LLMs and train the classifier with the</cell></row><row><cell>same total number of iterations. We present the results in</cell></row><row><cell>Figure 3. The performance of the classifier increases as it</cell></row><row><cell>is trained with more training samples. This trend is con-</cell></row><row><cell>sistently observed across chat APIs, instruct LLMs, and</cell></row><row><cell>base LLMs. Furthermore, as few as 10 training samples,</cell></row><row><cell>the classifier achieves non-trivial accuracy (e.g., 40.3% on</cell></row><row><cell>instruct LLMs), surpassing 20% chance-level guess.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="3">, responses from different LLMs exhibit lower text</cell></row><row><cell cols="3">similarities than those from the same model.</cell></row><row><cell></cell><cell cols="2">across LLMs within an LLM</cell></row><row><cell>ROUGE-1</cell><cell>0.499</cell><cell>0.660</cell></row><row><cell>ROUGE-L</cell><cell>0.256</cell><cell>0.414</cell></row><row><cell>BERTScore  *</cell><cell>0.220</cell><cell>0.482</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Text similarity scores. We evaluate the text similarity of LLM outputs using ROUGE-1, ROUGE-L, and BERTScore. * We follow<ref type="bibr" target="#b42">Zhang et al. (2020)</ref> to rescale BERTScore with respect to the human baseline. The results indicate that responses from different LLMs exhibit low lexical similarity.</figDesc><table><row><cell>Our products feature innovative</cell><cell>According to the text, Kai Fusser</cell><cell>1. Deliver Exceptional Service: The</cell><cell>1. Deliver Exceptional Service</cell></row><row><cell>sustainable materials, such as …</cell><cell>believes that traditional cardio …</cell><cell>foundation of word-of-mouth</cell><cell>• Consistently exceed customer</cell></row><row><cell>Certainly! If you're looking for cheese</cell><cell>Based on the text provided, here are</cell><cell>marketing is consistent excellence.</cell><cell>• expectations</cell></row><row><cell>alternatives to replace Brie in your …</cell><cell>the key details about Armon Binns' …</cell><cell>Providing top-notch services or …</cell><cell>• Focus on quality and attention …</cell></row><row><cell>Overall, while there are challenges,</cell><cell>While many winter sports in the</cell><cell>Ingredients:</cell><cell>Ingredients:</cell></row><row><cell>Tanzania is making progress …</cell><cell>Pyrenees are similar to those found …</cell><cell>• 2 (3 oz) packages of orange-</cell><cell>• 2 boxes orange-flavored Jello</cell></row><row><cell>Sure! Here's a simple guide to cooking</cell><cell>This appears to be a fragment of</cell><cell>flavored Jello</cell><cell>• 1 can evaporated milk</cell></row><row><cell>a juicy salmon fillet: …</cell><cell>poetry that creates a pastoral …</cell><cell>• 1 cup tonic water (this is what …</cell><cell>• Tonic water …</cell></row><row><cell>ChatGPT</cell><cell>Claude</cell><cell>ChatGPT</cell><cell>Claude</cell></row><row><cell cols="2">(a) characteristic phrases</cell><cell cols="2">(b) unique markdown formatting</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell cols="2">presents the classification results.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">chat instruct base</cell></row><row><cell>original</cell><cell>97.1</cell><cell>96.3</cell><cell>87.3</cell></row><row><cell cols="2">removing special characters 95.1</cell><cell>93.8</cell><cell>75.4</cell></row><row><cell>shuffling words</cell><cell>88.9</cell><cell>88.9</cell><cell>68.3</cell></row><row><cell>shuffling letters</cell><cell>39.1</cell><cell>38.6</cell><cell>38.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 .</head><label>8</label><figDesc>Classifications with only words and letters. While removing special characters and shuffling words have little impact on accuracies, shuffling letters greatly reduces the performance.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 .</head><label>9</label><figDesc>Table9shows the classification results after this transformation. Classifications with only markdown elements. Using markdown elements can achieve high accuracies for chat APIs and instruct LLMs, but marginally better results for base LLMs.</figDesc><table><row><cell></cell><cell cols="3">chat instruct base</cell></row><row><cell>original</cell><cell>97.1</cell><cell>96.3</cell><cell>87.3</cell></row><row><cell cols="2">markdown elements only 73.1</cell><cell>77.7</cell><cell>38.5</cell></row></table><note><p>Surprisingly, we observe our classifiers achieve high accuracies of 73.1% for chat APIs and 77.7% for instruct LLMs. However, the classification accuracies with base LLMs' responses are near chance-level guess (25%). This is likely because base LLMs tend to generate responses in plain text.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 .</head><label>10</label><figDesc>Classifications on rewritten responses. Paraphrasing or translating LLM outputs achieves an accuracy comparable to that using original counterparts. However, summarizing these texts makes the model less capable of predicting LLM identities.Specifically, we present an LLM judge with two responsesgenerated by different models based on the same promptand ask it to analyze these responses from different angles (e.g., tone and content). This process is repeated multiple times to gather a comprehensive collection of analyses. Finally, we query the LLM judge to summarize these analyses into bullet points that capture the characteristics of each model. The prompts are detailed in Appendix A.3.</figDesc><table><row><cell></cell><cell cols="3">chat instruct base</cell></row><row><cell>original</cell><cell>97.8</cell><cell>96.3</cell><cell>87.3</cell></row><row><cell cols="2">paraphrasing 91.4</cell><cell>92.2</cell><cell>71.7</cell></row><row><cell>translating</cell><cell>91.8</cell><cell>92.7</cell><cell>74.0</cell></row><row><cell cols="2">summarizing 58.1</cell><cell>57.5</cell><cell>44.7</cell></row><row><cell cols="4">Moreover, despite a noticeable accuracy drop (i.e., &gt;38%)</cell></row><row><cell cols="4">with the summarized text, the resulting performance remains</cell></row><row><cell cols="4">well above chance-level guess. This remarkable ability to</cell></row><row><cell cols="4">classify the summarized texts shows the high-level semantic</cell></row><row><cell cols="3">difference in LLM-generated responses.</cell><cell></cell></row></table><note><p><p><p><p><p><p>Open-ended language analysis. In this part, we focus on studying the semantic difference in responses generated by LLMs. We employ another LLM (e.g., ChatGPT) as a judge to provide open-ended, descriptive characterizations for each LLM's outputs. The results with other LLM judges for our language analysis are available in Appendix B.5.</p>The results of open-ended language analysis on ChatGPT vs. Claude are shown in Figure</p>9</p>. For a detailed pairwise comparison of the responses, see Figure</p>16</p>in Appendix C. ChatGPT is characterized by descriptive and detailed responses in an engaging tone. In contrast, Claude prioritizes simplicity with only key points and straightforward language. Additional results on chat API models and instruct LLMs are provided in Appendix B.5.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Inferring model similarity. Our framework offers a quantitative approach for assessing similarities between proprietary and open-weight LLMs. Given a set of N LLMs, we omit one model and train a classifier on responses from the remaining N -1 models. We then evaluate which LLM the classifier associates the responses of the excluded model with. The model that is most frequently predicted as the source is considered the closest match to the excluded LLM. This process is repeated for each of the N models. For this analysis, we include the open-weight Phi-4(Abdin et al.,  2024b)  alongside 5 chat API models. Notably, Phi-4 uses a substantial amount of synthetic data in its training.</figDesc><table><row><cell>90%</cell><cell cols="6">Prediction of ChatGPT outputs ChatGPT</cell><cell cols="5">Prediction of Claude outputs Claude</cell></row><row><cell>30% 60%</cell><cell>2.8</cell><cell>31.9</cell><cell>2.2</cell><cell>7.2</cell><cell>55.9</cell><cell></cell><cell>52.9</cell><cell>20.8</cell><cell>8.2</cell><cell>5.6</cell><cell>12.5</cell></row><row><cell></cell><cell cols="5">Claude Grok GeminiDeepSeek Phi-4</cell><cell cols="6">ChatGPT Grok GeminiDeepSeek Phi-4</cell></row><row><cell cols="2">90% 82.8</cell><cell cols="4">Prediction of Grok outputs Grok</cell><cell></cell><cell cols="5">Prediction of Gemini outputs Gemini</cell></row><row><cell cols="6">ChatGPTClaude GeminiDeepSeek Phi-4 30% 60% 1.1 1.2 5.8 9.1</cell><cell cols="6">ChatGPTClaude Grok DeepSeek Phi-4 40.9 5.3 26.5 22.0 5.2</cell></row><row><cell>90%</cell><cell cols="6">Prediction of DeepSeek outputs 76.0 DeepSeek</cell><cell></cell><cell cols="4">Prediction of Phi-4 outputs</cell></row><row><cell>60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>46.4</cell><cell></cell><cell></cell><cell></cell><cell>51.3</cell></row><row><cell cols="6">ChatGPTClaude Grok Gemini Phi-4 30% 16.9 0.2 6.6 0.3</cell><cell cols="6">ChatGPTClaude Grok GeminiDeepSeek 0.7 1.4 0.2</cell></row></table><note><p>These findings suggest that training with synthetic data can propagate the idiosyncrasies in the source model.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>Whilethe synthetic task shown in Figure1is conceptually similar to dataset classification, we focus not on training datasets but on the distinctive characteristics inherent to LLMs.</figDesc><table><row><cell>Torralba &amp; Efros (2011) introduced the "Name That Dataset" experiment a decade ago to high-light the bias present in visual datasets of that time. Recently, Liu &amp; He (2025) revisited this problem (termed dataset clas-sification) and found that current large-scale, supposedly more diverse visual datasets are still very biased. Zeng et al. Human vs. machine-generated texts. Many prior works have studied the problem of determining if a text is authored by a human or an AI system (Mitchell et al., 2023; Mc-Govern et al., 2024). Model-free approaches typically use linguistic properties such as n-gram frequencies (Badaskar</cell></row></table><note><p><p><p><p><p><p>(2024)  </p>further identified structural and semantic components in images as key contributors to these biases.</p><ref type="bibr" target="#b40">You et al. (2024)</ref> </p>and</p><ref type="bibr" target="#b23">Mansour &amp; Heckel (2024)</ref> </p>applied the dataset classification framework to study bias in synthetic images and LLM pretraining datasets respectively.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>Determine the unique characteristics of the 2 distributions, and summarize them with five bullet points for each. Each bullet point should analyze the response from the same angle and can be easily compared by a human. Focus on key aspects that differentiate the distributions. Focus on the overall tone, specific word choices, opening lines, markdown formatting, and content. Output only 5 bullet points per distribution, no additional text. summaries into a single summary. Focus on how each summary evaluates the overall tone, specific word choices, opening lines, markdown formatting, and content. Provide exactly five bullet points for each distribution, one for each feature. Each bullet point should consist of a descriptive title characterizing the feature and a short sentence explaining it concisely. Emphasize recurring and significant features, remove the redundant ones, and ensure the titles and sentences clearly differentiate the distributions for easy human comparison. Output only 5 bullet points per distribution, no additional text.</figDesc><table><row><cell></cell><cell>Below are 35 summaries that compare the unique features in 2 text</cell></row><row><cell></cell><cell>distributions point by point.</cell></row><row><cell></cell><cell>{Analysis 1}</cell></row><row><cell>Sample from distribution {1, 2}: {text sample 1} Sample from distribution {1, 2}: {text sample 2}</cell><cell>{Analysis 2} … {Analysis 35}</cell></row><row><cell></cell><cell>Condense all 35</cell></row><row><cell>(a) analysis prompt</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>Characteristic phrases for instruct 2 and base LLMs.</figDesc><table><row><cell>including</cell><cell>essentially</cell><cell>certainly</cell><cell></cell><cell>title</cell><cell>and</cell><cell>strong</cell><cell>the</cell><cell>is</cell></row><row><cell>the following</cell><cell>know if</cell><cell>below is</cell><cell cols="2">absolutely</cell><cell>re</cell><cell>h2</cell><cell>re</cell><cell>the</cell></row><row><cell>example use</cell><cell>me know</cell><cell cols="2">certainly! here</cell><cell>as</cell><cell>ve</cell><cell>em</cell><cell>ll</cell><cell>and</cell></row><row><cell>such as</cell><cell>let me</cell><cell cols="2">certainly! below</cell><cell>also</cell><cell>ll</cell><cell>h3</cell><cell>or</cell><cell>are</cell></row><row><cell>use case</cell><cell>here's a breakdown</cell><cell>enhance</cell><cell></cell><cell>help</cell><cell>the</cell><cell>blockquote</cell><cell>you re</cell><cell>to</cell></row><row><cell>which</cell><cell>breakdown</cell><cell>detailed</cell><cell cols="2">title comprehensive</cell><cell>In</cell><cell>as</cell><cell>answer</cell><cell>in</cell></row><row><cell>explanation the</cell><cell>states that</cell><cell>these</cell><cell></cell><cell>for</cell><cell>1 1</cell><cell>there</cell><cell>specific</cell><cell>in the</cell></row><row><cell>that</cell><cell>specific</cell><cell>can</cell><cell></cell><cell>an</cell><cell>to</cell><cell>here</cell><cell>might</cell><cell>you</cell></row><row><cell>2020</cell><cell>remember</cell><cell>broader</cell><cell cols="2">however</cell><cell>for</cell><cell>after</cell><cell>throughout</cell><cell>your</cell></row><row><cell>summary</cell><cell>states</cell><cell cols="2">certainly! the</cell><cell>during</cell><cell>they re</cell><cell>which</cell><cell>like</cell><cell>it</cell></row><row><cell>Llama</cell><cell>Gemma</cell><cell>Qwen</cell><cell cols="2">Mistral</cell><cell>Llama</cell><cell>Gemma</cell><cell>Qwen</cell><cell>Mistral</cell></row><row><cell></cell><cell cols="2">(a) instruct LLMs</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) base LLMs</cell></row><row><cell cols="3">Figure 14. it comprehensive title to in as based according here the Llama 0.12 0.24 0.36 according python it okay you please as this here 0.12 sure according comprehensive based creating to title in the certainly Qwen 0.12 0.24 0.36 creating subject based yes absolutely here to in 0.12</cell><cell>Gemma 0.24 the Mistral 0.24</cell><cell>the 0.36 title 0.36</cell><cell cols="2">this we for it is in of to 0.02 0.04 0.06 0.08 and the Llama you for it this is in to of and the Qwen 0.02 0.04 0.06 0.08</cell><cell cols="2">Gemma and 0.02 0.04 0.06 0.08 the of to in it is this for we Mistral the and to of in is for it with on 0.02 0.04 0.06 0.08</cell></row><row><cell></cell><cell cols="2">(a) instruct LLMs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 12 .</head><label>12</label><figDesc>Examples of prompt-level interventions on ChatGPT.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 13 .</head><label>13</label><figDesc>Examples of prompt-level interventions on Llama3.1-8b-Instruct.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 17 .</head><label>17</label><figDesc>Example response of ChatGPT.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>In LLama of instruct LLMs, the phrase "explanation the" corresponds to a markdown header or bold text for "explanation" followed by a new sentence starting with "the".</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Zekai Wang</rs> for valuable discussions. <rs type="person">Mingjie Sun</rs> was supported by funding from the <rs type="funder">Bosch Center for Artificial Intelligence</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Response Generation</head><p>We report our procedure for generating responses from chat APIs, instruct LLMs, and base LLMs. For chat APIs, we access a stable version of each model, including <ref type="bibr">GPT-4o-2024</ref><ref type="bibr">-08-06, Claude-3.5-Sonnet-20241022, Grok-Beta, Gemini-1.5-Pro-002, and DeepSeek-Chat, through its official API between November 28, 2024</ref><ref type="bibr">, and February 6, 2025</ref>, generating responses with their default sampling setting. For instruct LLMs, we use greedy decoding to sample outputs. For base LLMs, we set the temperature to T = 0.6 and apply a repetition penalty of 1.1 to avoid repetitive completions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Training Setup</head><p>In this part, we describe our fine-tuning process using the text embedding models on LLM responses. We use the first 512 tokens of each generated response for training and evaluation. To perform sequence classification, we add a linear layer as the classification head on top of each text embedding model. For ELMo, BERT, LLM2vec, this layer is applied to the average embeddings over all tokens in a sequence. For T5 and GPT-2, we follow the original setups <ref type="bibr" target="#b31">(Radford et al., 2019;</ref><ref type="bibr" target="#b32">Raffel et al., 2020)</ref> and apply the head on the output of the last token.</p><p>For smaller text embedding models, such as ELMo, BERT, T5, and GPT-2, we fine-tune the entire model along with the classification head, searching over base learning rates {3e-3, 1e-3, 3e-4, 1e-4, 3e-5, 1e-5, 3e-6, 1e-6}. For the largest LLM2vec model, we employ the parameter-efficient LoRA <ref type="bibr" target="#b13">(Hu et al., 2022)</ref> fine-tuning method with a rank of 16, LoRA α of 32, a dropout rate of 0.05, and a base learning rate of 5e-5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Idiosyncrasies in Large Language Models</head><p>Newswise -Sepsis is a serious medical condition caused by an overwhelming immune response to infection. The body's infection-fighting chemicals trigger widespread inflammation , which can lead to blood clots and leaky blood vessels. It damages the tiny capillaries in organs such as lungs, liver, kidney and brain. According to recent estimates, 1 million people die from sepsis each year. In the United States alone, it affects more than one million hospital patients annually and accounts for over $20 billion in healthcare costs, making it the most expensive condition treated in intensive care units (ICUs). A research team led by Dr. Robert W. Day at Washington University School of Medicine found that a common drug used for treating type 2 diabetes may reduce mortality rates among ICU patients with severe sepsis or septic shock. The researchers examined data on more than 340,000 adult patients admitted to ICUs across the U.S. between January 2015 and December 2017. Approximately 50% were diagnosed with severe sepsis or septic shock. They discovered that the use of metformin -the first-line treatment for type 2 diabetes -was associated with a significant reduction in the risk of death within 30 days after admission to the ICU. The benefit was even greater among those who didn't have pre-existing diabetes. The findings suggest that metformin could be considered as part of standard practice for all ICU patients with severe sepsis or septic shock. Metformin has been shown to improve outcomes in animal models of sepsis. According to the researchers, this study provides further evidence linking metformin use with better survival rates in human subjects experiencing extreme illness. Newswise -Sepsis is a serious medical condition caused by an overwhelming immune response to infection. The body's infection-fighting chemicals trigger widespread inflammation, which can damage healthy organs and blood vessels. Sepsis is treated in the hospital with antibiotics and fluids. If sepsis isn't detected early or if treatment doesn't work, multiple organ failure occurs and may lead to death.</p><p>Table <ref type="table">28</ref>. Example response of Qwen2.5-7b (base).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Abdin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Awadalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Awan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bahree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bakhtiari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Benhaim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bjorck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.14219</idno>
		<title level="m">Phi-3 technical report: A highly capable language model locally on your phone</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Abdin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hewett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Javaheripi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kauffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C T</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Saarikivi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.08905</idno>
		<title level="m">Phi-4 technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Identifying real or fake articles: Towards better language modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Anthropic ; Badaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<ptr target="https://www.anthropic.com/news/claude-3-5-sonnet" />
	</analytic>
	<monogr>
		<title level="j">IJCNLP</title>
		<imprint>
			<date type="published" when="2008">2024. 2008</date>
		</imprint>
	</monogr>
	<note type="report_type">Claude-3.5-sonnet</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast-detectgpt: Efficient zero-shot detection of machinegenerated text via conditional probability curvature</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Llm2vec: Large language models are secretly powerful text encoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Behnamghader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Adlakha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mosbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Werra</surname></persName>
		</author>
		<author>
			<persName><surname>Cosmopedia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.19437</idno>
		<ptr target="https://huggingface.co/datasets/HuggingFaceTB/cosmopedia" />
		<title level="m">DeepSeek-AI. Deepseek-v3 technical report</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Enhancing chat language models by scaling high-quality instructional conversations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<title level="m">The llama 3 herd of models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Describing differences in image sets with natural language</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dunlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung-Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discover and quantify qualitative differences in large language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dunlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><surname>Vibecheck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model equality testing: Which model is this api serving?</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gltr: Statistical detection and visualization of generated text</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.05530</idno>
	</analytic>
	<monogr>
		<title level="m">ACL, 2019. Google. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Lora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Authorship attribution in the era of llms: Problems, methodologies, and challenges</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.08946</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic detection of generated text is easiest when humans are fooled</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duckworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic detection of machine generated text: A critical survey</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdul-Mageed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Lakshmanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Lavaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Sayed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06825</idno>
		<title level="m">Mistral 7b</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense</title>
		<author>
			<persName><forename type="first">K</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karpinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detecting fake content with relative entropy scoring</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lavergne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Urvoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Best practices and lessons learned on synthetic data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A decade&apos;s battle on dataset bias: Are we there yet?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heckel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.02857</idno>
		<title level="m">Measuring bias of web-filtered text datasets and bias propagation through training</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Your large language models are leaving fingerprints</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mcgovern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stureborg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Suhara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alikaniotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCL Workshops</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Detectgpt: Zero-shot machine-generated text detection using probability curvature</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khazatsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gpt-2 output dataset detection</title>
		<ptr target="https://github.com/openai/gpt-2-output-dataset" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>OpenAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Gpt-4o system card</title>
		<idno type="arXiv">arXiv:2410.21276</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
		<respStmt>
			<orgName>OpenAI</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kydlíček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.17575</idno>
		<title level="m">The fineweb datasets: Decanting the web for the finest text data at scale</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">:</forename><surname>Qwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.15115</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>and Qiu, Z. Qwen2.5 technical report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Gemma 2: Improving open language models at a practical size</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Sessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hussonot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shahriari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.00118</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Can ai-generated text be reliably detected?</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Sadasivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feizi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2025">2025</date>
			<publisher>TMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Authorship attribution for neural text generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Uchendu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Turingbench: A benchmark environment for turing test in the age of neural text generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Uchendu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<ptr target="https://x.ai/blog/grok-2" />
		<title level="m">Grok-2 beta release</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.18029</idno>
		<title level="m">Are images indistinguishable to humans also indistinguishable to classifiers? arXiv preprint</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Understanding bias in largescale visual datasets</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Evaluating text generation with bert</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName><surname>Bertscore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Wildchat: 1m chatgpt interaction logs in the wild</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Lmsys-chat-1m: A large-scale real-world llm conversation dataset</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Explaining datasets in words: Statistical models with natural language parameters</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Provide exceptional service: The foundation of a strong referral network is built on delivering outstanding service to your current clients. Happy customers are more likely to refer you to others</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Ask for referrals: Don&apos;t be shy about asking satisfied clients for referrals. Make it easy for them by providing referral cards, email templates, or a simple script they can use to introduce you to their contacts</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Offer incentives: Consider offering incentives for referrals, such as discounts, freebies, or other rewards. This can motivate your clients to refer more people to your business</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Attend industry events, trade shows, and networking events to meet potential referral sources. Joining professional organizations and participating in local business groups can also help you expand your network</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Leverage social media: Use social media platforms to connect with potential clients and engage with your current network. Share valuable content, participate in discussions, and don&apos;t forget to ask for referrals on your social media profiles</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Follow up: Follow up with leads promptly and professionally. A quick response can make a positive impression and increase the chances of converting a referral into a client</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Build relationships: Focus on building long-term relationships with your clients and referral sources</title>
		<imprint/>
	</monogr>
	<note>Regularly touch base with them, show appreciation for their referrals</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Provide referral training: Teach your clients how to effectively refer you to others. This can include providing them with tips on how to introduce you</title>
		<imprint/>
	</monogr>
	<note>what information to share, and how to follow up on referrals</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Collaborate with non-competitive businesses: Partner with other non-competitive businesses to cross-refer clients. This can help you tap into their network and vice versa</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Ask for testimonials: Positive testimonials can help build trust with potential clients and encourage referrals. Ask satisfied clients if they would be willing to provide a testimonial for your website or social media profiles</title>
		<imprint/>
	</monogr>
	<note>Table 25. Example response of Mistral-v3-7b (instruct)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
