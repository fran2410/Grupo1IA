<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Performance analysis of breast cancer histopathology image classification using transfer learning models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Meena</forename><forename type="middle">Prakash</forename><surname>Ramasamy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Communication Engineering</orgName>
								<orgName type="institution">P.S.R. Engineering College</orgName>
								<address>
									<settlement>Sivakasi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thayammal</forename><surname>Subburaj</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Communication Engineering</orgName>
								<orgName type="institution">P.S.R. Engineering College</orgName>
								<address>
									<settlement>Sivakasi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Valarmathi</forename><surname>Krishnasamy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Communication Engineering</orgName>
								<orgName type="institution">P.S.R. Engineering College</orgName>
								<address>
									<settlement>Sivakasi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vimala</forename><surname>Mannarsamy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Communication Engineering</orgName>
								<orgName type="institution">P.S.R. Engineering College</orgName>
								<address>
									<settlement>Sivakasi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Meena</forename><surname>Prakash</surname></persName>
							<email>meenaprakash@psr.edu.in</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronics and Communication Engineering</orgName>
								<orgName type="institution">P.S.R. Engineering College Sivakasi</orgName>
								<address>
									<postCode>626140</postCode>
									<settlement>Tamil Nadu</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Performance analysis of breast cancer histopathology image classification using transfer learning models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2845AC25EF3F75F2A7BABAA933B38AD5</idno>
					<idno type="DOI">10.11591/ijece.v14i5.pp6006-6015</idno>
					<note type="submission">Received May 10, 2024 Revised Jul 12, 2024 Accepted Jul 17, 2024</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2025-05-12T17:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>BreakHis Breast cancer Deep learning Histopathology images Image classification Transfer learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional neural networks (CNN) which are deep learning-based methods are being currently successfully deployed and have gained much popularity in medical image analysis. CNN can handle enormous amounts of medical data which makes it possible for accurate detection and classification of breast cancer from histopathological images. In the proposed method, we have implemented transfer learning-based classification of breast cancer histopathological images using DenseNet121, DenseNet201, VGG16, VGG19, InceptionV3, and MobileNetV2 and made a performance analysis of the different models on the publicly available dataset of BreakHis. These networks were pre-trained on the ImageNet database and initialized with weights which are fine-tuned by training with input histopathological images. These models are trained with images of the BreakHis dataset with multiple image magnifications. From the comparative study of these pretrained models on histopathology images, it is inferred that DenseNet121 achieves the highest breast cancer classification accuracy of 0.965 compared to other models and contemporary methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Breast cancer is the second most prominent reason for cancer deaths occurring in women globally. To extend the lifespan of women diagnosed with breast cancer, early identification with prompt treatment is crucial. Early breast cancer detection allows for the selection of a suitable treatment, which effectively decreases the death rate. A significant number of handcrafted methods and deep learning-based methods have been employed for feature extraction and breast cancer histology image classification since the advent of pattern recognition and machine learning. To achieve maximum classification accuracy, while reducing the number of selected features, feature extraction is an essential process in image classification. Convolutional neural network architecture which is used to implement deep learning algorithms has the ability of autonomous feature extraction, data retrieval, and performing conceptual data representations for classification. Consequently, they are able to solve the issues with conventional feature extraction techniques. One of the main objectives of computer aided diagnosis (CAD) methods for breast cancer diagnosis is the automated categorization of histopathological images into benign and malignant tumors. Deep learning-based methods perform at achieving this objective by automated feature extraction and classification of the histopathology images. The problem addressed in this paper involves evaluating the efficacy and efficiency of different transfer learning models for the classification of breast histopathology images. The proposed solution is to give an insight into how performance metrics such as accuracy, recall, and precision can be 6007 enhanced through a transfer learning approach in the applicability of breast cancer diagnosis. A variety of pre-trained convolutional neural network (CNN) models, which include DenseNet, VGG, inception, and MobileNet have the advantages of applying connections between layers, fewer parameters, stronger propagation, and feature reuse. The network may be trained more quickly and easily as a result of the increased parameter efficiency. Using transfer learning, the information about data is gained from one task, and another associated task is solved using that information through which the computational complexity is greatly reduced. The model's performance during training and testing can be increased in an efficient manner.</p><p>In this paper, transfer learning-based breast cancer diagnosis techniques have been proposed and the method is validated on the BreakHis dataset with a comparative analysis of the different pre-trained CNN models. The contributions of the article include i) implementing transfer learning-based breast histopathology images classification, ii) comparative analysis of the different pre-trained models on BreakHis dataset, and iii) comparison of the transfer learning-based classification of breast cancer histopathology images with the recent methods of literature.</p><p>A method of selection of patches from breast histopathology images and a transfer learning-based method were suggested for classification from a few training images. Their proposed work performed three different types of experiments that support vector machine-based classification of deep features, patch-wise classification, and image-wise classification <ref type="bibr" target="#b0">[1]</ref>. A method that adapted a pre-trained CNN model using deep feature extraction and transfer learning techniques was proposed. In this study, feature extraction was done using VGG16 and AlexNet models, and final fine-tuning was done using AlexNet. Support vector machines (SVM) were used to classify the collected features <ref type="bibr" target="#b1">[2]</ref>.</p><p>An ensemble of deep learning methods was proposed for the classification of histopathology images of breasts into benign or malignant. In this work, the pre-trained models of the fine-tuned VGG16, fine-tuned VGG19, fully-trained VGG16, and fully-trained VGG19 were implemented, and 5-fold cross-validation was employed <ref type="bibr" target="#b2">[3]</ref>. Transfer learning method coupled with fine-tuning on blocks was suggested for the classification of both binary and 8-class images of the BreakHis dataset <ref type="bibr" target="#b3">[4]</ref>.</p><p>A modified residual neural network (ResNet) technique was proposed for the detection of breast tumors from images of histopathology which attained better results on the dataset of BreakHis <ref type="bibr" target="#b4">[5]</ref>. A combination of a saliency detector and a fully convolutional classifier network was proposed to label the entire slide pixel-wise. Followed by a polling method of the majority to perform ultimate slide-level diagnosis <ref type="bibr" target="#b5">[6]</ref>.</p><p>An approach that utilized CNN, a bag of features, and handcrafted features for the count of mitosis and then performed classification of breast histopathology images was proposed <ref type="bibr" target="#b6">[7]</ref>. A stacked generalized ensemble (SGE) algorithm was implemented for classifying images from breast cancer datasets as either benign or malignant <ref type="bibr" target="#b7">[8]</ref>. An ensemble model of ResNet50 and extreme learning machine (ELM) was implemented whose kernels were weighted for computer-aided diagnosis of breast cancer <ref type="bibr" target="#b8">[9]</ref>. The CNN architecture of ResNet-34 was proposed based on residual learning, for the classification of malignancy and benign cases of breast histopathology images <ref type="bibr" target="#b9">[10]</ref>. A transfer learning-based method was suggested which utilized two pre-trained models of DenseNet-161 and ResNet-50 for the breast histopathology images classification <ref type="bibr" target="#b10">[11]</ref>.</p><p>The method of cascaded SVM and multi-level features was proposed to classify images of breast cancer histopathology into different grades: low, intermediate, and high grades <ref type="bibr" target="#b11">[12]</ref>. A computer-aided diagnosis system was implemented for the classification of images from the BreakHis dataset <ref type="bibr" target="#b12">[13]</ref>. A method based on extreme gradient boosting and deep learning was proposed for which the pre-processing techniques of stain normalization and data augmentation were employed <ref type="bibr" target="#b13">[14]</ref>. A method comprising of convolutional long short-term memory model and optimized SVM was proposed to classify breast cancer images <ref type="bibr" target="#b14">[15]</ref>.</p><p>A tree-based multi-classification model called BrT (breast tumor) was employed utilizing deep learning techniques to extract distinctive features and achieve improved performance while utilizing fewer computational resources <ref type="bibr" target="#b15">[16]</ref>. A generative adversarial network (GAN) for classifying an imbalanced breast cancer dataset was proposed by Saini and Susan <ref type="bibr" target="#b16">[17]</ref>. A nucleus-guided transfer learning (NucTraL) approach to develop an accessible and cost-effective algorithm for breast tumor classification was suggested and evaluated on the publicly accessible BreakHis dataset <ref type="bibr" target="#b17">[18]</ref>. The CNN architecture of BreastNet was proposed, which incorporated a residual architecture with attention modules to classify breast cancer images <ref type="bibr" target="#b18">[19]</ref>. A breast histopathology image classification method that utilized a bag of visual words (BoW) approach to combine shape features and manually designed features was proposed <ref type="bibr" target="#b19">[20]</ref>. Ensembles of CNN models were proposed to classify breast cancer histopathology images <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b26">[27]</ref>. A classification method for breast histopathology images which included the steps of image enhancement, nucleus segmentation, extraction of features, and detection of breast cancer was proposed <ref type="bibr" target="#b27">[28]</ref>. A deep learning-based Heterogeneous Ensemble method for the classification of nuclei into mitotic and non-mitotic in breast histopathological images was proposed <ref type="bibr" target="#b28">[29]</ref>. The method employed techniques of feature invariance, region homogeneity, and residual learning. A novel loss function called concentric loss was proposed in CNN-based weakly supervised breast cancer diagnosis and the method was validated on the ICPR2014 MITOSIS dataset and AMIDA13 dataset  ISSN: 2088-8708 Int J Elec &amp; Comp Eng, Vol. 14, No. 5, October 2024: 6006-6015 6008 <ref type="bibr" target="#b29">[30]</ref>. A fully convolutional auto-encoder to learn the leading structural models and a multiple instance learning method were proposed to detect breast cancer in Histopathology images <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>.</p><p>Based on the literature survey, it is inferred that transfer learning-based methods give promising results in detecting breast cancer from histopathology images. The major advantages of transfer learning are that it requires fewer training images, and the requirement of computational resources is less. Hence in the proposed method, the pre-trained models including VGG16, VGG19, DenseNet121, DenseNet201, InceptionV3, and MobileNet are fine-tuned for the detection and classification of breast cancer histopathology images. The performance of these models on the breast cancer histopathology images dataset is analyzed and compared based on different performance metrics.</p><p>The rest of the section is organized as follows. Section 2 discusses CNN, transfer learning, pretrained models, and the proposed method, experimental results and analysis are discussed in Section 3 with a conclusion in Section 4. The novelty of the proposed work is the application of different transfer learning models for the classification of breast cancer histopathology images and a comparative analysis is done on the different models and with other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHOD 2.1. Convolutional neural network</head><p>Convolutional neural network (CNN) architecture comprises of input layer, several convolution layers with rectified linear units, pooling layers, and dense layers. CNN architecture's basic component is the convolutional layer which extracts the features from the input images. Pooling layers in convolutional networks typically comprise down-sampling layers that gradually reduce the size of the feature representation. Convolution and pooling processes together generate high-level features that can be used for categorization. CNNs' end-to-end learning architecture provides feature extraction and also classification. A large number of parameters in the CNN architecture are optimized during the training phase. The standard backpropagation algorithm is typically used for CNN training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Transfer learning</head><p>The process that learns data from one problem and applies it to different related problems for feature extraction and classification is called transfer learning (TL). Transfer learning is performed using pre-trained models that have previously been trained on a large amount of data. The pre-trained model is additionally trained and tuned using an original dataset comprising of small quantity of images. Recently, transfer learning has been applied to many image processing applications since fine-tuning a pre-trained model is typically quicker and simpler when compared to train a CNN model from scratch whose weights are randomly initialized. Deep learning's requirement for large quantities of data is one of its primary features. Under-fitting will happen during training if there is not sufficient data and hence the method of transfer learning is suggested as a way to train with deep learning on a smaller dataset. Accurate and effective image categorization can be accomplished with smaller datasets using transfer learning's learning capacity. For a newly formed task, if there is insufficient training data, the popular CNN models like VGG16 are pre-trained on the ImageNet dataset comprising 1.2 million images belonging to one thousand classes to acquire the characteristics of the ImageNet dataset. In the proposed work, transfer learning based on VGG16, VGG19, DenseNet121, DenseNet201, InceptionV3, and MobileNet is implemented for the classification of breast cancer histopathological images into benign and malignant categories. The pre-trained models on the ImageNet dataset are used and fine-tuned using the BreakHis dataset and the proposed strategy has achieved enhancement in classification accuracy based on the small dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Pre-trained models</head><p>In this research, the pre-trained models of DenseNet121, DenseNet201, VGG16, VGG19, InceptionV3, and MobileNetV2 are implemented for binary breast histopathology image classification into benign and malignant. Every layer in DenseNet is feed-forwardly linked to all previous layers of the network. DenseNet has the benefit of improved feature rebuilding, which dramatically increases the design's efficiency. DenseNet has been gradually enhancing its accuracy while also enhancing its feature set without leading to overfitting or performance loss. DenseNet also has the ability to save a lot of time by increasing the number of features that can be reused in subsequent tasks. The VGG architecture is a significant development in the field of deep learning since it is an implementation of CNN with a dense set of connections of layers to represent the visual data in a structured form for functioning. Even though many subsequent efforts improved the VGG structure, in this paper, we have included the implementation of VGG16 and VGG19 for analysis of breast cancer histopathology image classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6009</head><p>VGG16 contains sixteen weight layers and VGG19 convolutional neural network contains 19 layers, sixteen of which are convolutional layers, and three layers are fully connected. Feature extraction is done using 16 convolutional layers, while classification is done through the following three fully connected layers. This model imports an image with a size of 224×224 and produces the classification output label as a benign or malignant tumor.</p><p>Inception-V3 network architecture is a pre-trained model trained on the ImageNet dataset and its size is 89 MB. The input image size is 299×299 pixels and a total of 350 connected layers are available in the Inception-V3Net directed acyclic graph (DAG) network architecture. Inception modules exist in InceptionNet and they consist of blocks of layers that learn both global and local features from the input images using 1×1, 3×3, and 5×5 convolutional layers. The InceptionNet uses a modular design so that the network learns feature maps at different scales. Filter Concatenation is performed on these feature maps to represent the input data in a more widespread manner inclusive of both higher-level and low-level features. MobilenetV2 is being deployed because of its small size and its performance is better on mobile devices. It utilizes depth-aware independent convolutions, which means that each color channel performs a separate convolution rather than being combined and smoothed.</p><p>MobilenetV2 architecture consists of two residual blocks with stride of 1 and 2, which is used for downsampling. Three layers are present in each block which are the 1×1 convolution layer with ReLU6, depth-wise convolution with ReLU6, and the third layer is linear 1×1 convolution. The advantages of MobileNet are that it is a lightweight deep neural network and depth-wise convolutions are used so that the number of parameters is greatly reduced when compared with other networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Proposed method</head><p>The flow diagram of the proposed transfer learning-based histopathology image classification is shown in Figure <ref type="figure" target="#fig_2">1</ref>. The histopathological images belonging to the dataset of BreakHis are partitioned into training images and testing images in the ratio of 0.8:0.2. The input images are subjected to normalization and dataset balance as pre-processing steps. The pre-trained CNN models -DenseNet121, DenseNet201, VGG16, VGG19, InceptionV3, and MobileNetV2 are used for feature extraction, and the weights of these models were already initialized through training on millions of images from the ImageNet dataset. The top dense layers of these networks are removed and replaced with three fully connected layers for binary classification as benign and malignant tumors. The network is trained and fine-tuned using the breast histopathology images from the train dataset and then validated using images from the test dataset. An analysis of the performance of the different transfer learning models is done with regard to classification performance metrics of precision, recall, accuracy, and F1-score.</p><p>The size of the input images is 224×224×3 and the input images are first pre-processed in which normalization is done. The dataset is imbalanced as inferred from Table <ref type="table" target="#tab_0">1</ref>. The amount of benign images and malignant images in the 40x dataset are 625 and 1370 respectively and the total images are 1995. Hence, upsampling is done for the benign images from 625 to 1,370 and hence the dataset becomes balanced with a total number of images as 2,740. The pre-trained models of DenseNet121, DenseNet201, VGG16, VGG19, InceptionV3, and MobileNetV2 are loaded with weights of the ImageNet dataset, and the top dense layers of these models are removed. Then, flattened layers, dropout layers, and dense layers are added for binary classification. The model is compiled with the optimizer of Adam at 0.00001 learning rate, with a batch size of 256, and runs 60 epochs. The model is trained using the training dataset with the above parameters and validated with the testing images. A performance evaluation of all the models on the BreakHis dataset is done and a comparative analysis with other methods in the literature is done. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Description of the dataset and performance metrics</head><p>The breast tumor histopathological images belonging to two different categories, benign and malignant, were gathered for the BreakHis collection via clinical investigations. All patients identified with breast cancer were notified to the Brazilian P&amp;D lab to take part in the investigation during the time period if they had any clinical symptoms of the disease. Hematoxylin and Eosin staining were used to gather samples during surgical open biopsy (SOB). Pathologists at the R&amp;D laboratory can annotate these photos for histological studies. The 7,909 microscopic images of breast tumor tissue belonging to benign and malignant categories from 82 people that make up the BreakHis dataset were taken using a microscope (including 40x, 100x, 200x, 400x). Based on the way the cancerous cells are seen underneath the microscope, both classes of breast tumors are further classified into sub-categories. A breast tumor's prognosis and possible treatment of therapy can vary depending on its type or subtype. There are four forms of benign breast tumors and four forms of malignant breast tumors. In this experiment, 20% of the samples are used for testing, while 80% are used for training. It is ensured that the training images are not used during testing in order to successfully apply the task of categorization.</p><p>Figure <ref type="figure">2</ref> shows the sample histopathological images of breast cancer from the BreakHis dataset. The sub-categories of benign images are Tubular Adenoma, Adenosis, Phyllodes Tumor, and Fibroadenoma. The sub-categories of malignant tumors are Lobular Carcinoma, Ductal Carcinoma, Papillary Carcinoma, and Mucinous Carcinoma.</p><p>The proposed method of transfer learning-based breast cancer histopathology image classification is evaluated by means of the confusion matrix obtained while classification. There are four terms in this evaluation matrix. True positive represents images that have been correctly recognized as benign, and false positive refers to malignant images that are incorrectly identified as benign. True negative refers to correctly diagnosed malignant images, whereas false negative indicates images from the benign class that are incorrectly classified as malignant. Four metrics are used to evaluate the performance of the proposed transfer learning-based classification of breast cancer histopathological images from the test dataset which include accuracy, recall, precision, and F1-score. Precision is expressed as the proportion of benign images correctly categorized out of all predicted images of the same class.</p><formula xml:id="formula_0">𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 = 𝑇𝑟𝑢𝑒 𝑃𝑜𝑠𝑖𝑡𝑖𝑣𝑒 𝑇𝑟𝑢𝑒 𝑃𝑜𝑠𝑖𝑡𝑖𝑣𝑒+𝐹𝑎𝑙𝑠𝑒 𝑃𝑜𝑠𝑖𝑡𝑖𝑣𝑒<label>(1)</label></formula><p>Recall is expressed as the ratio of benign images truly categorized out of all predicted images of the malignant class. </p><p>Accuracy measures the number of images that were correctly identified out of all the testing images and evaluates how accurately a model performs.</p><formula xml:id="formula_2">𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 = 𝑇𝑟𝑢𝑒 𝑃𝑜𝑠𝑖𝑡𝑖𝑣𝑒+𝐹𝑎𝑙𝑠𝑒 𝑁𝑒𝑔𝑎𝑡𝑖𝑣𝑒 𝑇𝑜𝑡𝑎𝑙 𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑖𝑚𝑎𝑔𝑒𝑠<label>(3)</label></formula><p>F1-score shows the equilibrium value of recall and precision and is typically used for improving a model for recall or precision.</p><formula xml:id="formula_3">𝐹1 𝑆𝑐𝑜𝑟𝑒 = 2 * 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 * 𝑅𝑒𝑐𝑎𝑙𝑙 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛+𝑅𝑒𝑐𝑎𝑙𝑙<label>(4)</label></formula><p>Figure <ref type="figure">2</ref>. Histopathology images from the dataset of BreakHis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental results</head><p>The experiments are carried out with six pre-trained models: VGG16, VGG19, DenseNet 121, DenseNet201, MobileNetV2, and InceptionV3 on breast cancer histopathology images. The proposed method is evaluated using the publicly available BreakHis dataset comprising 7909 real images, categorized into two subsets of benign and malignant samples of quantity 2,480 and 5,429 respectively. These images belong to different magnification factors of 400x, 200x, 100x, and 40x. We obtained accuracy and loss plots and confusion Matrices for all the magnification factors and the results obtained for DenseNet121 architecture on the 100x are shown in Figure <ref type="figure" target="#fig_4">3</ref>. The results based on the experiments obtained for the proposed transfer learning-based method are shown in Tables <ref type="table" target="#tab_4">2 to 5</ref> for images of different magnification factors of 40x, 100x, 200x, and 400x respectively. The graphical results for the performance analysis of the proposed method on the 40x dataset are shown in Figure <ref type="figure">4</ref>.   <ref type="table" target="#tab_5">6</ref> shows the comparison of the proposed method with the method proposed by Zhu et al. <ref type="bibr" target="#b23">[24]</ref>, which shows improvement in classification accuracy by 9%.  <ref type="bibr" target="#b23">[24]</ref> 0.852 0.835 0.841 0.793 Proposed transfer learning-based method with DenseNet121 pre-trained model 0.944 0.965 0.955 0.935</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discussion on the results</head><p>A comprehensive analysis of the different performance metrics of classification accuracy, precision, recall, and F1-score were done on the BreakHis dataset for the classification of breast histopathology images into benign and malignant tumors. From Table <ref type="table" target="#tab_1">2</ref>, it is inferred that the DenseNet121 CNN model achieved the highest classification accuracy, recall, and F1-score of 0.944, 0.922, and 0.944 respectively while the highest precision of 0.981 is obtained for DenseNet201 on the 40x dataset. Similarly, on the 100x dataset, as shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this paper, a transfer learning-based method for classifying breast cancer histopathology images is proposed, utilizing the BreakHis dataset. We implement six popular pre-trained models, for automatic feature extraction for binary classification into benign and malignant categories. To evaluate the performance, we employed various metrics including accuracy, precision, sensitivity (recall), and F1-score for four different magnification factors. For binary classification into benign and malignant tumors, the highest classification accuracy of 96.5% is attained for the DenseNet121 architecture which is better compared to the recent method in literature. A comparative analysis of the six different pre-trained models is also done. The proposed transfer learning-based approach gives an enhancement in accuracy of 9% over the state-of-the-art method.</p><p> ISSN: 2088-8708</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Int J Elec &amp; Comp Eng ISSN: 2088-8708  Performance analysis of breast cancer histopathology image classification … (Meena Prakash Ramasamy)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Int J Elec &amp; Comp Eng ISSN: 2088-8708  Performance analysis of breast cancer histopathology image classification … (Meena Prakash Ramasamy)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Flow diagram of the transfer learning-based histopathology images classification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Int J Elec &amp; Comp Eng ISSN: 2088-8708  Performance analysis of breast cancer histopathology image classification … (Meena Prakash Ramasamy)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Accuracy and loss plots for DenseNet121 on 100x dataset</figDesc><graphic coords="6,309.55,572.99,211.41,167.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>F1-score respectively by DenseNet201 architecture while the highest value of 0.977 is obtained for precision by InceptionV3 architecture. The highest values of performance metrics are shown in bold in the respective tables. Dense connections are established between all of the previous and succeeding layers in the DenseNet architecture and hence better performance is achieved for DenseNet with reduced computational cost and less parameters compared to VGG, Inception, and MobileNet architecture. In the inception architecture, multiple convolutional kernels are employed to extract information from different scales of the image and finally, the features are concatenated to better represent the image. The results of inception architecture are second highest on the BreakHis dataset for all magnification factors of 40x, 100x, 200x, and 400x. The VGG architecture consists of multiple Convolutional layers with rectified linear unit activation function and pooling layers for feature extraction. Finally, three fully connected layers are used for classification with a SoftMax activation function. The basic unit of MobileNet comprises depthwise convolution and pointwise convolution. In depthwise convolution, dissimilar convolution kernels are used for each input channel and pointwise convolution uses kernels of 1x1 convolution. The experimental results indicate that the performance of DenseNet and Inception architecture is superior to the VGG and MobileNet architecture by approximately 4% for breast cancer histopathology image classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Description of the BreakHis dataset</figDesc><table><row><cell cols="4">Magnification factor Benign images Malignant images Total images</cell></row><row><cell>40x</cell><cell>625</cell><cell>1370</cell><cell>1995</cell></row><row><cell>100x</cell><cell>644</cell><cell>1437</cell><cell>2081</cell></row><row><cell>200x</cell><cell>623</cell><cell>1390</cell><cell>2013</cell></row><row><cell>400x</cell><cell>588</cell><cell>1232</cell><cell>1820</cell></row><row><cell cols="2">Total Images in the BreakHis dataset</cell><cell></cell><cell>7909</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of performance metrics of different pre-trained models on 40x images</figDesc><table><row><cell cols="5">Performance metrics Classification accuracy Precision Recall F1-Score</cell></row><row><cell>DenseNet 121</cell><cell>0.944</cell><cell>0.967</cell><cell>0.922</cell><cell>0.944</cell></row><row><cell>DenseNet 201</cell><cell>0.938</cell><cell>0.981</cell><cell>0.897</cell><cell>0.937</cell></row><row><cell>Inception V3</cell><cell>0.938</cell><cell>0.977</cell><cell>0.901</cell><cell>0.937</cell></row><row><cell>VGG 16</cell><cell>0.907</cell><cell>0.933</cell><cell>0.883</cell><cell>0.907</cell></row><row><cell>VGG 19</cell><cell>0.872</cell><cell>0.898</cell><cell>0.848</cell><cell>0.872</cell></row><row><cell>Mobile Net</cell><cell>0.938</cell><cell>0.970</cell><cell>0.908</cell><cell>0.938</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of performance metrics of different pre-trained models on 100x images</figDesc><table><row><cell cols="5">Performance metrics Classification accuracy Precision Recall F1-Score</cell></row><row><cell>DenseNet 121</cell><cell>0.965</cell><cell>0.972</cell><cell>0.958</cell><cell>0.965</cell></row><row><cell>DenseNet 201</cell><cell>0.953</cell><cell>0.982</cell><cell>0.924</cell><cell>0.952</cell></row><row><cell>Inception V3</cell><cell>0.934</cell><cell>0.953</cell><cell>0.913</cell><cell>0.933</cell></row><row><cell>VGG 16</cell><cell>0.906</cell><cell>0.930</cell><cell>0.879</cell><cell>0.904</cell></row><row><cell>VGG 19</cell><cell>0.887</cell><cell>0.906</cell><cell>0.865</cell><cell>0.885</cell></row><row><cell>Mobile Net</cell><cell>0.941</cell><cell>0.957</cell><cell>0.924</cell><cell>0.940</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison of performance metrics of different pre-trained models on 200x images</figDesc><table><row><cell cols="5">Performance metrics Classification accuracy Precision Recall F1-Score</cell></row><row><cell>DenseNet 121</cell><cell>0.955</cell><cell>0.973</cell><cell>0.934</cell><cell>0.953</cell></row><row><cell>DenseNet 201</cell><cell>0.962</cell><cell>0.974</cell><cell>0.949</cell><cell>0.961</cell></row><row><cell>Inception V3</cell><cell>0.942</cell><cell>0.955</cell><cell>0.926</cell><cell>0.94</cell></row><row><cell>VGG 16</cell><cell>0.899</cell><cell>0.906</cell><cell>0.886</cell><cell>0.896</cell></row><row><cell>VGG 19</cell><cell>0.867</cell><cell>0.881</cell><cell>0.842</cell><cell>0.861</cell></row><row><cell>Mobile Net</cell><cell>0.906</cell><cell>0.914</cell><cell>0.893</cell><cell>0.903</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison of performance metrics of different pre-trained models on 400x images As inferred from the results obtained from the different experiments conducted, DenseNet121 and DenseNet201 architectures give the best performance compared to other models with a classification accuracy of 0.96. Next to the DenseNet, InceptionV3 and MobileNet give higher classification accuracy of</figDesc><table><row><cell cols="5">Performance metrics Classification accuracy Precision Recall F1-Score</cell></row><row><cell>DenseNet 121</cell><cell>0.935</cell><cell>0.956</cell><cell>0.908</cell><cell>0.932</cell></row><row><cell>DenseNet 201</cell><cell>0.957</cell><cell>0.962</cell><cell>0.95</cell><cell>0.956</cell></row><row><cell>Inception V3</cell><cell>0.938</cell><cell>0.977</cell><cell>0.901</cell><cell>0.937</cell></row><row><cell>VGG 16</cell><cell>0.907</cell><cell>0.933</cell><cell>0.871</cell><cell>0.901</cell></row><row><cell>VGG 19</cell><cell>0.878</cell><cell>0.905</cell><cell>0.838</cell><cell>0.87</cell></row><row><cell>Mobile Net</cell><cell>0.909</cell><cell>0.945</cell><cell>0.863</cell><cell>0.902</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Comparison of classification accuracy of proposed method with method in literature</figDesc><table><row><cell>Method</cell><cell>40x</cell><cell>100x</cell><cell>200x</cell><cell>400x</cell></row><row><cell>Method proposed by</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 ,</head><label>3</label><figDesc>the highest values of 0.965, 0.958, and 0.965 are obtained for classification accuracy, recall, and F1-score respectively and the highest precision of 0.982 is obtained for DenseNet201. The classification results on the 200x dataset as shown in Table 4 indicate that the highest values of 0.962, 0.974, 0.949, and 0.961 are obtained for classification score, precision, recall, and F1-score respectively by DenseNet201 architecture. The classification results on the 400x dataset as shown in Table 5 indicate that the highest values of 0.957, 0.950, and 0.956 are obtained for classification score, recall, and</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>She is a life member of IETE and ISTE (India). She can be contacted at thaya.psr@gmail.com.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Valarmathi Krishnasamy</head><p>is currently working as a professor at the Department of Electronics and Communication Engineering, P.S.R. Engineering College, Sivakasi, Tamil Nadu, India. She has 25 years of teaching experience. She has published 43 papers in reputed international/national journals, presented 35 papers in national and international conferences, and received the best paper award at the International Conference on VLSI, Communication, and Instrumentation (ICVCI 2011). Her research interests are system identification, image processing, soft computing, wireless networks, cloud computing, and machine learning. She is a reviewer in various peer-reviewed journals such as the Journal of Instrumentation Science and Technology, Taylor and Francis, Optimal Control, Applications and Methods, Wiley Publishers, Applied Mathematical Modeling, Elsevier Science, and IEEE Transactions on Industrial Informatics. In her career, 2 patents have been granted and 19 patents were published. She is the recognized supervisor of Anna University Chennai. 11 Ph.D. scholars were awarded under her guidance, and 7 scholars are pursuing Ph.D. and also guided 29 PG Scholars. She has received a grant of Rs.80 lakhs for establishing Process Control and Biomedical Laboratory at Kalasalingam University under the DST-FIST scheme and also received Rs.7.53 lakhs from AICTE under the RPS Scheme. Technical talks/lectures were delivered in various workshops/seminars on the topic of neural networks, fuzzy logic, and soft computing applications. She has organized many national and international level conferences, workshops, seminars, and other technical events. She took various academic and professional responsibilities in her career such as academic council, board of studies, department research committee, and board of management in various technical institutions. She is actively involved in various professional societies such as IEEE, IETE, ISTE, and ISOI. She can be contacted at valarmathi@psr.edu.in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vimala Mannarsamy</head><p>is currently working as an assistant professor at the Department of Electronics and Communication Engineering, P.S.R. Engineering College, Sivakasi, Tamil Nadu, India. She has a teaching experience of 9 years. Her research interests include image processing and machine learning. She can be contacted at vimala@psr.edu.in.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Transfer learning-assisted multi-resolution breast cancer histopathological images classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Asghar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Gillani</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00371-021-02153-y</idno>
	</analytic>
	<monogr>
		<title level="j">Visual Computer</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2751" to="2770" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Transfer learning based histopathologic image classification for breast cancer detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Deniz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Şengür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kadiroğlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ü</forename><surname>Budak</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13755-018-0057-x</idno>
	</analytic>
	<monogr>
		<title level="j">Health Information Science and Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Breast cancer histopathology image classification using an ensemble of deep learning models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hameed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zahia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Garcia-Zapirain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Javier</forename><surname>Aguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">María</forename><surname>Vanegas</surname></persName>
		</author>
		<idno type="DOI">10.3390/s20164373</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">16</biblScope>
			<date type="published" when="2020-08">Aug. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A new transfer learning based approach to magnification dependent and independent classification of breast cancer in histopathological images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boumaraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ferkous</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bspc.2020.102192</idno>
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Breast cancer detection from histopathology images using modified residual neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vasudev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doegar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sambyal</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bbe.2021.08.011</idno>
	</analytic>
	<monogr>
		<title level="j">Biocybernetics and Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1272" to="1287" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detection and classification of cancer in whole slide breast histopathology images using deep convolutional networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gecer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mercan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Elmore</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2018.07.022</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="345" to="356" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An ensemble approach for classification of breast histopathology images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhivya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vasuki</surname></persName>
		</author>
		<idno type="DOI">10.1080/03772063.2019.1644974</idno>
	</analytic>
	<monogr>
		<title level="j">IETE Journal of Research</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1320" to="1329" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An ensemble algorithm for breast cancer histopathology image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Batra</surname></persName>
		</author>
		<idno type="DOI">10.1080/09720510.2020.1818451</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistics and Management Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1187" to="1198" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Breast cancer histopathology image classification using kernelized weighted extreme learning machine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gyanchandani</surname></persName>
		</author>
		<idno type="DOI">10.1002/ima.22465</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Imaging Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="168" to="179" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classification of breast cancer histopathological image with deep residual learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1002/ima.22548</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Imaging Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1583" to="1594" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automated classification of histopathology images using transfer learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Talo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artmed.2019.101743</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automated grading of breast cancer histopathology using cascaded ensemble with combination of multi-level image features</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2016.05.084</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">229</biblScope>
			<biblScope unit="page" from="34" to="44" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BreakHis based breast cancer automatic diagnosis using deep learning: taxonomy, survey and insights</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Benhammou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Achchab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tabik</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2019.09.044</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">375</biblScope>
			<biblScope unit="page" from="9" to="24" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An investigation of XGBoost-based algorithm for breast cancer classification</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hameed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clos</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.mlwa.2021.100154</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Learning with Applications</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2021-12">Dec. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DeepBreastNet: A novel and robust approach for automated breast cancer detection from histopathological images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Demir</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bbe.2021.07.004</idno>
	</analytic>
	<monogr>
		<title level="j">Biocybernetics and Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1123" to="1139" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A tree-based multiclassification of breast tumor histopathology images through deep learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Murtaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Abdul Wahab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shuib</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compmedimag.2021.101870</idno>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep transfer with minority data augmentation for imbalanced breast cancer dataset</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.asoc.2020.106759</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<date type="published" when="2020-12">Dec. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Breast cancer detection from biopsy images using nucleus guided transfer learning and belief based fusion</title>
		<author>
			<persName><forename type="first">K</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Faziludeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.compbiomed.2020.103954</idno>
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BreastNet: a novel convolutional neural network model through histopathological images for the diagnosis of breast cancer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Toğaçar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Özkurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cömert</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.physa.2019.123592</idno>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">545</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tumor malignancy detection using histopathology imaging</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kurmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ganesh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmir.2019.07.004</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Imaging and Radiation Sciences</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="514" to="528" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ensembled deep convolution neural network-based breast cancer classification with misclassification reduction algorithms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Murtaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shuib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W A</forename><surname>Wahab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mujtaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Raza</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-020-08692-1</idno>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="18447" to="18479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Breast cancer classification from histopathological images with inception recurrent residual convolutional neural network</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Alom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yakopcic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nasrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Asari</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10278-019-00182-7</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="605" to="617" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A new deep convolutional neural network model for classifying breast cancer histopathological images and the hyperparameter optimisation of the proposed model</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Burçak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ö</forename><forename type="middle">K</forename><surname>Baykan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Uğuz</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11227-020-03321-y</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Supercomputing</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="973" to="989" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Breast cancer histopathology image classification through assembling multiple compact CNNs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12911-019-0913-x</idno>
	</analytic>
	<monogr>
		<title level="j">BMC Medical Informatics and Decision Making</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microscopic images classification for cancer diagnosis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kurmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kesharwani</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11760-019-01584-4</idno>
	</analytic>
	<monogr>
		<title level="m">Signal, Image and Video Processing</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="665" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3PCNNB-Net: three parallel CNN branches for breast cancer classification through histopathological images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Ibraheem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Rahouma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F A</forename><surname>Hamed</surname></persName>
		</author>
		<idno type="DOI">10.1007/s40846-021-00620-4</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical and Biological Engineering</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="494" to="503" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Computer-aided diagnosis for breast cancer classification using deep neural networks and transfer learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Aljuaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alturki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alsubaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liotta</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cmpb.2022.106951</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">223</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Performance analysis of segmentation algorithms for the detection of breast cancer</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Aswathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jagannath</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2020.03.333</idno>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="page" from="666" to="676" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mitotic nuclei analysis in breast cancer histopathology images using deep ensemble classifier</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sohail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nisar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tabassum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zameer</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2021.102121</idno>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<date type="published" when="2021-08">Aug. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weakly supervised mitosis detection in breast histopathology images using concentric loss</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2019.01.013</idno>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="165" to="178" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discriminative pattern mining for breast cancer histopathology image classification via fully convolutional autoencoder</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Radulovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kanjer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2019.2904245</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="36433" to="36445" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiple instance learning for histopathological breast cancer image classification</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Sudharshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Spanhol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heutte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Honeine</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2018.09.049</idno>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="103" to="111" />
			<date type="published" when="2019-03">Mar. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
