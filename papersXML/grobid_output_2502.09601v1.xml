<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CoT-Valve: Length-Compressible Chain-of-Thought Tuning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-13">13 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinyin</forename><surname>Ma</surname></persName>
							<email>maxinyin@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guangnian</forename><surname>Wan</surname></persName>
							<email>guangnian@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Runpeng</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gongfan</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
							<email>xinchao@nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CoT-Valve: Length-Compressible Chain-of-Thought Tuning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-13">13 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">2885A8154993B4878CE383D2FDB69C4B</idno>
					<idno type="arXiv">arXiv:2502.09601v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.3-SNAPSHOT" ident="GROBID" when="2025-05-13T17:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">0.8.2-3-g65968aec5</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>John starts with €100. 2. He buys a roast for €17: €100 -€17 = €83 remaining 3. Then he buys vegetables for €11:\n €83 -€11 = €72 remaining So</term>
					<term>after his purchases</term>
					<term>John has €72 left</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Chain-of-Thought significantly enhances a model's reasoning capability, but it also comes with a considerable increase in inference costs due to long chains. With the observation that the reasoning path can be easily compressed under easy tasks but struggle on hard tasks, we explore the feasibility of elastically controlling the length of reasoning paths with only one model, thereby reducing the inference overhead of reasoning models dynamically based on task difficulty. We introduce a new tuning and inference strategy named CoT-Valve, designed to allow models to generate reasoning chains of varying lengths. To achieve this, we propose to identify a direction in the parameter space that, when manipulated, can effectively control the length of generated CoT. Moreover, we show that this property is valuable for compressing the reasoning chain. We construct datasets with chains from long to short for the same questions and explore two enhanced strategies for CoT-Valve: (1) a precise length-compressible CoT tuning method, and (2) a progressive chain length compression approach. Our experiments show that CoT-Valve successfully enables controllability and compressibility of the chain and shows better performance than the prompt-based control. We applied this method to QwQ-32B-Preview, reducing reasoning chains on GSM8K from 741 to 225 tokens with a minor performance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with only one additional incorrect answer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Chain-of-Thought (CoT) reasoning <ref type="bibr" target="#b38">(Wei et al., 2022)</ref> has emerged as a powerful technique for enhancing the reasoning capabilities of large language models <ref type="bibr" target="#b18">(Jaech et al., 2024;</ref><ref type="bibr" target="#b9">Dubey et al., 2024;</ref><ref type="bibr" target="#b0">Abdin et al., 2024)</ref>, particularly in complex * Equal contribution † Corresponding Author tasks such as mathematics and coding <ref type="bibr" target="#b31">(Sprague et al., 2024)</ref> that require multi-step inference. By simulating the process of human-like thought progression, CoT enables models to break down complex problems into sub-questions, improving accuracy and interpretability <ref type="bibr" target="#b19">(Joshi et al., 2023)</ref>. Those reasoning abilities have also been tested in different domains, such as image generation <ref type="bibr" target="#b26">(Ma et al., 2025)</ref> and visual understanding <ref type="bibr" target="#b30">(Shao et al., 2024)</ref>.</p><p>Training reasoning models often involves generating extensive reasoning paths through methods such as sampling <ref type="bibr" target="#b37">(Wang et al., 2023)</ref>, tree search <ref type="bibr" target="#b39">(Yao et al., 2023;</ref><ref type="bibr">Guan et al., 2025a;</ref><ref type="bibr" target="#b42">Zhang et al., 2024)</ref> or reinforcement learning (DeepSeek-AI, 2025) to ultimately reach the correct answer. However, these long chains often incorporate redundant intermediate steps that can be unnecessary or too complex <ref type="bibr" target="#b21">(Lightman et al., 2024)</ref>, and the redundancy in the reasoning paths for training leads to inefficiencies in token usage and increased inference costs. However, crafting an optimal reasoning chain that omits extraneous details is challenging due to the limited availability of intermediate rewards to guide the process and human annotations <ref type="bibr" target="#b43">(Zhang et al., 2025)</ref>. Removing some or all of the intermediate steps and then training or distilling the model <ref type="bibr">(Liu et al., 2024b;</ref><ref type="bibr" target="#b41">Yu et al., 2024)</ref> will degrade the performance. Alternative approaches employ information-theoretic measures <ref type="bibr" target="#b35">(Ton et al., 2024)</ref> or identify an "overthinking" solution in QwQ <ref type="bibr">(Team, 2024b)</ref> to evaluate the contribution of each sentence to the final answer.</p><p>We observe that current reasoning models, such as QwQ <ref type="bibr">(Team, 2024b)</ref> and DeepSeek-R1 (DeepSeek-AI, 2025) allocate an excessive number of tokens to simple tasks, while potentially providing insufficient tokens for complex tasks. Thus, a long reasoning path is still essential, while maintaining the ability to compress reasoning paths for simpler questions is equally important. To solve this, our goal is to fine-tune a model ca-So John starts with €100. He buys a roast that costs €17 and some vegetables for €11. I need to find out how much money he has left after these purchases. First, I should figure out the total amount he spent &lt;Omitted&gt; Let me add those up: 17 plus 11 is 28. So, he spent a total of €28. Now, to find out how much money he has left, I need &lt;Omitted&gt;So, final answer: John has €72 left.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>John goes to the market with €100. He buys a roast for €17 and vegetables for €11. How much money does he have left?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Medium</head><p>CoT Short CoT #Token: 85 #Token: 232 #Token: 449 Long CoT (QwQ-32B-Preview)</p><p>So John goes to the market with €100. That's his starting amount. Then he buys a roast for €17 and vegetables for €11. I need to find out how much money he has left after these purchases. Alright, first, I should figure out the total amount he spent. He bought a roast for €17 and vegetables for €11. So, the total expenditure is €17 plus €11. Let's add those up. 17 plus 11 is 28. So, he spent a total of €28. Now, he started with €100, and he spent €28. &lt;Omitted&gt; Yes, that seems straightforward. I don't think there's any trick here or additional steps I need to consider. So, the final answer is €72.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#Token: 833 Extrapolation to Unseen Length</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CoT Valve</head><p>Long Short pable of generating both long and short reasoning paths, rather than being restricted to a compressed form. We offer a new way to control the length of CoT, which we refer to as Length-Compressible Chain-of-Thought Tuning.</p><p>A central component of the proposed method is to identify an update direction in the parameter space, which, by manipulating it, acts as increasing or decreasing the length of CoT. Taking a large step in this direction leads the model to generate a short sequence, while a small step still produces a long and complex reasoning trajectory. We choose to incorporate this update direction by <ref type="bibr">LoRA (Hu et al., 2022)</ref>, enabling it to function as an additional branch that facilitates easy modulation of intensity while imposing minimal extra parameters on the model. We explore methods to identify this direction and demonstrate that it offers superior controllability compared to promptbased approaches, which enables the generation of short CoT that prompt-based methods are unable to achieve. Besides, we observe that the direction can be extrapolated, allowing the reasoning chains to be extended beyond or shortened to lengths unseen in the training set. Leveraging this compressibility, we construct a dataset that pairs long and short reasoning chains for each question. This dataset is then utilized in two ways: (1) to refine the direction for more precise tuning, and (2) to progressively compress the reasoning path.</p><p>We evaluate our method across different types of models, ranging from a pre-trained LLM with little reasoning ability, LLaMA-3.1-8B and LLaMA-3.2-1.5B-Instruct <ref type="bibr" target="#b9">(Dubey et al., 2024)</ref>, to post-trained reasoning models, QwQ-32B-Preview <ref type="bibr">(Team, 2024b)</ref>, and distilled reasoning models, DeepSeek-R1 (DeepSeek-AI, 2025). Our results demonstrate that, with training for one time, our approach enables a model to generate reasoning paths of varying lengths, and we can achieve better </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Chain-of-Thought. Chain-of-thought <ref type="bibr" target="#b38">(Wei et al., 2022)</ref> reasoning has shown promising progress in recent years, especially the success of OpenAi-O1 <ref type="bibr" target="#b18">(Jaech et al., 2024)</ref> and Deepseek-R1 models <ref type="bibr" target="#b5">(DeepSeek-AI, 2025)</ref>. This introduces the testtime scaling law, apart from the traditional scaling law for training <ref type="bibr" target="#b15">(Hoffmann et al., 2022)</ref>. Several approaches have been proposed to boost the language model to have better problem-solving abilities, including the model has its self-reasoning abilities <ref type="bibr">(Team, 2024b)</ref> or use Best-of-N <ref type="bibr" target="#b29">(Nakano et al., 2021)</ref>, beam search and Monte Carlo Tree Search <ref type="bibr">(Kocsis and Szepesvari, 2006;</ref><ref type="bibr">Guan et al., 2025b)</ref> to search and refine the solution without further finetune the large language models. The outcome reward model and process reward models are also introduced to evaluate the score for the entire solution, especially the final answer <ref type="bibr">(Cobbe et al., 2021a)</ref> and the quality of the reasoning path <ref type="bibr" target="#b36">(Wang et al., 2024;</ref><ref type="bibr">Luo et al., 2025b)</ref> Chain Compression in reasoning model. Due to the high computational cost associated with inference in reasoning models, particularly for longchain reasoning, chain compression has become a critical area of research. <ref type="bibr" target="#b41">(Yu et al., 2024)</ref> attempts to distill the chain-of-thought into System 1 but fails to observe improvements when intermediate steps are omitted. <ref type="bibr">(Deng et al., 2024b)</ref> proposes internalizing reasoning steps within the hidden states of models, while several implicitbased approaches <ref type="bibr">(Deng et al., 2024a;</ref><ref type="bibr" target="#b30">Hao et al., 2024;</ref><ref type="bibr" target="#b2">Cheng and Durme, 2024)</ref> aim to compress token-wise generation by transitioning from language space to hidden space. Other studies focus on skipping intermediate reasoning steps <ref type="bibr">(Liu et al., 2024b)</ref> or using summarization techniques to generate shorter reasoning chains <ref type="bibr" target="#b20">(Kang et al., 2024)</ref>. Additionally, <ref type="bibr" target="#b1">(Chen et al., 2024)</ref> addresses the overthinking issue in QwQ <ref type="bibr">(Team, 2024b)</ref> and employs SimPO <ref type="bibr" target="#b27">(Meng et al., 2024)</ref> for optimization. Kimi K1.5 <ref type="bibr" target="#b33">(Team et al., 2025)</ref> proposes merging long-CoT models with short-CoT models in a trainingfree manner. O1-Pruner <ref type="bibr">(Luo et al., 2025a)</ref> adopts reinforcement learning to shorten responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we provide an in-depth discussion of our method. Section 3.1 introduces a simple yet effective approach that enables a single tuning process to generate models with CoT with different lengths. This stage also serves as an initial step for subsequent refinements. Next, in Section 3.2, we explore multiple scenarios in which we can apply CoT-Valve to construct the dataset MixChain. In Section 3.3, we propose several advanced methods that take advantage of long-to-short datasets to improve precision and control over the generated reasoning paths in compressible fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Length-Compressible CoT Tuning</head><p>Our primary objective is to achieve a new way to control the length of reasoning paths after training a reasoning model. Existing approaches, such as prompt-based control, explicitly define sequence length in the prompt <ref type="bibr" target="#b13">(Han et al., 2024)</ref> or utilize summary tokens <ref type="bibr" target="#b8">(Ding et al., 2024)</ref> for guidance. However, these methods offer only limited control over the length of CoT generated. For instance, requesting a sequence of less than 20 tokens may result in the model generating over 350 tokens (see Table <ref type="table">12</ref> in the Appendix), and these methods struggle to produce answers with very short lengths. To address these limitations, we introduce CoT-Valve for training one model but can adjust the length of reasoning paths.</p><p>Consider a reasoning model defined by the parameter θ. For a given question q in the dataset D, the probability of generating an answer a and its reasoning thoughts {t i } n i=1 given the question q can be described by:</p><formula xml:id="formula_0">p (a | t 1 , . . . , t n , q; θ) n i=1 p (t i | t &lt;i , q; θ) (1)</formula><p>where {t i } n i=1 might include errors or unnecessary details. With short synthesized or human-annotated explanations {t i } m i=1 with m &lt; n, the training objective is to adjust the parameter in such a way that the chain is shortened while still yielding the correct answer:</p><formula xml:id="formula_1">max ∆θ E (q,a)∼D p (a | t 1 , . . . , t m , q; θ + ∆θ) m i=1 p (t i | t &lt;i , q; θ + ∆θ) (2)</formula><p>and ∆θ denotes the change in the parameter space that steers the model towards generating a more concise chain.</p><p>Since the model, with and without ∆θ, outputs the same final answer, ∆θ can be interpreted as a task vector <ref type="bibr" target="#b17">(Ilharco et al., 2022)</ref>. The task here is to control the length of the CoT, provided that the only difference in the training set lies in intermediate reasoning steps {t i } n i=1 . Those reasoning paths are different in length but ultimately lead to the same final answer. Thus, we can control the task vector to achieve the goal of adjusting the length of CoT. ∆θ is designed within a parameter-efficient space, functioning as an external branch for inference that incurs minimal overhead. Controlling this external branch enables the manipulation of the length of the reasoning path.</p><p>Task Arithmetic: Interpolation and Extrapolation of ∆θ. To manipulate this update within the parameter space, we can control the magnitude of a ∆θ as an arithmetic operation. We use two primary operations on ∆θ here: interpolation and extrapolation. Let α denote the magnitude of ∆θ for LoRA.</p><p>System 1: Short CoT Finetuned: Short CoT Large-scale Post-trained:</p><p>Long CoT</p><formula xml:id="formula_2">! ! ! " Δ! ! Δ # ! " = # ! " -! " # ! " Stage 1: Find Δ" ! , Δ $ " ! orΔ $ " " Finetuned/Distilled: Long CoT # ! ! Δ # ! ! ! " Medium Path Δ # ! " Extrapolate: Short Path &amp;Δ # ! " Interpolate: Long Path 'Δ # ! "</formula><p>Stage 2: Generate Long-to-Short Reasoning Dataset Stage 3.a (CoT-Valve++): Stage 3.b (CoT-Valve+P): When α falls within the range of (0,1), the model smoothly transitions between longer and shorter reasoning paths, similar to weight interpolation between two models <ref type="bibr" target="#b10">(Frankle et al., 2020;</ref><ref type="bibr" target="#b33">Team et al., 2025)</ref>. When α &gt; 1, extrapolation is introduced, further shortening the reasoning path beyond what was observed during training. This enables an exploration of the minimal reasoning length required to arrive at a given answer. Thus, by adjusting α at inference, we can modulate the model's behavior, with each value of α corresponding to different CoT lengths.</p><formula xml:id="formula_3">( # Δ ! $ ( % Δ ! $ ! " {( # , + , -# } {( % , + ,-% } Δ! $ satisfies … ! ! ! " $ ! " ! " $$ Synthesized Reasoning Path ! Model Gradient Update # ! " ! Δ! '</formula><p>Application Unlike prompt-based approaches that can only regulate the overall length of the reasoning process using prompt words, ∆θ provides finer granularity control. ∆θ is served in the external parameter space. This allows for greater flexibility in adjusting the reasoning trajectory. Specifically, it facilitates the selective retention of longchain reasoning in certain thoughts while applying stronger compression to simpler reasoning segments. As a result, reductions in chain length can be localized to specific portions of the inference process rather than being uniformly applied across the entire reasoning path. We remain the design of this segment selection in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Construct the MixChain Dataset</head><p>A crucial thing for the above process is the construction of the training dataset, especially the reasoning chain {t i } n i=1 . To have reasoning chains with different lengths, previous approaches rely on multiple rounds of sampling, selecting reasoning paths under different random seeds, or using some handcrafted way to remove parts of the answer <ref type="bibr" target="#b1">(Chen et al., 2024)</ref>.</p><p>We introduce MixChain, a dataset inherently generated by our method that contains reasoning paths of varying lengths. This dataset is structured such that each question is associated with multiple reasoning paths, with lengths progressively decreasing from long to short. By simply adjusting the parameter α, our approach avoids the need for repeated sampling and achieves this diverse set of reasoning paths. In contrast to multi-sampling techniques, MixChain enables a more reliable and consistent generation of shorter reasoning paths while simultaneously capturing a spectrum of reasoning lengths. To construct MixChain, we consider two possible scenarios:</p><p>• If a well-annotated dataset with humanlabeled solutions is available, such as GSM8K <ref type="bibr">(Cobbe et al., 2021b)</ref> or PRM800k <ref type="bibr" target="#b21">(Lightman et al., 2024)</ref>, it can be leveraged to fine-tune the model for generating shorter reasoning chains as a cold start (θ 1 → θ1 and θ 2 → θ2 in Figure <ref type="figure" target="#fig_5">2</ref>).</p><p>• In the absence of a dataset containing explicit reasoning paths, or when only final answers are available without full explanations, training solely on final answers is unlikely to enable the model to generate reasoning steps. To address this limitation, we propose an alternative method for constructing Mix-Chain. Specifically, we leverage an existing base LLM (e.g., LLaMA-3.1-8B or Qwen-32B-Instruct) as θ 1 and use its corresponding reasoning model (e.g., DeepSeek-R1-Distill-Llama-8B or QwQ-Preview) to derive ∆θ.</p><p>The parameter update between these models serves as a form of linear interpolation, enabling the transition from θ 1 to θ 2 . This transition is then used to construct the dataset, as illustrated in Figure <ref type="figure" target="#fig_5">2</ref>, where the parameter shift is represented by θ 1 → θ 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Improved Tuning for CoT-Valve</head><p>In this section, we present two enhanced variants of CoT-Valve: one aimed at achieving improved controllability and the other focused on optimizing the compression ratio of the reasoning paths.</p><p>A More Precise CoT-Valve Paradigm: CoT-Valve++. In the previously proposed CoT-Valve framework, the training process only constrained ∆θ to satisfy the final objective with α = 1. However, during inference, we expect all positions along this direction to exhibit reasoning trajectories of varying lengths. This leads to the inconsistency between training and inference. With MixChain, we can explicitly incorporate this requirement during training by introducing an additional constraint, ensuring that the model can adapt to reasoning chains of different lengths across all positions in this direction. For each training sample, in addition to the question, answer, and solution, we have introduced a normalized term β, which represents the factor for the length of the reasoning path. Under this dataset, our training objective is modified to find a parameter update ∆θ ′ such that it satisfies:</p><formula xml:id="formula_4">max ∆θ ′ E (q,a)∼D ′ p a | t &lt;m , q; θ + β∆θ ′ m i=1 p(t i |t &lt;i , q; θ + β∆θ ′ ) (3)</formula><p>Where D ′ is the Mixchain dataset. Each sample consists of the question q, the answer a, the solution {t i } m i=1 and β, where β is calculated as:</p><formula xml:id="formula_5">β = 1 - m -m min m max -m min (4)</formula><p>Here, m min and m max is the length of the shortest solution and longest solution for this question.</p><p>Based on synthetic samples, we introduce additional constraints that enable us to better identify the updated parameter ∆θ ′ , facilitating more precise compressibility and controllability.</p><p>Progressive Chain Compression: CoT-Valve+P. The structure of MixChain, which features progressively shorter reasoning paths for each question, facilitates a progressive chain-length compression strategy. This approach is similar to iterative pruning in model compression <ref type="bibr" target="#b28">(Molchanov et al., 2016)</ref>.</p><p>In this process, the model is trained with a shorter reasoning path from the dataset at each iteration, rather than training directly with the shortest reasoning CoT. This gradual compression method allows the model to progressively reduce the length of its reasoning paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Models. We evaluate our method under several models: QwQ-32B-Preview <ref type="bibr">(Team, 2024b)</ref>, DeepSeek-R1-Distill-Llama-8B (DeepSeek-AI, 2025), LLaMA-3.1-8B <ref type="bibr" target="#b9">(Dubey et al., 2024)</ref>, LLaMA-3.2-1B <ref type="bibr" target="#b9">(Dubey et al., 2024)</ref> and Qwen-32B-Instruct <ref type="bibr">(Team, 2024a)</ref> with LIMO <ref type="bibr" target="#b40">(Ye et al., 2025)</ref>. We tested different scenarios for CoT-Valve:</p><p>• (Long to Short CoT) For QwQ-32B-Preview (QwQ for abbreviation) and DeepSeek-R1-Distill-Llama-8B (R1-Distill), we used our method to control and compress the length of the reasoning chain.</p><p>• (Short to Long CoT) For LLaMA-3.1-8B and LLaMA-3.2-1B-Instruct, we applied our method to distill reasoning abilities from QwQ-32B-Preview and incorporated CoT-Valve in the distillation process.</p><p>• (Short-Long-Short CoT) We tested another setting to first post-train a short-CoT LLM, Qwen-2.5-32B-Instruct <ref type="bibr">(Team, 2024a)</ref>, to generate Long CoT and then compress it to Short CoT. CoT-Valve can be applied in both two stages.</p><p>Metrics. We report both accuracy and the number of tokens in the answer for each experiment.</p><p>Given the trade-off between reasoning path length, model size, and performance, we use a new metric, Accuracy per Computation Unit(ACU), to better capture this balance and evaluate model efficiency.</p><p>It is defined as:</p><formula xml:id="formula_6">ACU = Accuracy #Params × #Tokens (5)</formula><p>Since the ACU value typically falls within the range of 10 -5 to 10 -2 , we report it in units of 10 2 for improved readability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training and Evaluation.</head><p>For training the model, we use LoRA <ref type="bibr" target="#b16">(Hu et al., 2022)</ref> in most of our experiments, except in the experiment for LIMO on Qwen-2.5-32B-Instruct we use full parameter fine-tuning. We also show the results using DoRA <ref type="bibr">(Liu et al., 2024a)</ref> in the Appendix. The hyper-parameters for each experiment are shown in Appendix A. We select two math datasets to evaluate the performance, for one easy math dataset, GSM8K <ref type="bibr">(Cobbe et al., 2021b)</ref> and one hard math dataset, AIME24.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets</head><p>We find in our experiments that the quality of the solution is important to the performance, even if all the human-annotated solutions or synthesized solutions reach the final answer. In our experiments, we use the question from the train set of GSM8K, the math split of PRM800K or the question from LIMO, and we employ three types of datasets with those questions in our experiments:</p><p>• Ground-truth Dataset: The dataset provides a human-annotated or model-synthesized solution. We use this as the cold start.</p><p>• MixChain from cold-start (MixChain-C): After taking the ground-truth dataset to train the model, we can get the first model to generate solutions from short to long. Then we use it to generate the dataset.</p><p>• MixChain from zero-shot (MixChain-Z): We employ CoT-Valve between a reasoning model (θ 2 ) and a base LLM (θ 1 ) to generate the solutions.</p><p>For each dataset, we filter out all the solutions with incorrect answers. We show the statistics of the dataset in</p><p>Table 9 in the Appendix. Method Accuracy #Token ACU ↑ Llama-3.3-70B-Instruct 92.6 235.4 0.56 Llama-3.1-405B-Instruct 95.6 186.7 0.13 Qwen2.5-32B-Instruct 93.1 269.3 1.09 Qwen2.5-Math-72B-Instruct 95.8 312.1 0.43 QwQ-32B-Preview 95.1 741.1 0.40 Prompt (Han et al., 2024) 93.6 355.5 0.82 Prompt (Ding et al., 2024) 95.5 617.7 0.48 In-domain Train Set: GSM8K CoT-Valve -Ground-Truth 94.0 352.8 0.83 CoT-Valve++ -MixChain-C 94.4 276.3 1.07 CoT-Valve+P -MixChain-Z 96.1 317.1 0.95 CoT-Valve+P -MixChain-Z 94.9 225.5 1.32 Out-of-Domain Train Set: PRM12K Overthink(Chen et al., 2024) -SFT 94.8 749.5 0.40 Overthink(Chen et al., 2024) -SimPO 94.8 326.2 0.91 O1-Pruner(Luo et al., 2025a) -SFT 95.7 717 0.42 O1-Pruner(Luo et al., 2025a) 96.5 534 0.56 CoT-Valve+P -MixChain-Z 95.4 288.5 1.03 Table 1: Results of QwQ-32B-Preview on GSM8K. Values of ACU are scaled by 10 2 for readability. We list the dataset we use after the method name. 4.3 From Long-CoT to Short-CoT. Controllable Results. We illustrate the result in Figure 3a. First, using ground-truth samples as a cold start, we develop a model capable of generating reasoning paths of various lengths, as demonstrated in 'CoT-Valve' in Figure 3a. CoT-Valve already matches the performance of prompt-based control but can generate shorter reasoning chains. We then extrapolate ∆θ to produce even shorter reasoning paths. Then, building on MixChain-C from this first model, we conduct further training by CoT-Valve++. CoT-Valve++ substantially surpasses the baseline and shows greater generalization capabilities in cases of extrapolation. Compression Results. We evaluated our method against previous chain compression approaches, with the results detailed in Table 1, Table 2, and Method AIME24 #Token ACU↑ Qwen2.5-32B-Instruct 4/30 1794.2 0.023 Qwen2.5-Math-72B-Instruct 7/30</p><p>1204.5 0.061 Gemini-Flash-Thinking <ref type="bibr" target="#b32">(Team et al., 2023)</ref> 15/30 10810.5 -QwQ-32B-Preview.Train set: GSM8K</p><p>QwQ-32B-Preview 14/30 6827.3 0.021 Prompt <ref type="bibr" target="#b13">(Han et al., 2024)</ref> 13/30 6102.5 0.022 Prompt <ref type="bibr" target="#b8">(Ding et al., 2024)</ref> 13/30 5562.3 0.024 Overthink <ref type="bibr" target="#b1">(Chen et al., 2024)</ref> 13 Table <ref type="table" target="#tab_2">3</ref>. For GSM8K, we adhered to the baseline setup to train with PRM12K. Utilizing progressive compression, our method surpassed the baseline by producing shorter reasoning paths and improved performance.</p><p>We also report experimental results on AIME, where the model was trained using MixChain-Z derived from GSM8K. To minimize the impact of randomness on performance, we employed greedy decoding in our AIME experiments. Compared to the baseline <ref type="bibr" target="#b1">(Chen et al., 2024)</ref>, our method reduced the token count from 5155 to 4630 while maintaining the same accuracy, despite being trained on an easier dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">From Short-CoT to Long-CoT &amp; Short-Long-Short CoT</head><p>Our method can also be applied if a short-CoT model is distilled or post-trained to be a Long-CoT model. The results are shown in Figure <ref type="figure" target="#fig_6">3b</ref>, Table <ref type="table" target="#tab_3">4</ref> and <ref type="table">Table 5</ref>. We found that CoT-Valve can also effectively control the length of the chains in this setting. Notably, we observed that shorter chains could achieve higher accuracy on GSM8K. Moreover, if the model is trained using the MixChain-Z dataset, the results are significantly better, whether using CoT-Valve (55.5 to 58.9) or just simply SFT Table 5: Result on LLaMA-3.1-8B. We report the result of Strict Match here.</p><p>(52.7 to 57.0). Additionally, after training a longchain model, we can employ the MixChain dataset to reduce the length of its reasoning chains further. As illustrated in Figure <ref type="figure" target="#fig_6">3c</ref>, the results suggest that initially training the chains to be long and subsequently compressing them to be shorter (Results with Long-to-Short) can yield better performance than directly using CoT-Valve in the short-to-long stage (Results with Short-to-Long). This demonstrates significant potential for compressing the reasoning chains. We can also surpass the result of Gemini-Flash-Thinking, with the same accuracy but fewer tokens (10810.5 v.s. 8174.8)</p><p>Training dynamics does not have the same effect as CoT-Valve. We also explore whether intermediate training steps can achieve similar effects. As depicted in Figure <ref type="figure" target="#fig_6">3c</ref>, during the early training phases, the length of the CoT increases but does not correspond with the same rapid improvement in performance. As training progresses, the token length begins to decrease while performance improves. CoT-Valve exhibits a distinct pattern, smoothly bridging the gap between the length of CoT and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Observations</head><p>Based on the results from LLaMA-3.1-8B, LLaMA-3.2-1.5B, QwQ, DeepSeek-R1-Distill-</p><p>Solution Solution Length Accuracy #Token Ground-Truth (Solution 0) 116.0 43.8 139.4 Solution 1 279.6 57.0 288.4 Solution 2 310.7 55.1 330.0 Solution 3 386.7 56.5 414.6 Solution 4 497.2 52.5 558.3 Table 6: Train LLaMA-3.2-1B-Instruct with solutions in MixChain-Z of different lengths on GSM8K.</p><p>Llama-8B and Qwen2.5-32B-Instruct with LIMO, we summarize the following observations:</p><p>• Longer reasoning chains are not always the best on simple datasets. Across nearly all models, we find that those directly trained on long CoT data typically do not show the best performance. These models often underperform compared to those generated through CoT-Valve, which results in shorter but more accurate reasoning chains. This trend is particularly pronounced in smaller models. For instance, in the LLaMA-3.2-1B model, training on QwQ synthesized data yields an accuracy of 52.69 with 759.3 tokens. However, using CoT-Valve, we can achieve an accuracy of 55.50 with only 267.0 tokens. However, we do not observe this phenomenon in more complex datasets, indicating that while the reasoning model may be redundant for simple datasets, it still requires test-time scaling to effectively handle complex datasets.</p><p>• Some reasoning chains are difficult for model to learn, especially for small LLMs.</p><p>We fine-tuned LLaMA-3.2-1B-Instruct using only one solution from MixChain, where all solutions lead to the same final answer but involve different reasoning steps.</p><p>The results, presented in Table <ref type="table">6</ref>, indicate that neither the shortest nor the longest chains are optimal for learning. Instead, the model most effectively learns from moderately short chains, achieving the highest accuracy while maintaining a relatively low token count. This phenomenon is particularly evident in smaller models, but it is not observed in larger models. We believe this could be beneficial for the distillation of CoT in small LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Analysis</head><p>Ablation on Progressive Compression. Table <ref type="table" target="#tab_7">7</ref> demonstrates the effect of progressive compression.</p><p>Solution Used #Epoch #Samples Accuracy #Tokens ACU↑ ---95.07 741.1 0.40 4 1 6.8k 95.68 597.3 0.50 4+3 1 13.7k 94.84 458.4 0.65 4+3+2 1 20.5k 94.84 339.9 0.87 4+3+2+1 1 27.4k 96.13 317.1 0.95 4+3+2+1+0 1 34.2k 94.92 225.5 1.32 0 5 37.4k 92.19 250.5 1.15 Table 8: CoT-Valve can achieve shorter chains than prompts with better performance.</p><p>We compare two settings: training directly with the ground-truth solution for five epochs and applying progressive compression for five epochs in total, with the final epoch using the ground-truth data.</p><p>Our results show that progressive compression significantly improves the performance of short CoT (from 92.19 to 94.92). For each turn, progressive compression gradually reduces the token number while maintaining accuracy.</p><p>CoT-Valve achieves shorter chains compared to prompt control We also present in Table <ref type="table">8</ref> the shortest chain achieved by our method and compare these with those obtained using prompt control.</p><p>Our method outperforms prompt control methods at shorter chain lengths. Additionally, we explored the limits of chain length for both methods and found that our approach can generate substantially shorter chains than what can be achieved through prompt control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a method that enables a model to generate reasoning chains of varying lengths instead of the prompt control. Based on this approach, we construct a dataset containing both long and short reasoning chains to further enhance controllability and compression efficiency. Experimental results demonstrate the effectiveness of our method in dynamic reasoning chain control and the compression of CoT. Future research can further explore finer-grained control strategies to improve reasoning efficiency and model controllability. Table 10: Results of LLaMA-3.2-1B-Instruct trained with DoRA using different α values for interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Analysis</head><p>Experiments on DoRA. In addition to LoRA, we also train LLaMA-3.2-1B using DoRA <ref type="bibr">(Liu et al., 2024a)</ref> and control the magnitude of ∆θ by adjusting the α for DoRA. The model is trained on QwQ synthesized data for a maximum of five epochs. We set the batch size to 8 and the peak learning rate to 4e-5, following a cosine decay schedule. A weight decay of 0.01 is applied. For DoRA, the rank is set to 32, and the lora_alpha for training is set to 64. As shown in Table <ref type="table">10</ref>, the chain length increases with the α value, demonstrating the effectiveness of interpolating ∆θ for DoRA. Furthermore, similar to our observations with LoRA, the best result is not obtained by directly training the model on long CoT data. Specifically, training on QwQ synthesized data (α = 1.0) achieves an accuracy of 54.5 with 772.7 tokens, whereas the best model obtained via   with applying LoRA fine-tuning to only specific model components in QwQ. Table <ref type="table" target="#tab_11">11</ref> shows that fine-tuning only the query, key, or value projection had less effect than other linear projections on shortening the reasoning chain. This suggests that attention computation may have less impact on controlling chain length, whereas the MLP layers and the final projection in attention play a more significant role. We find this to be an intriguing phenomenon, and further investigation into its underlying causes could provide deeper insights into optimizing reasoning chain compression.</p><p>Prompt Control. We evaluate the length of CoT under constraint in prompts. Table <ref type="table">12</ref> presents the number of tokens generated when using various prompts across two models. For these two models, the prompts used here are:</p><p>Prompt Template for QwQ-32B-Preview System: You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step. User: Generate the solution in less than &lt;to-ken_count&gt; tokens. &lt;Question&gt; Prompt Template for LLaMA-3.2-1B-Instruct System: Cutting Knowledge Date: December 2023 Today Date: 01 Jan 2025 User: Given the following problem, reason and give a final answer to the problem using less than &lt;token_count&gt; tokens. &lt;Question&gt;</p><p>From the results, we observe that while these prompts provide control over the length, there remains a significant discrepancy between the generated token count and the intended target. Generated Answer for the Same Question Question: Amanda had 10 notebooks. This week, she ordered 6 more and then lost 2. How many notebooks does Amanda have now?</p><p>Ground Truth Solution -49 Tokens: This week, Amanda added 6 notebooks -2 notebooks = «6-2=4»4 notebooks. Now, Amanda has 10 notebooks + 4 notebooks = «10+4=14»14 notebooks.</p><p>Extrapolation -300 Tokens: So Amanda started with 10 notebooks. Then, she ordered 6 more. So, I think I need to add those 6 to the original 10. That should be 10 plus 6, which is 16. Okay, now she lost 2 notebooks. So, I need to subtract those 2 from the 16. That should be 16 minus 2, which equals 14. So, Amanda has 14 notebooks now. Wait a minute, let me double-check that. She had 10, ordered 6 more, so that's 10 + 6 = 16. Then lost 2, so 16 -2 = 14. Yeah, that seems right.Alternatively, maybe I can think of it differently. Maybe consider the net gain of notebooks. She ordered 6 and lost 2, so net gain is 6 -2 = 4. Then, add that net gain to the original 10: 10 + 4 = 14. Same answer. Or, maybe another way: starting with 10, losing 2 would bring her to 8, and then ordering 6 more would bring her to 14. Again, the same result. I think no matter how I look at it, the answer is 14 notebooks. **Final Answer** \boxed{14}</p><p>QwQ-32B-Preview -257 tokens: So Amanda started with 10 notebooks. Then, she ordered 6 more. So, I need to add those 6 to the original 10. That should be 10 plus 6, which is 16. Okay, now she lost 2 notebooks. So, I need to subtract those 2 from the 16 she had after ordering more. That means 16 minus 2, which equals 14. So, Amanda has 14 notebooks now. Wait a minute, let me double-check that. She had 10, ordered 6 more, so that's 10 + 6 = 16. Then lost 2, so 16 -2 = 14. Yeah, that seems right. Alternatively, maybe I can think of it differently. Maybe combine the ordered and lost notebooks first. She ordered 6 and lost 2, so overall, she gained 6 -2 = 4 notebooks. Then, add that to the original 10: 10 + 4 = 14. Same answer. I think that's correct. Amanda has 14 notebooks now. **Final Answer** \boxed{14}</p><p>Interpolation -207 Tokens: Let's break this down step by step. Amanda started with 10 notebooks. Then, she ordered 6 more. So, I need to add those 6 to the original 10. That would be 10 plus 6, which equals 16 notebooks. Okay, so now she has 16 notebooks. But then, she lost 2. So, I need to subtract those 2 from the 16. That would be 16 minus 2, which equals 14 notebooks. Therefore, Amanda now has 14 notebooks. Wait a minute, let me double-check that. She started with 10, ordered 6 more, so that's 10 + 6 = 16. Then lost 2, so 16 -2 = 14. Yeah, that seems right. I think that's the answer: Amanda has 14 notebooks now.</p><p>Interpolation -133 Tokens: Let's solve this step-by-step: 1. **Initial number of notebooks**: Amanda started with 10 notebooks. 2. **Notebooks ordered**: This week, she ordered 6 more notebooks. -So, we add these 6 notebooks to her initial 10 notebooks. -10 + 6 = 16 notebooks. 3. **Notebooks lost**: Then, Amanda lost 2 notebooks. -We need to subtract these 2 lost notebooks from the total she had after ordering more. -16 -2 = 14 notebooks. Therefore, Amanda now has 14 notebooks.</p><p>Extrapolation -103 Tokens: To find out how many notebooks Amanda has now, we can break the problem into steps: 1. Amanda initially had 10 notebooks. 2. This week, she ordered 6 more notebooks, so we add 6 to the initial amount: 10 + 6 = 16 notebooks. 3. Amanda then lost 2 notebooks, so we subtract 2 from the total: 16 -2 = 14 notebooks. So, Amanda has 14 notebooks now. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Examples</head><p>Here we show in Fig. <ref type="figure" target="#fig_7">4</ref> an example of the generated CoT from short to long, and we also show two extrapolation cases to show the generalization ability of our method. Our method notably generates a longer reasoning process compared to the original QwQ model, incorporating an extra reflection phase. During the chain shortening process, it reduces multiple rounds of reasoning and streamlines the language, ultimately enabling us to produce an answer with only 103 tokens through extrapolation.</p><p>QwQ-32B-Preview Llama-3.2-1B Instruct Token in Prompt #Token Generated Token in Prompt #Token Generated 20 355 50 118 50 422 100 132 100 511 200 141 200 569 300 160 300 623 400 183 400 666 500 186</p><p>Table <ref type="table">12</ref>: Significant discrepancies exist between the conditions specified in the prompt and the number of generated tokens on GSM8k.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The reasoning model, after the length-compressible CoT tuning, can generate reasoning paths from long to short, leveraging LoRA as a 'Valve'. We show one example from our constructed dataset MixChain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>results than previous chain compression baselines. Besides, our study highlights several interesting findings: (1) Short reasoning paths can sometimes outperform longer ones, underscoring the significance of CoT-Valve in enhancing model efficiency. (2) Not every reasoning chain, despite all leading to the correct final answer, is conducive to model optimization. Excessively long or short chains complicate the distillation of CoT, posing challenges to the model training. In summary, our contributions are: (1) CoT-Valve: Enables elastic control of length for CoT within the parameter space, allowing a single model to generate CoT from short to long. (2) MixChain Dataset: A dataset with reasoning paths of varying lengths for each question. (3) Improved Tuning &amp; Progressive Compression: Refines the directiontuning process based on MixChain and introduces progressive compression for inference efficiency. (4) Performance &amp; Controllability: Achieves controllable reasoning generation and state-of-theart results for compressed CoT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of CoT-Valve. In Stage 1, we first determine ∆θ from distilling or post-training. Then, the trained ∆θ is utilized to construct the MixChain dataset. Using this dataset, we can then apply two enhanced training methods to achieve more precise control over reasoning paths, or to shorten the reasoning paths as needed.</figDesc><graphic coords="4,72,70,73,16,168,42,152,98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Token length and accuracy for different methods, datasets and reasoning models. Points connected by curves in (a) and (b) represent results from one model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example of the generated solution for the same question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Result of DeepSeek-R1-Distill-Llama-8B.</figDesc><table><row><cell></cell><cell></cell><cell>/30</cell><cell cols="2">5154.5 0.026</cell></row><row><cell>CoT-Valve -GSM8K</cell><cell></cell><cell>14/30</cell><cell cols="2">5975.0 0.024</cell></row><row><cell>CoT-Valve++ -MixChain-C</cell><cell></cell><cell>13/30</cell><cell cols="2">5360.5 0.025</cell></row><row><cell>CoT-Valve+P -MixChain-Z</cell><cell></cell><cell>13/30</cell><cell cols="2">4629.6 0.029</cell></row><row><cell cols="3">Qwen-32B-Instruct. Train set: LIMO</cell><cell></cell><cell></cell></row><row><cell>Qwen-32B-LIMO</cell><cell></cell><cell>15/30</cell><cell cols="2">10498.2 0.015</cell></row><row><cell>CoT-Valve</cell><cell></cell><cell>11/30</cell><cell cols="2">6365.2 0.018</cell></row><row><cell>SFT -MixChain -Solution 1</cell><cell></cell><cell>13/30</cell><cell cols="2">5368.0 0.025</cell></row><row><cell>CoT-Valve -MixChain -Solution 1</cell><cell></cell><cell>15/30</cell><cell cols="2">8174.8 0.019</cell></row><row><cell cols="5">Table 2: Results of QwQ-32B-Preview and Qwen-32B-</cell></row><row><cell cols="2">Instruct w/ LIMO on AIME 24.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">GSM8k</cell><cell cols="2">AIME24</cell></row><row><cell>Model</cell><cell cols="2">Acc #Token</cell><cell cols="2">Acc # Token</cell></row><row><cell>Llama-3.1-8B (0-shot)</cell><cell>15.7</cell><cell>915.0</cell><cell>0/30</cell><cell>1517.6</cell></row><row><cell>R1-Distill-Llama-8B</cell><cell cols="4">87.1 1636.6 14/30 12359.9</cell></row><row><cell>CoT-Valve</cell><cell cols="2">87.3 1315.2</cell><cell>6/30</cell><cell>7410.5</cell></row><row><cell cols="2">CoT-Valve+P -MixChain-Z 84.0</cell><cell cols="2">755.2 11/30</cell><cell>9039.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results on LLaMA-3-2-1B-Instruct. We report the result of Flexible Match here. QwQ Distill means we use QwQ to synthesize the solution and distill it.</figDesc><table><row><cell>Method</cell><cell cols="3">Accuracy #Tokens ACU↑</cell></row><row><cell>LLaMA-3.2-1B-Instruct(8-shot)</cell><cell>45.9</cell><cell cols="2">104.3 44.008</cell></row><row><cell>LLaMA-3.2-1B-Instruct(0-shot)</cell><cell>45.9</cell><cell cols="2">199.8 22.973</cell></row><row><cell>SFT-Full Finetune -GSM8k</cell><cell>46.1</cell><cell cols="2">139.4 33.070</cell></row><row><cell>SFT -GSM8k</cell><cell>43.8</cell><cell cols="2">137.7 31.808</cell></row><row><cell>Prompt</cell><cell>46.7</cell><cell cols="2">209.9 22.249</cell></row><row><cell>SFT -QwQ Distill</cell><cell>52.7</cell><cell>759.3</cell><cell>6.941</cell></row><row><cell>CoT-Valve -QwQ Distill</cell><cell>55.5</cell><cell cols="2">267.0 20.786</cell></row><row><cell>CoT-Valve+P -MixChain-Z</cell><cell>55.8</cell><cell cols="2">291.0 19.175</cell></row><row><cell>SFT -MixChain-Z -Solution 1</cell><cell>57.0</cell><cell cols="2">288.4 19.764</cell></row><row><cell>CoT-Valve -MixChain-Z -Solution 1</cell><cell>58.9</cell><cell cols="2">275.4 21.387</cell></row><row><cell>Method</cell><cell cols="3">Accuracy #Tokens ACU↑</cell></row><row><cell>LLaMA-3.1-8B (8-shot)</cell><cell>56.9</cell><cell cols="2">282.1 2.521</cell></row><row><cell>LLaMA-3.1-8B (0-shot)</cell><cell>15.7</cell><cell cols="2">915.0 0.214</cell></row><row><cell>SFT-LoRA -GSM8k</cell><cell>59.0</cell><cell cols="2">191.9 3.843</cell></row><row><cell>SFT-LoRA -QwQ Distill</cell><cell>76.3</cell><cell cols="2">644.8 1.479</cell></row><row><cell>CoT-Valve -QwQ Distill</cell><cell>77.5</cell><cell cols="2">569.8 1.700</cell></row><row><cell>CoT-Valve+P -MixChain-Z</cell><cell>77.1</cell><cell cols="2">371.2 2.596</cell></row><row><cell>CoT-Valve + MixChain-Z -Solution 1</cell><cell>75.7</cell><cell cols="2">264.1 3.583</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Ablation of Progressive Compression on QwQ.</figDesc><table><row><cell cols="5">Here, solution 0 is the human-annotated solution from</cell></row><row><cell>the original dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">QwQ-32B-Preview Llama-3.2-1B-I</cell></row><row><cell>Method</cell><cell>Acc</cell><cell>#Token</cell><cell cols="2">Acc #Token</cell></row><row><cell cols="2">Prompt (Shortest) 93.6</cell><cell>355.5</cell><cell>52.5</cell><cell>621.0</cell></row><row><cell>Ours (Best)</cell><cell>94.4</cell><cell>276.3</cell><cell>55.5</cell><cell>267.0</cell></row><row><cell>Ours (Shortest)</cell><cell>87.5</cell><cell>133.8</cell><cell>50.4</cell><cell>247.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Dataset Statistic. Here we use the tokenizer from QwQ-32B-Preview to count the number of tokens.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">Solution Index</cell><cell cols="3">#Samples #Avg Token</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GSM8K</cell><cell></cell><cell></cell></row><row><cell>Ground-Truth</cell><cell></cell><cell>1</cell><cell></cell><cell>7473</cell><cell></cell><cell>121.8</cell></row><row><cell>MixChain-C</cell><cell></cell><cell>1</cell><cell></cell><cell>22419</cell><cell></cell><cell>294.8</cell></row><row><cell></cell><cell cols="3">0 (Ground-Truth)</cell><cell></cell><cell></cell><cell>116.0</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell>279.6</cell></row><row><cell>MixChain-Z</cell><cell></cell><cell>2</cell><cell></cell><cell>6863</cell><cell></cell><cell>310.7</cell></row><row><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell>386.7</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell>497.2</cell></row><row><cell></cell><cell></cell><cell cols="2">PRM12K</cell><cell></cell><cell></cell></row><row><cell>Ground-Truth</cell><cell></cell><cell>1</cell><cell></cell><cell>12000</cell><cell></cell><cell>223.1</cell></row><row><cell></cell><cell cols="3">0 (Ground-Truth)</cell><cell></cell><cell></cell><cell>172.3</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell>583.2</cell></row><row><cell>MixChain-Z</cell><cell></cell><cell>2</cell><cell></cell><cell>8841</cell><cell></cell><cell>613.7</cell></row><row><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell>739.3</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell cols="2">1003.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>LIMO</cell><cell></cell><cell></cell></row><row><cell>Ground-Truth</cell><cell></cell><cell>1</cell><cell></cell><cell>817</cell><cell cols="2">6984.1</cell></row><row><cell>MixChain-C</cell><cell></cell><cell>1 2</cell><cell></cell><cell>474 564</cell><cell cols="2">2994.7 4890.6</cell></row><row><cell>α</cell><cell>0</cell><cell cols="2">0.125 0.25</cell><cell>0.5</cell><cell>0.75</cell><cell>1.0</cell></row><row><cell cols="7"># Tokens 199.8 219.4 233.4 257.7 466.3 772.7</cell></row><row><cell cols="2">Accuracy 45.9</cell><cell>47.5</cell><cell>50.2</cell><cell>57.1</cell><cell>55.0</cell><cell>54.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>achieves an accuracy of 55.72 with only 257.7 tokens.LoRA on Different Modules.</figDesc><table><row><cell>Attention has less effect on the length of the</cell></row><row><cell>reasoning path than MLP. We experimented</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>A.1 Evaluation Metric.</p><p>For experiments on LLaMA, we use lm-evalharness 1 to evaluate the model performance. For LLaMA-3.1-8B, we report the strict matching metric due to observed repetition in the model's responses, which causes the flexible match to extract incorrect numerical values. For LLaMA-3.2-1B-Instruct, we report results using the flexible match metric. For QwQ-32B-Preview, DeepSeek-R1-Distill-Llama-8B and Qwen-2.5B-LIMO, we first extract the result enclosed within \boxed{}. If no such boxed answer is found, we default to using the last digit in the response as the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training Setting.</head><p>LLaMA-3.1-8B The model is trained using eight A5000 24GB GPUs. We set the batch size to 64 and the peak learning rate to 4e-5, following a cosine decay schedule. A weight decay of 0.01 is applied. For the progressive chain compression experiment, we train the model for two epochs with each type of solution. For all other experiments, we train for a maximum of eight epochs. For LoRA, the rank is set to 32, and the lora_alpha for training is set to 64. During inference, the maximum number of tokens is set to 2048.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LLaMA-3.2-1B-Instruct</head><p>The model is trained using 8 A5000 24GB GPUs. We set the batch size to 8 for the CoT-Valve experiment and 64 for all other experiments. The peak learning rate is 4e-5, following a cosine decay schedule, except for the SFT -GSM8K experiment, where the peak learning rate is 1e-5. A weight decay of 0.01 is applied. For the CoT-Valve and SFT-Full Finetune -GSM8k experiment, we train for a maximum of four and six epochs, respectively. For the progressive chain compression experiment, we train the model for two epochs with each type of solution. For all other experiments, training is conducted for up to 8 epochs. For LoRA, the rank is set to 32, and the lora_alpha for training is set to 64. During inference, the maximum number of tokens is set to 2048.</p><p>QwQ-32B-Preview. The model is trained on two H100-80G GPUs. We set the batch size to 64 and trained for a maximum of five epochs. The learning rate is 1e-5, with a weight decay of 0.01 applied 1 <ref type="url" target="https://github.com/EleutherAI/lm-evaluation-harness">https://github.com/EleutherAI/lm-evaluation-harness</ref> during training. For LoRA, the rank is set to 2, and the lora_alpha for training is set to 8. During inference, we set the maximum token to be 4192 for GSM8K and the maximum token as 8192 for AIME correspondingly.</p><p>DeepSeek-R1-Distill-Llama-8B. Our experiment on DeepSeek-R1-Distill-Llama-8B 2 is conducted using the MixChain-zero-shot-GSM8K dataset. The batch size is set to 128, and training is performed for a maximum of five epochs. To ensure that the inference process successfully generates the final answer, we set the maximum token limit to 30K.</p><p>Qwen2.5-32B-LIMO. We fine-tuned Qwen-32B-Instruct using LIMO, training on four H100 GPUs for 10 epochs with a batch size of 4 and a maximum sequence length of 16K. The learning rate was set to 5e-6. We define Qwen-32B-Instruct as θ 0 and the trained model as θ 1 , treating the update direction between them as ∆θ. By adjusting α, we generated the MixChain-C-LIMO dataset, which includes two solutions: solution 1 (α=0.8) and solution 0 (α=0.6).</p><p>Based on this, we further trained θ 2 for 5 epochs with a batch size of 32, a learning rate of 5e-6, and a weight decay of 0.01, obtaining the results of MixChain-Solution 0 in Table <ref type="table">2</ref>. This model can be further refined through CoT-Valve (Results: CoT-Valve + MixChain -Solution 0). Unlike previous experiments, we applied full fine-tuning instead of LoRA. The maximum generated sequence length in this experiment was 15K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Dataset Explanation</head><p>As detailed in Section 4.2, we constructed two types of datasets: MixChain-C and MixChain-Z. The statistics for the datasets are shown in 9. For these datasets, we select α values ranging from [0.6, 0.8] for LIMO and [0.2, 0.4, 0.6, 0.8] for other datasets, ensuring all incorrect responses are excluded.</p><p>For MixChain-Z, while the training transition from θ 1 to θ 2 remains a black box, we can still identify numerous model pairs such as Qwen-32B-Instruct → QwQ-32B-Preview, and LLaMA-3.1-8B → R1-Distill-Llama-8B, as documented in the technical report. We find that the performance of the base model significantly influences the quality of the dataset.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Jyoti</forename><surname>Marah Abdin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harkirat</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><forename type="middle">J</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojan</forename><surname>Hewett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Javaheripi</surname></persName>
		</author>
		<author>
			<persName><surname>Kauffmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.08905</idno>
		<title level="m">Phi-4 technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhui</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuzhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.21187</idno>
		<title level="m">Do not think that much for 2+ 3=? on the overthinking of o1-like llms</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.13171</idno>
		<title level="m">Efficient reasoning through dense representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">2021a. Training verifiers to solve math word problems</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno>ArXiv, abs/2110.14168</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">2021b. Training verifiers to solve math word problems</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</title>
		<author>
			<persName><forename type="first">Deepseek-Ai</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2501.12948</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">2024a. From explicit cot to implicit cot: Learning to internalize cot step by step</title>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.14838</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">2024b. Implicit chain of thought reasoning via knowledge distillation</title>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Smolensky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Break the chain: Large language models can be shortcut reasoners</title>
		<author>
			<persName><forename type="first">Mengru</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanmeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizhang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.06580</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiesha</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<title level="m">The llama 3 herd of models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Linear mode connectivity and the lottery ticket hypothesis</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karolina</forename><surname>Gintare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><surname>Carbin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3259" to="3269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">2025a. rstar-math: Small llms can master math reasoning with self-evolved deep thinking</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">Lyna</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youran</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mao</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.04519</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">2025b. rstar-math: Small llms can master math reasoning with self-evolved deep thinking</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">Lyna</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youran</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mao</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.04519</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Tingxu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunrong</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenting</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.18547</idno>
		<title level="m">Token-budget-aware llm reasoning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Training large language models to reason in a continuous latent space</title>
		<author>
			<persName><forename type="first">Shibo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dijia</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.06769</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Sifre</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LoRA: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.04089</idno>
		<title level="m">Editing models with task arithmetic</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Jaech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiden</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Helyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Carney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.16720</idno>
		<title level="m">Openai o1 system card</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Brihi</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahana</forename><surname>Ramnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoliang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.07095</idno>
		<title level="m">Are machine rationales (not) useful to humans? measuring and improving human utility of free-text rationales</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">C3ot: Generating shorter chain-of-thought without compromising effectiveness</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianghui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.11664</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2006">2024. 2006</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note>Levente Kocsis and Csaba Szepesvari Bandit based monte-carlo planning</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Let&apos;s verify step by step</title>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Hunter Lightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harrison</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teddy</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Cobbe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dora: Weightdecomposed low-rank adaptation</title>
		<author>
			<persName><forename type="first">Shih-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">2024b. Can language models learn to skip steps?</title>
		<author>
			<persName><forename type="first">Tengxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangkun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Jiayang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eighth Annual Conference on Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Xiaochun Cao, and Dacheng Tao. 2025a. O1-pruner: Lengthharmonizing fine-tuning for o1-like reasoning pruning</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiying</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiqiang</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.12570</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">2025b. Improve mathematical reasoning in language models with automated process supervision</title>
		<author>
			<persName><forename type="first">Liangchen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samrat</forename><surname>Phatale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Lara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Inference-time scaling for diffusion models beyond scaling denoising steps</title>
		<author>
			<persName><forename type="first">Nanye</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangyuan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haolin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.09732</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simpo: Simple preference optimization with a reference-free reward</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengzhou</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06440</idno>
		<title level="m">Pruning convolutional neural networks for resource efficient inference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Webgpt: Browserassisted question-answering with human feedback</title>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ouyang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyna</forename><surname>Eloundou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Button</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno>ArXiv, abs/2112.09332</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengju</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuofan</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Letian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.16999</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Zayne</forename><surname>Sprague</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangcong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manya</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasann</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.12183</idno>
		<title level="m">To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Gemini</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Hauth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11805</idno>
		<title level="m">Gemini: a family of highly capable multimodal models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Kimi</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bofei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowei</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changjiu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenzhuang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chonghua</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.12599</idno>
		<title level="m">Kimi k1. 5: Scaling reinforcement learning with llms</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Qwq: Reflect deeply on the boundaries of the unknown</title>
		<author>
			<persName><forename type="first">Qwen</forename><surname>Team</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Understanding chain-of-thought in llms through information theory</title>
		<author>
			<persName><forename type="first">Jean-Francois</forename><surname>Ton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Faaiz Taufiq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.11984</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations</title>
		<author>
			<persName><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.acl-long.510</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9426" to="9439" />
		</imprint>
	</monogr>
	<note>Long Papers) Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tree of thoughts: Deliberate problem solving with large language models</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><surname>Karthik R Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2502.03387</idno>
		<title level="m">Limo: Less is more for reasoning</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Distilling system 2 into system 1</title>
		<author>
			<persName><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Kulikov</surname></persName>
		</author>
		<idno>ArXiv, abs/2407.06023</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoshui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongzhan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.07394</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The lessons of developing process reward models in mathematical reasoning</title>
		<author>
			<persName><forename type="first">Zhenru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chujie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangzhen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.07301</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
