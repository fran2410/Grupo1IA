<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2025-05-15">15 May 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yiwen</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jessica</forename><surname>Bader</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Helmholtz Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jae</forename><forename type="middle">Myung</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of TÃ¼bingen</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Helmholtz Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-05-15">15 May 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">188A1A5439317C43C0B2B2B5BD34FA8B</idno>
					<idno type="arXiv">arXiv:2505.10551v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2025-05-26T20:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the development of photorealistic diffusion models, models trained in part or fully on synthetic data achieve progressively better results. However, diffusion models still routinely generate images that would not exist in reality, such as a dog floating above the ground or with unrealistic texture artifacts. We define the concept of feasibility as whether attributes in a synthetic image could realistically exist in the real-world domain; synthetic images containing attributes that violate this criterion are considered infeasible. Intuitively, infeasible images are typically considered out-of-distribution; thus, training on such images is expected to hinder a model's ability to generalize to real-world data, and they should therefore be excluded from the training set whenever possible. However, does feasibility really matter? In this paper, we investigate whether enforcing feasibility is necessary when generating synthetic training data for CLIP-based classifiers, focusing on three target attributes: background, color, and texture. We introduce VariReal, a pipeline that minimally edits a given source image to include feasible or infeasi-ble attributes given by the textual prompt generated by a large language model. Our experiments show that feasibility minimally affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference in top-1 accuracy across three fine-grained datasets. Also, the attribute matters on whether the feasible/infeasible images adversarially influence the classification performance. Finally, mixing feasible and infeasible images in training datasets does not significantly impact performance compared to using purely feasible or infeasible datasets. Code is available at https://github.com/Yiveen/SyntheticDataFeasibility.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose VariReal, a pipeline for minimal-change editing of real images, enabling isolation of target attributes in three categories: background, color, and texture. We compare images generated by VariReal to those produced by prior text-guided editing methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34]</ref>, examining both feasible and infeasible attributes. The editing prompts are provided below each generated image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, large-scale pre-trained models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b59">60]</ref> have significantly surpassed traditional learning approaches in various tasks. However, as the scale of training data grows, access to high-quality data has become increasingly limited <ref type="bibr" target="#b63">[64]</ref>, posing challenges to further improving these large models' capabilities. With the popularity of generative models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30]</ref> like Stable Diffusion <ref type="bibr" target="#b50">[51]</ref>, researchers are increasingly leveraging these models to generate high-fidelity synthetic data that closely resembles real-world data, offering a solution to data scarcity <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Prior studies have explored synthetic data generation under a limited few-shot real image setting <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b58">59</ref>]. These works aim to create synthetic data that approximates the real-world data distribution while avoiding overfitting to the limited available examples. Some studies <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref> suggest that synthetic data can offer benefits beyond those of real data. However, the inherent randomness in the diffusion-based image generation process <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b50">51]</ref> can introduce domain shifts <ref type="bibr" target="#b23">[24]</ref> or implausible scenarios, such as "a dog floating in the sky" <ref type="bibr" target="#b53">[54]</ref>, which fail to reflect realistic patterns. Such data could intuitively be perceived as out-of-distribution (OOD), potentially becoming counterproductive for downstream tasks.</p><p>Interestingly, previous studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19]</ref> suggest that OOD data can positively impact downstream tasks when mixed with real data in certain proportions. A typical example is data augmentation <ref type="bibr" target="#b18">[19]</ref>, where some data augment methods introduce OOD data relative to the original distribution yet still provide benefits. While these advantages generally diminish as divergence from the original distribution increases <ref type="bibr" target="#b8">[9]</ref>, these findings demonstrate OOD data is not always harmful. Conversely, incorporating feasible content, which is considered in-distribution, is naturally beneficial. For instance, Dunlap et al. <ref type="bibr" target="#b13">[14]</ref> propose augmenting training data by synthesizing data with diverse feasible backgrounds and show performance gain. This raises a question: does the feasibility matter for synthetic training data?</p><p>In this paper, we study the impact of the feasibility on synthesized data when using them as training data for the classification task. We define feasibility as whether classspecific attributes could realistically occur in the real world. Attributes that meet this criterion are considered feasible while others are infeasible. For instance, given a Yorkshire terrier in Figure <ref type="figure" target="#fig_0">1</ref>, it is likely to find it at the lake shore, while not at the oil rig platform. Therefore, we assume an image of Yorkshire terrier at the lake shore background as a feasible image, while an image of it at the oil rig platform as an infeasible image.</p><p>To generate feasible and infeasible images and study their impact of a downstream classification task, we propose VariReal, an editing pipeline with minimal change of attributes given a real image. We first generate a list of feasible and infeasible attribute names for each class by using GPT-4 <ref type="bibr" target="#b0">[1]</ref>, with generated attributes further being validated through a user study. We then use a proposed imageediting pipeline based on Stable Diffusion <ref type="bibr" target="#b50">[51]</ref> that generates feasible (or infeasible) images given a source real image and a prompt with a feasible (or infeasible) attribute name. We then assess the impact of the feasibility of images to downstream tasks by fine-tuning CLIP-based classifiers under two conditions: synthetic-only training and mixed real-synthetic training.</p><p>Our study of feasibility for a downstream task in three different attributes (background, color, texture) on three fine-grained datasets reveals the following insights. First, we show that changing the background regardless of feasibility brings performance gain, which loosens a restriction considered in ALIA <ref type="bibr" target="#b13">[14]</ref> where it only uses a feasible background scenario. Second, foreground modifications, like color or texture attributes, often challenge the classifier's learning process especially when the training datasets are infeasible inputs.</p><p>In summary, our contributions are as follows: â¢ We propose VariReal, an automated generation pipeline for producing minimal-change synthetic data by altering only one attribute from real images at a time. This approach can be applied out-of-the-box to any objectcentric classification dataset without additional finetuning. â¢ We define and generate feasible and infeasible dataset comparison pairs based on real images, covering three controlled attributes. â¢ To explore feasible and infeasible data roles, we fine-tune CLIP with LoRA <ref type="bibr" target="#b26">[27]</ref>. Analyzing classification scores, we offer new insights into the impact of feasibility and the strategic use of synthetic data for enhancing downstream classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Effect of out-of-distribution data. OOD data, defined relative to in-distribution data, introduces a distribution shift between train and test data. OOD data is generally categorized into semantic and covariance shifts <ref type="bibr" target="#b56">[57]</ref>; here, we focus on covariance shifts. The impact of OOD data is commonly evaluated using classification tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Early works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref> attributed OOD data's benefits to feature invariance and the stochasticity it adds in gradient descent, helping avoid local minima and improving optimization. However, this conclusion was drawn only using simple OOD data types like rotation. Silva et al. <ref type="bibr" target="#b8">[9]</ref> and Geiping et al. <ref type="bibr" target="#b18">[19]</ref> show that, for small domain shifts, adding OOD data reduces generalization error on the original test set and exhibits non-monotonic behavior. While most research has relied on basic models (e.g., ResNet <ref type="bibr" target="#b22">[23]</ref>) and datasets (e.g., MNIST <ref type="bibr" target="#b10">[11]</ref>), our work seeks to produce OOD data study to more complex scenarios with diffusion models, utilizing advanced classification architectures to deepen the understanding of OOD effects.</p><p>Learning with synthetic data. Several studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b58">59</ref>] focus on generating synthetic data that approximates real-world distributions. These approaches aim to create a dataset larger than the few-shot samples. Generated data supports various tasks, including object recogni- tion <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b53">54]</ref>, object detection <ref type="bibr" target="#b16">[17]</ref>, and semantic segmentation <ref type="bibr" target="#b54">[55]</ref>. Its effectiveness is demonstrated by training CLIP <ref type="bibr" target="#b46">[47]</ref> models exclusively on synthetic data or in combination with real data <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref>. As a result, we focus specifically on object classification using CLIP model. Automatic approach for minimal change generation. Unlike synthetic data generation methods that focus on creating novel and diverse in-distribution images <ref type="bibr" target="#b27">[28]</ref>, minimal change generation aims only to modify specific areas or attributes of existing real images. Generative models, particularly diffusion-based approaches <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53]</ref>, facilitate efficient image editing without requiring manual annotation <ref type="bibr" target="#b23">[24]</ref> or physical graphics engines <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b47">48]</ref>. In particular, text-to-image stable diffusion methods are popular for minimal-change editing due to their high fidelity generation. Beyond text guidance, these models also support diverse conditioning inputs, such as reference images through IP-Adaptor <ref type="bibr" target="#b57">[58]</ref> and Canny edge maps through Control-Net <ref type="bibr" target="#b60">[61]</ref>.</p><p>These methods fall into two main categories: finetuning needed approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b62">63]</ref>, and non-fine-tuning needed approaches such as attention-or mask-based diffusion methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34]</ref>. Fine-tuned methods, such as In-structPix2Pix <ref type="bibr" target="#b5">[6]</ref>, require model retraining to achieve desired edits across new input domains. In contrast, attentionand mask-based diffusion models can target specific modifications without further fine-tuning. Attention-based methods, like FPE <ref type="bibr" target="#b33">[34]</ref> and P2P <ref type="bibr" target="#b24">[25]</ref>, substitute certain selfor cross-attention layers in the U-Net <ref type="bibr" target="#b51">[52]</ref>'s denoising process, leveraging the interpretability of attention maps. However, these methods may not perform well in all scenarios, particularly with real images <ref type="bibr" target="#b33">[34]</ref>. Mask-based diffusion models, such as inpainting methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b55">56]</ref>, offer strong generalization and method versatility by enabling controlled edits within specified areas while preserving unmasked regions. However, when modifying objects itself, these models may occasionally alter subtle shape details. Methods like ControlNet <ref type="bibr" target="#b60">[61]</ref> can help maintain an object's original structure during edits.</p><p>The most closely related work is VisMin <ref type="bibr" target="#b1">[2]</ref>, which generates minimal-change data to improve vision-language model comprehension. However, VisMin does not support controlled edits across our targeted three attributes. In contrast, we introduce an automatic, off-the-shelf approach enabling minimal, photorealistic edits for arbitrary combinations of real images and textual instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Task formulation.</p><p>Our goal is to analyze the impact of feasible and infeasible synthetic data (I Syn ), with feasibility defined per individual class c i , where i â 1, . . . , C. Our VariReal method generates minimal-change I Syn pairs from a shared real-image base (I Real ) using distinct textual prompts. Our approach isolates feasibility across three targeted attribute categories-background, color, and texture-while minimally altering other image content (e.g., the same dog depicted with different colors). The textual guidances are class-specific, LLM-generated prompts categorized as feasible (P f ) and infeasible (P if ). Each real image (I Real ) is combined with all prompts from both categories, ensuring every real image is repeated equally,</p><formula xml:id="formula_0">|P f | = |P if |.</formula><p>By varying the number of prompts (|P f | â¥ 1), we assess the impact of additional synthetic augmentations. Note that the texture attribute inherently includes color characteristics. Finally, we LoRA fine-tune CLIP models on in-distribution and OOD synthetic datasets to compare how each data type influences downstream classification performance. Fine-tuning with low-rank adaptation. The Low-Rank Adaptation <ref type="bibr" target="#b26">[27]</ref>  pre-trained weight matrix to reduce the number of learnable parameters. The final weights after fine-tuning could be expressed by h = W 0 x + BAx, where W 0 represents the pre-trained weights. The decomposed weights B â R dÃr and A â R rÃk , with LoRA rank r âª min(d, k). Latent diffusion models. Latent Stable Diffusion <ref type="bibr" target="#b50">[51]</ref> encodes an image into a latent space using an encoder, defined as z 0 = E(x 0 ), and learns a conditional distribution p(z|c) by predicting the Gaussian noise added to the latent vector. The objective function can be expressed as:</p><formula xml:id="formula_1">min Î¸ E (x,c)â¼D, Ïµâ¼N (0,1), t â¥Ïµ â Ïµ Î¸ (z t , c, t)â¥ 2 2 (1)</formula><p>where z t is the noisy latent representation, c is corresponding conditions and Ïµ represents the Gaussian noise added at each time step t. For the inference process, a randomly noised vector is sampled and denoised over total T steps to obtain the final latent representation z 0 , which is then decoded back into pixel space using the decoder D(z 0 ) of the VAE <ref type="bibr" target="#b29">[30]</ref>. A naive solution. Naive solutions could employ textguided Inpainting models <ref type="bibr" target="#b39">[40]</ref> (e.g., SDXL Inpainting) or Canny-edge-based ControlNet <ref type="bibr" target="#b60">[61]</ref> models (e.g., SDXL ControlNet), using a base prompt P base = "a photo of a [CLS]". Inpainting methods generate more natural images but are heavily influenced by the original attributes, limiting their effectiveness for substantial changes, as illustrated by the persistent dark hue when changing a black car's color of Figure <ref type="figure" target="#fig_1">2 (c)</ref>.</p><p>Conversely, ControlNet preserves object structure independently from original attributes in Figure <ref type="figure" target="#fig_1">2</ref> (d) but often produces less natural edits for color and texture and can cause objects to appear floating when modifying backgrounds, reducing realism. Motivation. To overcome the limitations of existing methods, we design a pipeline which overcomes the individual weaknesses of out-of-the-box methods by combining the individual strengths, i.e. combining Inpainting's realism with ControlNet's preciseness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">VariReal: Generating minimal-change data</head><p>We present a zero-shot pipeline for minimal-change image generation. Sec. 3.2.1 details prompt generation P f and P if , followed by our prior-based generation process in Sec. 3.2.2, including key steps like guidance maps and final processing. We also compare candidate models to determine the optimal modification strategy. Lastly, Sec. 3.2.3 covers MLLM-based filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Guidance prompt</head><p>P f and P if are as text prompts for Stable Diffusion model to guide desired content. To generate as many accurate P f and P if per fine-grained class as possible, we utilize ChatGPT-4 <ref type="bibr" target="#b0">[1]</ref> with In-Context Learning <ref type="bibr" target="#b11">[12]</ref>, providing the model with positive examples Example+ and negative examples Exampleâ to help avoid errors and repetitive content. To improve the fine-grained detail and realism of the generated backgrounds or textures, we instruct GPT to append a brief explanatory description when generating prompts, providing more detailed guidance for image generation.</p><p>Although large language models possess broad knowledge across various domains, ChatGPT still regularly designates attributes as 'feasible' for a target object that do not exist in the real world, particularly for fine-grained classes for which it has limited knowledge. For example, fine-grained airplane class "737-500" normally do not have color in purple. To address this issue, we design additional prompts to instruct the model to perform preliminary checks and filtering on its outputs. Manual verification ensures that feasible prompts align with the training domain. Using the same base prompt and ChatGPT-generated results, we form our final prompts shown in the Figure <ref type="figure" target="#fig_0">1</ref> and Figure <ref type="figure" target="#fig_1">2</ref>. Details of the generation process and filtered ratios are provided in the Supplementary Sec. D.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Prior-guided minimal change generation</head><p>Guidance map generation. The guided mask and canny images are for inpainting model and ControlNet respectively. We use Grounding DINO <ref type="bibr" target="#b35">[36]</ref> to generate bounding boxes bbox i , which are then fed into the SAM2 <ref type="bibr" target="#b49">[50]</ref> model to produce masks m i for each category c i . For samples without detectable bounding boxes, we use the RMBG1.4 <ref type="bibr" target="#b4">[5]</ref> foreground segmentation model as a fallback to ensure each sample has a mask.</p><p>In our method, we use the Canny image ControlNet <ref type="bibr" target="#b60">[61]</ref> model. For all settings, the Canny image is created by extracting the foreground Foreground i from mask i . Prior generation and process. We use prompts P f or P if with Stable Diffusion to generate "Background" and "Texture" Priors, and predefined RGB values from a Color Bank for the "Color Prior". These initial outputs are termed Raw Priors.</p><p>To integrate these priors with real images, we merge the original object's region with the Background Prior, applying mask dilation to preserve spatial context and realism (e.g., ensuring pets remain grounded). We ablate this operation effect in the Supplementary Sec. H. For color and texture edits, the generated prior is overlaid via an alpha channel to retain the original shape and details of the subject. These refined results are referred to as Real Priors. The Figure <ref type="figure" target="#fig_1">2 (a-b</ref>) illustrate these Priors. ControlNet leverages both Raw and Real Priors as conditions via IP-Adaptor <ref type="bibr" target="#b57">[58]</ref>, whereas Inpainting exclusively employs Real Prior to retain unchanged original information. Final process.</p><p>Before outputting the final images, the last step involves copying invariant regions from the original image and pasting them onto the generated image, ensuring minimal alterations. Minimal change for background. Figure <ref type="figure" target="#fig_1">2</ref> (e) demonstrates that incorporating prior information significantly enhances background editing quality, fulfilling our minimalchange requirement. Our optimal results are obtained using Inpainting with the Real Prior, a background-region mask, and the corresponding prompt P shown in Figure <ref type="figure">3</ref>. Minimal change for foreground. In contrast, color and texture edits require foreground modifications. As shown in Figure <ref type="figure" target="#fig_1">2</ref> (e-g), single-stage Inpainting and ControlNet models are insufficient under either Raw or Real Priors: Inpainting may distort object shapes, while ControlNet can produce unnatural results. To address this, we first produce an initial refined image using SDXL Inpainting, then use it as a conditional input for ControlNet to generate the final image. This combined approach (Figure <ref type="figure">3</ref>) leverages the strengths of both methods, preserving the object's shape while achieving natural and precise color or texture changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Automatic filtering</head><p>To ensure generated images meet prompt requirements, the MLLM Llava-Next <ref type="bibr" target="#b32">[33]</ref> model checks each image's feasibility and attributes. Using predefined questions, we filter out images that do not match the specified background, color, or texture. More details and example about the filtering questions can be found in the Supplementary Sec. D.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feasibility effectiveness validation</head><p>Following the common practice <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref> to evaluate the impact of data feasibility, we fine-tune a CLIP <ref type="bibr" target="#b46">[47]</ref> classifier, which encodes images and corresponding text prompts to calculate similarity scores for classification. We apply LoRA <ref type="bibr" target="#b26">[27]</ref> modules to fine-tune both CLIP's image and text encoders. For each class c i â C, we use the prompt "a photo of [CLS]" as text input. Training is performed via supervised learning using cross-entropy loss, updating only the LoRA modules while keeping pretrained weights frozen.</p><p>In mixed training scenarios (real and synthetic data), the loss is a weighted combination defined as:</p><formula xml:id="formula_2">L C = Î» â¢ CE(Real) + (1 â Î») â¢ CE(Synth)<label>(2)</label></formula><p>where Î» balances the contribution from real data, and CE denotes cross-entropy loss. Our synthetic data for background, color, and texture modifications require images with clearly defined foreground objects and visible backgrounds; hence, datasets dominated by foreground-only images, such as ImageNet <ref type="bibr" target="#b9">[10]</ref>, are unsuitable. Fine-grained datasets offer clearer comparisons between feasible and infeasible attribute variations. Therefore, we generate our minimalchange synthetic datasets from three fine-grained sources: Oxford Pets <ref type="bibr" target="#b45">[46]</ref>, FGVC Aircraft <ref type="bibr" target="#b40">[41]</ref>, and Stanford Cars <ref type="bibr" target="#b31">[32]</ref>. Additionally, to specifically evaluate background modifications, we use the binary classification WaterBirds dataset <ref type="bibr" target="#b13">[14]</ref>, which pairs landbirds and waterbirds with water or land backgrounds. Implementation details. Our VariReal pipeline utilizes SDXL Inpainting v0.1 and SDXL ControlNet v1.0 <ref type="bibr" target="#b60">[61]</ref> based on Canny-edge conditioning, along with Stable Diffusion v2.1 <ref type="bibr" target="#b50">[51]</ref> for prior image generation in background and texture modifications. The Llava-1.6-7B <ref type="bibr" target="#b34">[35]</ref> model is employed for automatic filtering. Real images used for modification are sourced from the training split of each dataset, and performance is evaluated on the original test R S Pets <ref type="bibr" target="#b45">[46]</ref> AirC <ref type="bibr" target="#b40">[41]</ref> Cars <ref type="bibr" target="#b31">[32]</ref> Average We fine-tune a CLIP ViT-B/16 <ref type="bibr" target="#b12">[13]</ref> classifier using LoRA modules with the rank of 16 applied to both image and text encoders, optimized with AdamW <ref type="bibr" target="#b37">[38]</ref>. The scale factor Î» is set to 0.5 to equally weight real and synthetic cross-entropy losses. To ensure fair training budget despite varying dataset sizes (real-only, synthetic-only, and mixed synth+real training), we fix the total maximum training iterations to ensure same optimizer update steps. Detailed generation and training hyperparameters are provided in the Supplementary Sec. B. Baseline methods. To evaluate the impact of feasible versus infeasible synthetic data, we use zero-shot CLIP and CLIP fine-tuned on real images as baselines. We compare these baselines with CLIP trained exclusively on synthetic data and on combinations of synthetic and real data. Evaluation protocol. We measure classification performance using top-1 accuracy (%). For dataset distribution analysis in Sec. 4.3.2, we report FID <ref type="bibr" target="#b41">[42]</ref>, CLIP score <ref type="bibr" target="#b46">[47]</ref>, DINO score <ref type="bibr" target="#b44">[45]</ref>, and LPIPS <ref type="bibr" target="#b61">[62]</ref> scores. More details on those metrics are described in Sec. 4.3.2. As shown in the second-to-last column, 4 out of 6 cases yield positive â 1 , suggesting that feasible data generally performs slightly better than infeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><formula xml:id="formula_3">F IF Mix â 1 â 2 F IF Mix â 1 â 2 F IF Mix â 1 â 2 F IF Mix â 1 â 2 0-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Classification with minimally changed data</head><p>Under the synthetic-only setting, 56% of â 1 values (5/9) fall within 0.3% across each dataset column. Specifically, in the AirC <ref type="bibr" target="#b40">[41]</ref> dataset, feasible data outperforms infeasible by 1.8% under the background setting, while infeasible data performs better by 0.8% and 0.3% under the color and texture settings, respectively. After incorporating real data (real + synthetic), 78% of â 1 values (5/9) remain within 0.3%, indicating that the performance gap between feasible and infeasible data is consistently small across settings.</p><p>Observation 1: Although feasible images perform slightly better, feasibility shows no clear impact on classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">The role of attribute</head><p>Although all settings in Table <ref type="table" target="#tab_1">1</ref> outperform the zero-shot baseline, synthetic color and texture data remain less effective compared to the real data. For instance, in the synthetic-only setting, feasible and infeasible color data achieve 89.0% and 89.1% average performance, both below the real-only fine-tuning baseline of 90.8%. Even when combined with real data, color edits perform slightly worse by 0.1% and 0.2%.</p><p>In contrast, background modifications consistently improve performance. For instance, under synthetic-only training, feasible and infeasible backgrounds yield average accuracy gains of 1.2% and 0.6%, respectively, and 1.6% and 1.7% in the real + synthetic setting.</p><p>We further validate the benefits of background modifications on the WaterBirds <ref type="bibr" target="#b13">[14]</ref> dataset (see Supplementary Sec. E). Both feasible and infeasible background edits outperform real-only setting, with improvements of 0.9% and 6.7% respectively in the synthetic-only setting, and 7.2% and 8.8% in the real + synthetic setting.</p><p>Observation 2: Compared to fine-tuning on real data alone, adding synthetic data with background modifications improves performance, whereas synthetic foreground edits (color and texture) are less effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">The role of mixed training</head><p>To assess the effect of mixing feasible and infeasible data, we construct a balanced synthetic dataset (third column of each subtable in Table <ref type="table" target="#tab_1">1</ref>), with a total size five times that of the real training set. We define the metric</p><formula xml:id="formula_4">â 2 = Mix â F +IF 2</formula><p>to measure the performance gain from mixing, relative to the average of using feasible and infeasible data separately.</p><p>In the real + synthetic setting, mixed training yields comparable performance, with average â 2 deviations within 0.2%. In contrast, under the synthetic-only setting, mixing leads to greater gains-0.4%, 0.2%, and 0.7% improvements on average for background, color, and texture edits-indicating stronger complementarity between feasible and infeasible data. Further analysis for this is provided in Supplementary Sec. F. This suggests that, unlike ALIA <ref type="bibr" target="#b13">[14]</ref>, modifications need not be strictly feasible.</p><p>Observation 3: It is not necessary to strictly generate only feasible synthetic images to achieve performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis of minimally changed data 4.3.1. Qualitative results</head><p>To assess the quality of VariReal-generated images, Figure <ref type="figure" target="#fig_4">4</ref> presents qualitative examples from all three datasets. These examples demonstrate that the edits follow the text prompts with minimal changes and align with our feasibility definition-existing in real world. For instance, "neon pink" is not a released color for the "Audi RS 4 Convertible 2008" and is thus treated as infeasible. The images show the expected modification, while the rest of the image remains unchanged from the real source. More examples are provided in Supplementary Sec. G.</p><p>To further validate image quality, we conducted a user study with six human annotators using a questionnaire. Evaluators assessed each image on two aspects: (1) feasibility-whether feasible images appear realistic and infeasible ones do not-and (2) naturalness, rated on a 1-5 scale, where 5 indicates the most natural appearance. More de-tails about the scoring setup are included in Supplementary Sec. G.</p><p>Feasibility is central to our pipeline, ensuring a clear distinction between feasible and infeasible subsets. As shown in Table <ref type="table">2</ref>, feasibility correctness is high, with error rates below 8% for feasible and 16% for infeasible data. The slightly lower accuracy for infeasible cases stems from occasional mismatched background-object combinations and difficulty capturing fine-grained texture details-particularly in the AirC dataset, as noted in annotator feedback (see Supplementary Sec. G). These results support the effectiveness of our approach, with VariReal reliably generating high-quality edits, further refined by automatic filtering (Sec. 3.2.3).</p><p>Regarding how natural the generated images are, VariReal images received acceptable naturalness scores from human annotators-averaging 3.94 for feasible and 3.96 for infeasible data. For failure cases, some generated images appear less natural (see Supplementary Sec. G) because of a dramatic change from the original color to a new color, such as red to white. Table <ref type="table">2</ref>. Human evaluation of the generated dataset based on feasibility correctness and naturalness scores, validating its suitability for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Distribution analysis</head><p>We analyze the dataset using several similarity metrics to better understand the distributional differences between feasible and infeasible data and their relation to in-and out-ofdistribution. We compute the FrÃ©chet Inception Distance (FID) <ref type="bibr" target="#b41">[42]</ref> to quantify the distributional similarity between generated and real data. Additionally, we use: CLIP Score: calculated cosine similarity for feature from the ViT-L/14 model <ref type="bibr" target="#b12">[13]</ref>. DINO Score: computed cosine similarity for feature from the DINOv2-Base model <ref type="bibr" target="#b44">[45]</ref> for feature extraction. And LPIPS Score <ref type="bibr" target="#b61">[62]</ref>.</p><p>Figure <ref type="figure" target="#fig_5">5</ref> shows that feasible samples generally resemble in-distribution data more closely than infeasible ones, aligning better with the real data distribution. This observation is supported by the metrics in Table <ref type="table" target="#tab_3">3</ref>, which reports average scores across the three datasets. All three metrics indicate that feasible data is closer to real data. While CLIP and DINO scores show limited sensitivity to fine-grained differences, LPIPS captures subtle variations more effectively.</p><p>Interestingly, both feasible and infeasible foreground modifications (color and texture) are closer to real data than background edits. For instance, in the AirC <ref type="bibr" target="#b40">[41]</ref> dataset, FID peak scores for foreground edits are much lower (around 95 and 110 for feasible/infeasible) than for background edits (around 170 and 220). Table <ref type="table" target="#tab_3">3</ref> shows similar trends-for instance, the average DINO score for color is about 10% higher than for background. However, as discussed in Sec. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Scaling the number of training images</head><p>To further understand the impact of synthetic data by VariReal, we conducted a scaling analysis on the AirC dataset <ref type="bibr" target="#b40">[41]</ref>, adjusting feasible/infeasible synthetic-to-real ratios from 1:1 to 5:1. Our results in Figure <ref type="figure">6</ref> reveal a nonlinear relationship between performance and data scale. While background modifications always benefit the downstream tasks, color and texture modifications achieve peak accuracy at smaller scales. Notably, performance slightly exceeds the baseline at these peaks but declines as more synthetic images are Figure <ref type="figure">6</ref>.</p><p>The scaling experiment results for the FGVC-Aircraft <ref type="bibr" target="#b40">[41]</ref> dataset are shown for background, color, and texture settings. The horizontal axis represents the scale factor for synthetic images relative to real images. Here, the total real image training set is used, with scale factors ranging from 1 to 5.</p><p>added. This indicates that both feasible and infeasible color and texture data behave similar to OOD data, while feasible data being relatively closer to the real distribution. Large-scale use of such data does not provide meaningful in-distribution information for downstream tasks. However, a limited amount can serve as effective augmentation, enhancing model performance and robustness.</p><p>Observation 5: Synthetic data with color and texture modifications can enhance classification performance as augmentation, but their effectiveness is limited to specific scaling ranges. In contrast, background modifications consistently yield performance gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we present VariReal, a pipeline for systematically investigating the impact of minimal-change feasible and infeasible synthetic data. By introducing controlled variations in background, color, and texture across three fine-grained datasets, we assess the role of feasibility through LoRA-based fine-tuning of a CLIP classifier. Our findings reveal a counter-intuitive result: feasibility does not significantly affect classification performance. Although typically assumed to benefit downstream tasks, feasible synthetic variations in color and texture are no more effective than real data-and in some cases, even degrade performance. In contrast, background modifications consistently improve accuracy, regardless of feasibility. This suggests that, for object-centric classification, altering foreground attributes may disrupt class-relevant signals and yield limited gains. Overall, our results underscore the nuanced effects of different attribute modifications and offer new insights for designing effective synthetic data generation strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this supplementary material, we first discuss the broader impacts and limitations of our analysis in Sec. A. Experimental setups for our method are provided in Sec. B, and configurations for other image editing models are detailed in Sec. C. Sec. D describes our method in detail, including guidance prompts and automatic filtering. Additionally, we present background-specific classification results on the WaterBird <ref type="bibr" target="#b13">[14]</ref> dataset in Sec. E. Further classification result analysis is provided in Sec. F, followed by additional qualitative examples and user study details in Sec. G. Finally, an ablation study of the VariReal pipeline is included in Sec. H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Broader Impact and Limitation</head><p>Our VariReal pipeline focuses on generating feasible and infeasible image pairs for downstream tasks, with potential applications beyond classification. It offers a robust method for modifying backgrounds, colors, and textures in both prompts and real images, making it suitable for image editing tasks that require precise changes while preserving other regions. VariReal can also serve as a dataset generation tool to fine-tune Stable Diffusion models for text-guided image editing, enabling targeted modifications. Additionally, it supports data augmentation, showing that augmenting both feasible and infeasible backgrounds improves classification performance-unlike ALIA <ref type="bibr" target="#b13">[14]</ref>, which only uses feasible backgrounds.</p><p>We define feasibility as alignment with real-world plausibility. For instance, feasible car colors are those officially released by manufacturers. Rare custom paint jobs-such as a "cyan" Audi RS 4 Convertible 2008-are excluded, as they do not reflect typical production offerings. Within our scope, such extreme cases are treated as infeasible settings.</p><p>Our approach targets datasets with clear foregroundbackground separation and focuses on classification tasks under minimal-change settings. Although we strive to preserve structure, slight deviations-particularly in color and texture edits-are sometimes unavoidable due to current image editing limitations. In the meantime, our method requires adjusting hyperparameters when modifying images to meet specific requirements. We believe advances in image editing techniques will make our experimental setup more effective and easier to implement. Due to resource constraints, we explored only three attributes (background, color, texture), but future work could extend to others, such as lighting. Developing a unified method for minimal, single-step edits across multiple attributes would en-hance scalability and enable broader application to diverse datasets and tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We provide additional implementation details for VariReal in Table <ref type="table" target="#tab_5">5</ref>. Key parameters include noise strength for the SDXL Inpainting model <ref type="bibr" target="#b38">[39]</ref> and conditioning strength for IP-Adapter <ref type="bibr" target="#b57">[58]</ref> with ControlNet <ref type="bibr" target="#b60">[61]</ref>. Due to varying difficulty across datasets and between feasible and infeasible generation, we use dataset-specific settings.</p><p>Following DataDream <ref type="bibr" target="#b27">[28]</ref>, we tune learning rates and weight decay for classification tasks. We use a batch size of 64, AdamW <ref type="bibr" target="#b37">[38]</ref> optimizer, and a cosine annealing scheduler. Table <ref type="table" target="#tab_6">6</ref> lists the CLIP <ref type="bibr" target="#b46">[47]</ref> fine-tuning parameters. Learning rates and weight decay are selected from a predefined range based on validation performance. The number of training iterations is fixed as described in Sec. 4.1, with dataset-specific counts provided in the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Other Image Editing Method Setups</head><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, we compare VariReal with In-structPix2Pix <ref type="bibr" target="#b5">[6]</ref> and FPE <ref type="bibr" target="#b33">[34]</ref>. To ensure fairness and leverage each model's strengths, we follow their original usage guidelines. For FPE, we maintain aspect ratio via resizing and padding, and use the original training setup with recommended prompts-e.g., "a [CLS] in the [ATTRIBUTE] background" for background changes and "a [ATTRIBUTE] [CLS]" for color or texture edits, where [CLS] denotes the class name and [ATTRIBUTE] refers to feasible or infeasible prompts from Sec. 3.2.1. InstructPix2Pix uses prompts like "put it in [ATTRIBUTE] background" for background changes and "make it a [ATTRIBUTE] aircraft" for foreground edits. We conducted multiple trials and selected the best outputs for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Method Details D.1. Guidance prompt</head><p>As detailed in Sec. 3.2.1 and shown in Figure <ref type="figure" target="#fig_6">7</ref>, the prompt generation process includes initial prompt generation and preliminary checks.</p><p>Specifically, we use GPT-4 <ref type="bibr" target="#b0">[1]</ref> to generate feasible or infeasible initial attributes (prompt words), which are then combined into a final prompt using our template: "a photo of a [CLS]", as shown in Figure <ref type="figure" target="#fig_6">7</ref>. These initial attributes are then preliminarily checked by:  For example, "deep cave" is not a feasible background for the pets class in the initial generation results and is filtered out by GPT-4. To ensure feasible attributes align with the training set, we manually check the existing backgrounds, colors, and textures in the training data and remove those absent from it. Table <ref type="table" target="#tab_1">1</ref> shows the acceptance ratio at each stage.</p><p>An example of generated attributes is the following, where the placeholders [ATTRIBUTE] represents the feasible/infeasible background, color, or texture, and [CLASS] represents a specific class. Here we also give one specific example for generating feasible and infeasible background for Oxford Pets dataset <ref type="bibr" target="#b45">[46]</ref> after replacing the placeholders in the above template in Figure <ref type="figure">8</ref>. By using the prompts described above, we also select some generated attributes (prompt words) to replace the placeholder in the prompt template. Due to space limitations, we provide up to five attributes as an example for the Oxford Pets <ref type="bibr" target="#b45">[46]</ref> dataset. Some generated feasible and infeasible prompt words can be found in Figure <ref type="figure">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Automatic filtering</head><p>As introduced in Sec. 3.2.3, we present the filtering questions for background, color, and texture changes. These checks ensure that the generated attributes align with the text prompt. For background attributes, we also verify if the foreground objects are feasible within the given background. Using placeholders for each background, color, texture prompt, object class, and feasibility information, we formulate questions based on the following filtering question template.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background-related questions:</head><p>â¢ If we change the color and texture, we use the following questions:</p><p>Color and Texture-related questions:</p><p>â¢ We show an example process for the automatic filtering in the Figure <ref type="figure" target="#fig_7">14</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. WaterBird Experiment Details</head><p>In this section, we present detailed experimental results for the WaterBird <ref type="bibr" target="#b13">[14]</ref> dataset under background modification settings, as shown in Table <ref type="table" target="#tab_9">7</ref>. Notably, infeasible background edits improve performance by 5.8 percentage points in the synthetic-only setting and 1.6 percentage points in the real + synthetic setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Classification Results Analysis</head><p>In Sec. 4.2.1, we analyze mixing the feasible and infeasible data has no clear impact on classification tasks but some times will help the model learn complementary knowledge. We evaluate prediction correctness per test sample to compare knowledge learned by models trained under different settings. To measure whether one model's correctly predicted set is a subset of another's, we use: Inclusion Coefficient = |Aâ©B| |A| , with values closer to 1 indicating greater overlap. Additionally, we quantify the overlap of correctly predicted samples between models using the Jaccard index:J(A, B) = |Aâ©B| |AâªB| , where A and B, where A and B represent correct predictions from two training configurations.</p><p>The Inclusion matrix in Figure <ref type="figure" target="#fig_7">15</ref> shows no subset relationship exists between model predictions.Notably, the feasible-only and infeasible-only settings labeled with dashed lines yield the lowest Jaccard scores, indicating minimal similarity.</p><p>Observation: The feasible and infeasible data lead the model to learn different directions, while they achieve very similar performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Qualitative Examples and User Study</head><p>We provide additional qualitative examples to demonstrate the generation quality of our VariReal method. One additional example from the Oxford Pets <ref type="bibr" target="#b45">[46]</ref>, FGVC Aircraft <ref type="bibr" target="#b40">[41]</ref>, and Stanford Cars <ref type="bibr" target="#b31">[32]</ref> datasets is included, along with one randomly selected example across these datasets.</p><p>Figure <ref type="figure" target="#fig_7">11</ref> shows the Abyssinian pet generation results, where our VariReal method produces more detailed backgrounds, such as "active war zone." Figure <ref type="figure" target="#fig_7">12</ref> presents a Spitfire aircraft sample, illustrating snow in the background "arctic tundra landing strip." Figure <ref type="figure" target="#fig_9">13</ref> features a BMW X3 SUV 2012 example. Finally, Figure <ref type="figure" target="#fig_11">16</ref> provides randomly selected examples from the three datasets for further visualization. The instruction for the questionnaire is shown in Figure <ref type="figure" target="#fig_7">17</ref>.</p><p>Figure <ref type="figure" target="#fig_7">18</ref> presents examples of correctly and incorrectly classified feasibility cases. More detail can be seen by zooming into the figures. For infeasible texture modifications, failure cases often like infeasible texture change of fish scale or brick wall, which are fine-grained and hard to represent clearly. In such cases, the output may only reflect the color rather than the intended texture, so human evaluators will classify these to the incorrect cases. Another source of error involves implausible object-background combinations-for example, a "flying aircraft in an airplane hangar" shown in the lower part of Figure <ref type="figure" target="#fig_7">18</ref>.</p><p>For the naturalness criterion, some images-such as those in Figure <ref type="figure" target="#fig_7">19</ref> where the feasible color is changed from red to gray or white-receive lower scores, as the resulting colors appear less natural.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Ablation Study</head><p>We ablate the mask dilation step introduced in Sec. 3.2.2, which helps maintain spatial coherence between objects and backgrounds. Without mask dilation, generated images often exhibit a "floating" effect shown in Figure <ref type="figure" target="#fig_1">20</ref>, where objects appear unnaturally integrated into their backgrounds.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatic Filtering</head><p>Is the woven basket texture feasible for cars to appear in the real world? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>Color Texture In this questionnaire, you will be shown an image and instructions that specify some edits to be made to the image (we call this "edit instruction"). You will also be shown the edited image. Your task is to evaluate the edited image's correctness/feasibility/naturalness.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. We propose VariReal, a pipeline for minimal-change editing of real images, enabling isolation of target attributes in three categories: background, color, and texture. We compare images generated by VariReal to those produced by prior text-guided editing methods<ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34]</ref>, examining both feasible and infeasible attributes. The editing prompts are provided below each generated image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. We compare images generated by various candidate methods: Inpainting model [39] alone, ControlNet [61] alone, Inpainting model with Real Prior, ControlNet with Raw Prior or Real Prior, and our final results for three attribute modifications. The first two columns illustrate the priors used (Raw Prior and Real Prior), and generation prompts used are listed beneath each image.</figDesc><graphic coords="3,58.50,132.65,69.04,51.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4. 1 .</head><label>1</label><figDesc>Experiments setupDataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 2 . 1 .</head><label>21</label><figDesc>The role of feasibility Table 1 compares model performance across four training settings: (1) two baselines, (2) synthetic-only, and (3) real + synthetic training. To assess the role of feasibility, we define the metric â 1 = F â IF , where F and IF denote performance using feasible and infeasible data, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Selected generation results from the three datasets. Only target prompt keywords are shown; detailed background and texture descriptions are omitted. Please zoom in for visual details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The FID score settings compared using feasible and infeasible settings across different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The generated attributes(prompt words) and self-filtering process using ChatGPT-4 [1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Question 1 :</head><label>1</label><figDesc>Is the object in the image located in the [BACKGROUND] environment? Choices: ['yes', 'no'] Answer: 'yes' â¢ Question 2: Does the image background represent [BACKGROUND]? Choices: ['yes', 'no'] Answer: 'yes' â¢ Question 3: Does the [BACKGROUND] look feasible for the [CLS]? Choices: ['yes', 'no'] Answer: 'yes' if [FEASIBLE] else 'no' â¢ Question 4: Is it possible for the [CLS] in this image to exist in the real world with its background? Choices: ['yes', 'no'] Answer: 'yes' if [FEASIBLE] else 'no' Note: The placeholder [CLS] represents the current class name, [BACKGROUND] represents the target background being generated, and [FEASIBLE] denotes its feasibility.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .Figure 11 .Figure 12 .</head><label>101112</label><figDesc>Figure10. The automatic filtering process using a MLLM model to filter the generated images using pre-defined qustions to check certain aspect for the generated image and ground truth answers.</figDesc><graphic coords="18,106.75,133.48,395.97,188.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. TQualitative results of the class BMW X3 SUV 2012 from Stanford Cars dataset [32], as a supplement for Figure 4.</figDesc><graphic coords="19,399.59,596.02,67.96,50.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 .Figure 15 .</head><label>1415</label><figDesc>Figure 14. Example of automatic texture filtering on the Cars [32] dataset.</figDesc><graphic coords="19,470.54,596.02,67.96,50.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 16 .</head><label>16</label><figDesc>Figure 16. Randomly selected generated samples across three datasets and feasibility attributes are shown. For visualization purposes, all images are resized to the same dimensions.</figDesc><graphic coords="21,385.09,586.22,69.65,69.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>â¢ 5 Example 2 ( 4 Example 3 (</head><label>5243</label><figDesc>The edited image is judged to be feasible if attributes assigned to an object in the synthetic image could realistically exist in the real-domain with high probability; On the contrary, it is infeasible. â¢ Please rate the naturalness of the image subjectively, with 1 being the lowest score and 5 being the most natural. Please see the examples below to understand the task better: (Left: original; Right: edited) Example 1(Back): Is the edited image feasible? â YES â No Please rate the naturalness of the image: 4.Color): Is the edited image feasible? â YES â No Please rate the naturalness of the image: Texture): Is the edited image feasible? â YES â No Please rate the naturalness of the image: 4Please answer the questions below to the best of your knowledge. Thank you for your careful attention to detail and your valuable contribution!</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 17 .Figure 18 .Figure 19 .</head><label>171819</label><figDesc>Figure 17. Instructions for feasibility and naturalness generated images human study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>introduces low-rank decomposition into the "a photo of [CLS] on/with "737-500" lava rock texture: "â¦Descriptionsâ¦" hot pink color the train tunnel: "â¦Descriptionsâ¦" ControlNet Back Color Texture Guidance Map Gen. Prior Gen. &amp; Proc. Diffusion Inpainting Real image Mask Canny image Real Prior Refined images Pf / Pif Figure</head><label></label><figDesc>3. An overview of VariReal pipeline. Minimal-change steps for background, color, and texture are highlighted in green, pink, and grey, respectively. Real images are processed to generate guidance maps (e.g., masks, Canny edges) for Inpainting and ControlNet. GPT-4 generates feasible and infeasible prompts (P f and P if ), which guide color retrieval or prior image generation via Stable Diffusion. These Real Priors, combined with masks and prompts, are input to the inpainting model. For color and texture, ControlNet with Canny conditioning ensures precise foreground shapes. A final refinement step produces the optimal output for each setting.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Top-1 performance using the full training set and synthetic images generated by VariReal, including baseline, synthetic-only and synth + real. The number of synthetic images is set to five times the number of real images across all experiments. R/S indicates real/synthetic fine-tuning. F/IF denotes feasible/infeasible inputs, Mix indicates training with both. â1 = F âIF , and â2 = M ixâ F +IF 2 measures the gain/loss of mixing compared to the average of individual setting. set. We use |P f | = |P if | = 5 prompts per class, thus generating five synthetic images per real-image base.</figDesc><table><row><cell>shot</cell><cell></cell><cell>-91.0 -</cell><cell>-23.8 -</cell><cell>-63.2 -</cell><cell>-59.3 -</cell></row><row><cell>Real</cell><cell>â</cell><cell>-95.2 -</cell><cell>-84.5 -</cell><cell>-92.6 -</cell><cell>-90.8 -</cell></row><row><cell>Back.</cell><cell></cell><cell cols="4">â 95.4 95.3 95.2 +0.1 -0.2 86.8 85.0 87.1 +1.8 +1.2 93.7 93.8 93.8 -0.1 +0.1 92.0 91.4 92.0 +0.6 +0.4</cell></row><row><cell>Color</cell><cell></cell><cell cols="4">â 94.5 94.4 94.1 +0.1 -0.4 80.8 81.6 81.9 -0.8 +0.7 91.6 91.5 91.6 +0.1 +0.1 89.0 89.1 89.2 -0.1 +0.2</cell></row><row><cell>Text.</cell><cell></cell><cell cols="4">â 93.8 93.3 92.8 +0.5 -0.8 81.6 81.9 82.0 -0.3 +0.3 90.9 87.7 91.8 +3.2 +3.0 88.8 87.6 88.9 +0.2 +0.7</cell></row><row><cell>Back.</cell><cell cols="5">â â 95.3 95.3 95.3 +0.0 +0.0 88.0 88.4 88.6 -0.4 +0.4 93.8 93.7 93.6 +0.1 -0.2 92.4 92.5 92.5 -0.1 +0.1</cell></row><row><cell>Color</cell><cell cols="5">â â 95.3 95.2 95.0 +0.1 -0.3 84.6 84.0 83.6 +0.6 -0.7 92.7 92.5 92.8 +0.2 +0.2 90.9 90.5 90.4 +0.4 -0.2</cell></row><row><cell>Text.</cell><cell cols="5">â â 95.3 95.2 95.2 +0.1 -0.1 83.9 83.8 83.8 +0.1 -0.1 93.0 92.8 92.6 +0.2 -0.3 90.7 90.6 90.5 +0.1 -0.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>The average DINO, CLIP and LPIPS scores calculated between generated synthetic image and corresponding real images for three datasets. F/IF denotes feasible/infeasible inputs.</figDesc><table><row><cell>Settings</cell><cell cols="4">Inputs CLIP (â) DINO (â) LPIPS (â)</cell></row><row><cell>Background</cell><cell>F IF</cell><cell>0.914 0.886</cell><cell>0.861 0.830</cell><cell>0.447 0.477</cell></row><row><cell>Color</cell><cell>F IF</cell><cell>0.951 0.904</cell><cell>0.956 0.939</cell><cell>0.189 0.254</cell></row><row><cell>Texture</cell><cell>F IF</cell><cell>0.936 0.898</cell><cell>0.949 0.925</cell><cell>0.207 0.218</cell></row></table><note>4.2.2, only background modifications consistently improve classification performance. This highlights the following: Observation 4: Classification tasks are object-centric: although foreground (color and texture) modifications align more closely with real data distributions, changing them may deviate from meaningful class-relevant features, leading to weaker effects.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>The number of prompts which are generated initially by LLM, after self-filtering and manual-filtering for each specific settings and some datasets. The Pets, AirC, Cars refer to our experimental dataset introduced in 4.1.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Background</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Color(Per CLS)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Texture</cell><cell></cell></row><row><cell></cell><cell>Pets</cell><cell></cell><cell cols="2">AirC</cell><cell>Cars</cell><cell></cell><cell>Pets</cell><cell></cell><cell>AirC</cell><cell></cell><cell cols="2">Cars</cell><cell></cell><cell cols="2">Pets(Per CLS)</cell><cell cols="2">AirC</cell><cell></cell><cell>Cars</cell></row><row><cell></cell><cell>F</cell><cell>IF</cell><cell>F</cell><cell>IF</cell><cell>F</cell><cell>IF</cell><cell>F</cell><cell>IF</cell><cell>F</cell><cell>IF</cell><cell>F</cell><cell>IF</cell><cell></cell><cell>F</cell><cell>IF</cell><cell>F</cell><cell>IF</cell><cell>F</cell><cell>IF</cell></row><row><cell>Raw output</cell><cell>50</cell><cell>70</cell><cell>50</cell><cell>70</cell><cell>50</cell><cell>70</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell></cell><cell>8</cell><cell>50</cell><cell>30</cell><cell>50</cell><cell cols="2">15</cell><cell>70</cell></row><row><cell>Auto-filtering</cell><cell>47</cell><cell>64</cell><cell>36</cell><cell>68</cell><cell>44</cell><cell>67</cell><cell cols="2">6â¼7 8â¼9</cell><cell>7â¼8</cell><cell>8â¼9</cell><cell cols="2">7â¼8 8â¼10</cell><cell></cell><cell>7</cell><cell>42</cell><cell>25</cell><cell>46</cell><cell cols="2">12</cell><cell>64</cell></row><row><cell>Manual-filtering</cell><cell>43</cell><cell>50</cell><cell>22</cell><cell>50</cell><cell>31</cell><cell>50</cell><cell>5</cell><cell>5</cell><cell>5â¼8</cell><cell>5â¼6</cell><cell>5</cell><cell>5</cell><cell></cell><cell>5</cell><cell>27</cell><cell>24</cell><cell>44</cell><cell>7</cell><cell>57</cell></row><row><cell cols="7">Final Accept Rate 0.86 0.714286 0.44 0.71429 0.62 0.71429</cell><cell>0.5</cell><cell cols="4">0.5 0.5â¼0.8 0.5â¼0.8 0.5</cell><cell>0.5</cell><cell></cell><cell cols="6">0.625 0.54 0.8 0.88 0.467 0.814</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Back.</cell><cell></cell><cell></cell><cell></cell><cell>Color</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Texture</cell><cell></cell></row><row><cell></cell><cell cols="2">Parameters</cell><cell></cell><cell></cell><cell cols="8">Pets F IF F IF F IF F IF F IF F IF AirC Cars Pets AirC Cars</cell><cell>F</cell><cell>Pets IF</cell><cell>F</cell><cell>AirC</cell><cell>IF</cell><cell>F</cell><cell>Cars</cell><cell>IF</cell></row><row><cell cols="5">Guidance Scale for SDXL Inpainting [39]</cell><cell>40</cell><cell>7.5</cell><cell cols="2">7.5</cell><cell>12</cell><cell>12</cell><cell>30</cell><cell></cell><cell></cell><cell>12</cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell>30</cell></row><row><cell cols="4">Guidance Scale for ControNet [61]</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell>7.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">7.5</cell><cell></cell></row><row><cell cols="3">Strength for SDXL</cell><cell></cell><cell></cell><cell>0.99</cell><cell>0.95</cell><cell cols="2">0.9</cell><cell>0.3</cell><cell>0.8</cell><cell>0.85</cell><cell></cell><cell cols="7">0.3 0.3 0.65 0.3 0.65 0.3</cell></row><row><cell cols="4">IP-Adptor [58] Strength</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>0.7</cell><cell>0.4</cell><cell>0.4</cell><cell></cell><cell cols="7">0.2 0.5 0.65 0.4 0.65 0.4</cell></row><row><cell cols="3">Inference Step for SD</cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>15</cell><cell></cell><cell></cell></row><row><cell cols="4">Inference Step for SDXL Inpainting</cell><cell></cell><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell></row><row><cell cols="4">Inference Step for ControlNet</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell></cell><cell></cell></row><row><cell cols="4">Mask dilated factor/alpha factor</cell><cell></cell><cell>120</cell><cell>50</cell><cell cols="2">25</cell><cell>0.3</cell><cell>0.6</cell><cell>0.6</cell><cell></cell><cell cols="7">0.5 0.4 0.5 0.65 0.65 0.65</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>The detailed generation parameters for VariReal. We introduce the parameters for feasible and infeasible settings of three dataset respectively.</figDesc><table><row><cell>HyperParameters</cell><cell>lamda</cell><cell>lr</cell><cell>Min lr</cell><cell>Weight decay</cell><cell>Warm up steps</cell><cell>CLIP LoRA rank</cell><cell>CLIP LoRA alpha</cell></row><row><cell>Values</cell><cell>0.5</cell><cell>{1e-3,5e-4,1e-4,5e-5,1e-5}</cell><cell>1e-08</cell><cell>1e-3, 1e-4, 5e-5</cell><cell>5% total iterations</cell><cell>16</cell><cell>32</cell></row><row><cell cols="2">HyperParameters Training bs</cell><cell>Test bs</cell><cell>Train iterations</cell><cell>Val iterations</cell><cell></cell><cell>Data augmentation</cell><cell></cell></row><row><cell>Values</cell><cell>64</cell><cell>8</cell><cell cols="2">Pets:20700/AirC:72000/Cars:91840 1/70 Train iterations</cell><cell cols="3">random resized crop, random horizontal flip, random color jitter, and random gray scale</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>The hyper-parameter details for CLIP<ref type="bibr" target="#b46">[47]</ref> model fine-tuning.</figDesc><table><row><cell>"Can you modify or filter your</cell></row><row><cell>answers to ensure each</cell></row><row><cell>[background/color/texture] is</cell></row><row><cell>definitely [feasible/infeasible]</cell></row><row><cell>for class [CLASS]? Please delete</cell></row><row><cell>and ignore some of the answers</cell></row><row><cell>if you can't guarantee them."</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>The top-1 performance using the full training set and synthetic data, with training setups including synthetic-only and synth. + real data. The attribute of experimented dataset Water-Birds<ref type="bibr" target="#b13">[14]</ref> is background. All results use synthetic images set to five times the number of real images.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. Jae Myung Kim thanks the International Max Planck Research School for Intelligent Systems (IMPRS-IS) and the European Laboratory for Learning and Intelligent Systems (ELLIS) PhD programs for support.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task and Criteria</head><p>As an AI language model, generate backgrounds where the given class of objects typically exists ('feasible') and where they absolutely cannot exist ('unfeasible'). For each background, provide a one-sentence description detailing its visual appearance. The description should be vivid and adhere to the specified criteria.</p><p>1. Feasible Backgrounds: Identify environments where the object class naturally occurs in the real world.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feasible Prompt Word Examples from Pets</head><p>Background:</p><p>â¢ suburban backyard: A grassy area with a wooden fence, a few trees, and a doghouse in one corner. â¢ city park: A green space with open fields, walking paths, and other people walking their dogs. â¢ ... â¢ patio: A stone patio with outdoor furniture, potted plants, and a view of the garden.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Color:</head><p>â¢ Abyssinian: ruddy, blue gray, silver, fawn, fawn.</p><p>â¢ American Bulldog: white, brindle, brown, fawn, brown.</p><p>â¢ ...</p><p>â¢ Yorkshire Terrier: blue gray, tan, black, gold, tan. Texture:</p><p>â¢ Abyssinian:</p><p>ruddy ticked coat: warm ruddy brown fur with black ticking throughout.</p><p>sorrel coat: light reddish-brown fur with coppery tones.</p><p>blue coat: soft blue-gray fur with warm undertones.</p><p>fawn coat: light cream-colored fur with a gentle rose tint.</p><p>chocolate ticked coat: rich chocolate fur with lighter ticking. â¢ ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â¢ Yorkshire Terrier:</head><p>steel blue and tan coat: long, silky fur in steel blue with tan points.</p><p>black and tan coat: shiny black fur with tan points.</p><p>golden tan coat: long fur in a rich golden tan color.</p><p>blue and gold coat: dark blue fur with golden tan accents.</p><p>silver and tan coat: light silver fur with warm tan points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Infeasible Prompt Word Examples from Pets</head><p>Background:</p><p>â¢ space station: A high-tech interior with floating objects, control panels, and a view of Earth through a window. â¢ ... â¢ mars surface: A barren, reddish landscape with rocks, dust, and no signs of life.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Color:</head><p>â¢ Abyssinian: purple, blue, pink, orange, neon green.</p><p>â¢ American Bulldog: purple, pink, blue, green, yellow.</p><p>â¢ ...</p><p>â¢ Yorkshire Terrier: green, purple, blue, yellow, orange. Texture:</p><p>â¢ elephant skin texture: characterized by thick, rough, and wrinkled surfaces, with deep creases. â¢ wood grain: parallel grooves and rings resembling tree bark, with a natural flow pattern typically seen in wooden planks. â¢ ... â¢ metallic scales: small, shiny scales arranged in an overlapping pattern. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lama</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florencia</forename><surname>Leoni Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janko</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Altman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Shyamal Anadkat, et al. Gpt-4 technical report</title>
				<imprint>
			<date type="published" when="2003">2023. 2, 4, 1, 3</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">VisMin: Visual minimal-change understanding</title>
		<author>
			<persName><forename type="first">Rabiul</forename><surname>Awal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saba</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.16772</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">ClearDepth: enhanced stereo perception of transparent objects for robotic manipulation</title>
		<author>
			<persName><forename type="first">Kaixin</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.08926</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learners benefit more from out-of-distribution examples</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">FrÃ©dÃ©ric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youssouf</forename><surname>Chherawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myriam</forename><surname>CÃ´tÃ©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Eustache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<publisher>JMLR Workshop and Conference Proceedings</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="164" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Briaai background removal v1.4 model</title>
		<author>
			<persName><surname>Briaai</surname></persName>
		</author>
		<ptr target="https://huggingface.co/briaai/RMBG-1.4.5" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">In-structpix2pix: Learning to follow image editing instructions</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiannan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muyan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinglong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="24185" to="24198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Diversified in-domain synthesis with efficient fine-tuning for few-shot classification</title>
		<author>
			<persName><forename type="first">G Turrisi</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Da Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Dall'asen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><surname>Ricci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.03046</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The value of out-ofdistribution data</title>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silva</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carey</forename><surname>Priebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">T</forename><surname>Vogelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7366" to="7389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The MNIST database of handwritten digit images for machine learning research</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="141" to="142" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A survey on in-context learning</title>
		<author>
			<persName><forename type="first">Qingxiu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heming</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.00234</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Diversify your vision datasets with automatic diffusion-based augmentation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Dunlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alyssa</forename><surname>Umino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="79024" to="79034" />
			<date type="published" when="2004">2023. 2, 3, 5, 6, 7, 1, 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep generative models for synthetic data: A survey</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Eigenschink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Reutterer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Vamosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Vamosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaudius</forename><surname>Kalcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="47304" to="47320" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scaling laws of synthetic images for model training... for now</title>
		<author>
			<persName><forename type="first">Lijie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Katabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Instagen: Enhancing object detection by training on synthetic dataset</title>
		<author>
			<persName><forename type="first">Chengjian</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="14121" to="14130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Guiding instruction-based image editing via multimodal large language models</title>
		<author>
			<persName><forename type="first">Tsu-Jui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenze</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.17102</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">How much data are augmentations worth? an investigation into scaling laws, invariance, and implicit regularization</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gowthami</forename><surname>Somepalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravid</forename><surname>Shwartz-Ziv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2210.06441</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Synthetic data in health care: A narrative review</title>
		<author>
			<persName><forename type="first">Aldren</forename><surname>Gonzales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guruprabha</forename><surname>Guruswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Digital Health</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">e0000082</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative adversarial networks. Communications of the ACM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="139" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Hasan</forename><surname>Abed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><forename type="middle">Kader</forename><surname>Hammoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hani</forename><surname>Itani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Pizzati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adel</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.01832</idno>
		<title level="m">SynthCLIP: Are we Ready for a fully synthetic CLIP training?</title>
				<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Is synthetic data from generative models ready for image recognition?</title>
		<author>
			<persName><forename type="first">Ruifei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuhui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.07574</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Prompt-to-prompt image editing with cross attention control</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Mokady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.01626</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Lora: Low-rank adaptation of large language models</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">DataDream: Few-shot guided dataset generation</title>
		<author>
			<persName><forename type="first">Jae Myung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Alaniz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.10910</idno>
		<imprint>
			<date type="published" when="2001">2024. 2, 3, 5, 1</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Sungnyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kibeom</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daesik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Namhyuk</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><surname>Diffblender</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.15194</idno>
		<title level="m">Scalable and composable multimodal text-to-image diffusion models</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Segment anything</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Rolland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4015" to="4026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2008">2013. 5, 6, 4, 8</date>
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zejun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.07895</idno>
		<title level="m">Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models</title>
				<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards understanding cross and self-attention in stable diffusion for text-guided image editing</title>
		<author>
			<persName><forename type="first">Bingyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingfeng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual instruction tuning</title>
				<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Marrying dino with grounded pre-training for open-set object detection</title>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.05499</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Summary of chatgpt-related research and perspective towards the future of large language models</title>
		<author>
			<persName><forename type="first">Yiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengshen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengliang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Meta-Radiology</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">100017</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Repaint: Inpainting using denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="11461" to="11471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Inpainting using denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L Repaint</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2004">2013. 5, 6, 7, 8, 4</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Mathiasen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederik</forename><surname>HvilshÃ¸j</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14075</idno>
		<title level="m">Backpropagating through fr\&apos;echet inception distance</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Context diffusion: In-context aware image generation</title>
		<author>
			<persName><forename type="first">Ivona</forename><surname>Najdenkoska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radenovic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.03584</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Glide: Towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10741</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">TimothÃ©e</forename><surname>Darcet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">ThÃ©o</forename><surname>Moutakanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huy</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Szafraniec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasil</forename><surname>Khalidov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Haziza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.07193</idno>
		<title level="m">Learning robust visual features without supervision</title>
				<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Omkar M Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2012. 5, 6, 3, 4, 7</date>
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
	<note>PmLR, 2021. 3, 5, 6, 1</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Meenal Parakh, Stamatis Alexandropoulos, Lahav Lipson, et al. Infinigen indoors: Photorealistic indoor scenes using procedural generation</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Raistrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjie</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karhan</forename><surname>Kayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beining</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="21783" to="21794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>Pmlr</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Ryali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitham</forename><surname>Khedr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>RÃ¤dle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Rolland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.00714</idno>
		<title level="m">Segment anything in images and videos</title>
				<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">BjÃ¶rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2005">2022. 1, 2, 3, 4, 5</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical image computing and computer-assisted intervention-MICCAI 2015: 18th international conference</title>
				<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">October 5-9, 2015. 2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note>proceedings, part III 18</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Photorealistic text-to-image diffusion models with deep language understanding</title>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burcu</forename><surname>Karagol Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="36479" to="36494" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fake it till you make it: Learning transferable representations from synthetic imagenet clones</title>
		<author>
			<persName><forename type="first">Mert</forename><surname>BÃ¼lent SarÄ±yÄ±ldÄ±z</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Diffumask: Synthesizing images with pixel-level annotations for semantic segmentation using diffusion models</title>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Paint by example: Exemplar-based image editing with diffusion models</title>
		<author>
			<persName><forename type="first">Binxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuejin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="18381" to="18391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Generalized out-of-distribution detection: A survey. International Journal of Computer Vision</title>
		<author>
			<persName><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Jun</forename><surname>Hu Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sibo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Ip-Adapter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.06721</idno>
		<title level="m">Text compatible image prompt adapter for text-to-Image diffusion models</title>
				<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Diversify, don&apos;t fine-tune: Scaling up visual recognition training with synthetic images</title>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Culatana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghuraman</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.02253</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lionel</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.03605</idno>
		<title level="m">Dino: Detr with improved denoising anchor boxes for end-to-end object detection</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Adding conditional control to text-to-image diffusion models</title>
		<author>
			<persName><forename type="first">Lvmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2002">2023. 3, 4, 5, 1, 2</date>
			<biblScope unit="page" from="3836" to="3847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Hive: Harnessing human feedback for instructional visual editing</title>
		<author>
			<persName><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia-Chih</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="9026" to="9036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Programming every example: Lifting pretraining data quality like experts at scale</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junlong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.17115</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
